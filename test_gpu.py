"""
GPU å’Œ CUDA é…ç½®æ¸¬è©¦è…³æœ¬
"""

import sys

import torch


def test_cuda_setup():
    """æ¸¬è©¦ CUDA è¨­ç½®"""
    print("=" * 60)
    print("ğŸ” PyTorch å’Œ CUDA é…ç½®æª¢æŸ¥")
    print("=" * 60)

    # PyTorch ç‰ˆæœ¬
    print(f"\nğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")

    # CUDA å¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print(f"âœ… CUDA å¯ç”¨: {cuda_available}")

    if not cuda_available:
        print("\nâŒ CUDA ä¸å¯ç”¨ï¼")
        print("å¯èƒ½åŸå› :")
        print("1. å®‰è£äº† CPU ç‰ˆæœ¬çš„ PyTorch")
        print("2. NVIDIA é©…å‹•æœªæ­£ç¢ºå®‰è£")
        print("3. CUDA toolkit ç‰ˆæœ¬ä¸åŒ¹é…")
        print("\nè«‹åŸ·è¡Œä»¥ä¸‹å‘½ä»¤é‡æ–°å®‰è£ CUDA ç‰ˆæœ¬:")
        print("pip uninstall torch torchvision torchaudio -y")
        print(
            "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128"
        )
        return False

    # CUDA è©³ç´°ä¿¡æ¯
    print(f"ğŸ”¢ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"ğŸ® cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print(f"ğŸ“Š å¯ç”¨ GPU æ•¸é‡: {torch.cuda.device_count()}")

    # GPU è©³ç´°ä¿¡æ¯
    for i in range(torch.cuda.device_count()):
        print(f"\nğŸ–¥ï¸  GPU {i}: {torch.cuda.get_device_name(i)}")
        total_mem = torch.cuda.get_device_properties(i).total_memory
        print(f"   ç¸½è¨˜æ†¶é«”: {total_mem / 1024**3:.2f} GB")
        props = torch.cuda.get_device_properties(i)
        print(f"   è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")

    # æ¸¬è©¦å¼µé‡é‹ç®—
    print("\n" + "=" * 60)
    print("ğŸ§ª GPU é‹ç®—æ¸¬è©¦")
    print("=" * 60)

    try:
        # å‰µå»ºå¼µé‡
        x = torch.randn(1000, 1000)
        print(f"âœ“ CPU å¼µé‡å‰µå»º: {x.shape}")

        # ç§»å‹•åˆ° GPU
        x_gpu = x.cuda()
        print(f"âœ“ GPU å¼µé‡å‰µå»º: {x_gpu.shape}, device: {x_gpu.device}")

        # GPU çŸ©é™£ä¹˜æ³•
        result = torch.mm(x_gpu, x_gpu)
        print(f"âœ“ GPU çŸ©é™£ä¹˜æ³•: {result.shape}")

        # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated() / 1024**2
        memory_reserved = torch.cuda.memory_reserved() / 1024**2
        print("\nğŸ’¾ GPU è¨˜æ†¶é«”ä½¿ç”¨:")
        print(f"   å·²åˆ†é…: {memory_allocated:.2f} MB")
        print(f"   å·²ä¿ç•™: {memory_reserved:.2f} MB")

        print("\nâœ… GPU é‹ç®—æ¸¬è©¦é€šéï¼")
        return True

    except Exception as e:
        print(f"\nâŒ GPU é‹ç®—æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_training_speed():
    """æ¯”è¼ƒ CPU vs GPU è¨“ç·´é€Ÿåº¦"""
    if not torch.cuda.is_available():
        print("\nâš ï¸  GPU ä¸å¯ç”¨ï¼Œè·³éé€Ÿåº¦æ¸¬è©¦")
        return

    print("\n" + "=" * 60)
    print("âš¡ CPU vs GPU é€Ÿåº¦æ¯”è¼ƒ")
    print("=" * 60)

    import time

    # æ¸¬è©¦æ•¸æ“š
    size = 5000
    iterations = 10

    # CPU æ¸¬è©¦
    print(f"\nğŸ–¥ï¸  CPU æ¸¬è©¦ ({iterations} æ¬¡ {size}x{size} çŸ©é™£ä¹˜æ³•)...")
    x_cpu = torch.randn(size, size)
    y_cpu = torch.randn(size, size)

    start = time.time()
    for _ in range(iterations):
        _ = torch.mm(x_cpu, y_cpu)
    cpu_time = time.time() - start
    print(f"   è€—æ™‚: {cpu_time:.3f} ç§’")

    # GPU æ¸¬è©¦
    print(f"\nğŸ® GPU æ¸¬è©¦ ({iterations} æ¬¡ {size}x{size} çŸ©é™£ä¹˜æ³•)...")
    x_gpu = x_cpu.cuda()
    y_gpu = y_cpu.cuda()

    # Warm up
    _ = torch.mm(x_gpu, y_gpu)
    torch.cuda.synchronize()

    start = time.time()
    for _ in range(iterations):
        _ = torch.mm(x_gpu, y_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start
    print(f"   è€—æ™‚: {gpu_time:.3f} ç§’")

    # é€Ÿåº¦æå‡
    speedup = cpu_time / gpu_time
    print(f"\nğŸš€ GPU åŠ é€Ÿæ¯”: {speedup:.1f}x")

    if speedup > 10:
        print("âœ… GPU æ€§èƒ½å„ªç§€ï¼")
    elif speedup > 5:
        print("âœ… GPU æ€§èƒ½è‰¯å¥½")
    elif speedup > 2:
        print("âš ï¸  GPU æ€§èƒ½ä¸€èˆ¬")
    else:
        print("âŒ GPU æ€§èƒ½ä¸ä½³ï¼Œå¯èƒ½å­˜åœ¨é…ç½®å•é¡Œ")


def test_neural_network():
    """æ¸¬è©¦ç¥ç¶“ç¶²è·¯åœ¨ GPU ä¸Šé‹è¡Œ"""
    if not torch.cuda.is_available():
        print("\nâš ï¸  GPU ä¸å¯ç”¨ï¼Œè·³éç¥ç¶“ç¶²è·¯æ¸¬è©¦")
        return

    print("\n" + "=" * 60)
    print("ğŸ§  ç¥ç¶“ç¶²è·¯ GPU æ¸¬è©¦")
    print("=" * 60)

    import torch.nn as nn

    # å‰µå»ºç°¡å–®ç¶²è·¯
    class SimpleNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(5, 64)
            self.fc2 = nn.Linear(64, 64)
            self.fc3 = nn.Linear(64, 2)

        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            return self.fc3(x)

    # ç§»å‹•åˆ° GPU
    net = SimpleNet().cuda()
    print("âœ“ ç¶²è·¯å·²å‰µå»ºä¸¦ç§»å‹•åˆ° GPU")

    # æ¸¬è©¦å‰å‘å‚³æ’­
    x = torch.randn(32, 5).cuda()
    output = net(x)
    print(f"âœ“ å‰å‘å‚³æ’­: input {x.shape} -> output {output.shape}")

    # æ¸¬è©¦åå‘å‚³æ’­
    loss = output.sum()
    loss.backward()
    print("âœ“ åå‘å‚³æ’­å®Œæˆ")

    print("\nâœ… ç¥ç¶“ç¶²è·¯ GPU æ¸¬è©¦é€šéï¼")


if __name__ == "__main__":
    print("\n" + "ğŸ¯" * 30)
    print("CUDA PyTorch é…ç½®æª¢æŸ¥å·¥å…·")
    print("ğŸ¯" * 30 + "\n")

    # æ¸¬è©¦ CUDA è¨­ç½®
    cuda_ok = test_cuda_setup()

    if cuda_ok:
        # é€Ÿåº¦æ¯”è¼ƒ
        test_training_speed()

        # ç¥ç¶“ç¶²è·¯æ¸¬è©¦
        test_neural_network()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼æ‚¨çš„ GPU å·²æ­£ç¢ºé…ç½®")
        print("=" * 60)
        print("\nğŸ’¡ å»ºè­°:")
        print("1. åœ¨è¨“ç·´æ™‚è¨­ç½® device='cuda' ä¾†ä½¿ç”¨ GPU")
        print("2. ä½¿ç”¨è¼ƒå¤§çš„ batch_size (å¦‚ 256) ä»¥å……åˆ†åˆ©ç”¨ GPU")
        print("3. å¢åŠ ä¸¦è¡Œç’°å¢ƒæ•¸é‡ (å¦‚ 8) æå‡è¨“ç·´æ•ˆç‡")
        print("\nğŸ“š æŸ¥çœ‹ utils/training_config.py äº†è§£ GPU å„ªåŒ–é…ç½®")
    else:
        print("\n" + "=" * 60)
        print("âŒ CUDA é…ç½®æœ‰å•é¡Œï¼Œè«‹æª¢æŸ¥å®‰è£")
        print("=" * 60)
        sys.exit(1)
