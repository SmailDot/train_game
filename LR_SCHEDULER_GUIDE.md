# 學習率調度器使用指南

## 🎯 什麼是學習率調度器？

學習率調度器會**自動調整學習率**，無需手動干預。就像汽車的自動變速箱，根據訓練狀況自動切換"檔位"。

## 🚀 快速開始

編輯 `training_config.json` 中的 `lr_scheduler` 部分：

```json
{
  "lr_scheduler": {
    "type": "adaptive",  // 選擇調度器類型
    "patience": 30,      // 30次迭代無改善後降低學習率
    "factor": 0.5,       // 降低為原來的 50%
    "min_lr": 1e-6       // 最低不低於 0.000001
  }
}
```

## 📊 調度器類型對比

### 1. `adaptive` - 自適應調度（**推薦**）⭐

**適用場景**：大多數情況，尤其是不確定該用哪種時

**工作原理**：
- 追蹤最佳獎勵
- 如果連續 N 次迭代沒有改善，降低學習率
- 獎勵改善後重置計數器

**配置參數**：
```json
{
  "type": "adaptive",
  "patience": 30,              // 容忍多少次迭代無改善
  "factor": 0.5,               // 降低幅度（0.5 = 減半）
  "min_lr": 1e-6,              // 最低學習率
  "improvement_threshold": 0.01 // 至少提升1%才算改善
}
```

**效果示例**：
```
迭代 #10:  學習率 0.000300, 獎勵 -2.5
迭代 #20:  學習率 0.000300, 獎勵 -1.8 ✅ 改善
迭代 #30:  學習率 0.000300, 獎勵 -1.9
迭代 #40:  學習率 0.000300, 獎勵 -1.7 ✅ 新最佳
迭代 #50:  學習率 0.000300, 獎勵 -1.8 (無改善 1/30)
...
迭代 #350: 學習率 0.000300, 獎勵 -1.6 (無改善 30/30)
📉 學習率自適應調整: 0.000300 → 0.000150
```

---

### 2. `step` - 階梯式衰減

**適用場景**：訓練穩定，想要規律降低學習率

**工作原理**：
- 每 N 個迭代自動降低學習率
- 不考慮訓練效果，按計劃執行

**配置參數**：
```json
{
  "type": "step",
  "step_size": 100,  // 每100個迭代
  "gamma": 0.9       // 學習率 ×0.9
}
```

**效果示例**：
```
迭代 #0:   學習率 0.000300
迭代 #100: 學習率 0.000270 (×0.9)
迭代 #200: 學習率 0.000243 (×0.9)
迭代 #300: 學習率 0.000219 (×0.9)
```

---

### 3. `exponential` - 指數衰減

**適用場景**：希望學習率平滑連續下降

**工作原理**：
- 每個迭代都降低一點點
- 衰減非常平滑

**配置參數**：
```json
{
  "type": "exponential",
  "gamma": 0.999  // 每迭代 ×0.999
}
```

**效果示例**：
```
迭代 #0:   學習率 0.000300
迭代 #100: 學習率 0.000271 (100次累積衰減)
迭代 #500: 學習率 0.000182
迭代 #1000: 學習率 0.000110
```

---

### 4. `cosine` - 餘弦退火

**適用場景**：已知訓練總迭代次數，希望平滑降到最小值

**工作原理**：
- 遵循餘弦曲線降低學習率
- 在週期結束時達到最小值

**配置參數**：
```json
{
  "type": "cosine",
  "T_max": 500,     // 500個迭代為一個週期
  "eta_min": 1e-6   // 最小學習率
}
```

**效果示例**：
```
迭代 #0:   學習率 0.000300 (起點)
迭代 #125: 學習率 0.000152 (1/4週期)
迭代 #250: 學習率 0.000001 (1/2週期，最低點)
迭代 #375: 學習率 0.000152 (3/4週期)
迭代 #500: 學習率 0.000300 (週期重置)
```

---

### 5. `reduce_on_plateau` - 性能停滯降低

**適用場景**：訓練不穩定，需要基於實際表現調整

**工作原理**：
- 監控平均獎勵
- 如果獎勵停滯，自動降低學習率
- PyTorch 內建，自動顯示調整信息

**配置參數**：
```json
{
  "type": "reduce_on_plateau",
  "patience": 20,  // 等待20次迭代
  "factor": 0.5    // 降低為原來的50%
}
```

**效果示例**：
```
迭代 #50: 獎勵 5.2 (最佳)
迭代 #60: 獎勵 5.0
迭代 #70: 獎勵 5.1 (停滯中...)
Epoch    71: reducing learning rate to 0.000150
```

---

### 6. `none` - 不使用調度器

**適用場景**：想要完全手動控制學習率

**工作原理**：
- 學習率保持不變
- 除非您手動修改配置文件

---

## 🎓 如何選擇調度器？

### 🔰 新手推薦流程

1. **第一次訓練**：使用 `adaptive`
   ```json
   {"type": "adaptive", "patience": 30, "factor": 0.5}
   ```

2. **觀察 100-200 個迭代**：
   - 如果獎勵持續上升 → 繼續使用
   - 如果獎勵震盪不穩 → 試試 `reduce_on_plateau`
   - 如果獎勵過早停滯 → 減小 `patience` 到 20

3. **訓練後期**（500+ 迭代）：
   - 如果想要更穩定 → 切換到 `cosine`
   - 如果效果很好 → 保持 `adaptive`

### 🎯 根據訓練階段選擇

| 訓練階段 | 推薦調度器 | 原因 |
|---------|-----------|------|
| 早期探索 (0-200) | `none` 或 `adaptive` | 需要較高學習率快速學習 |
| 中期優化 (200-1000) | `adaptive` | 根據效果自動調整 |
| 後期精調 (1000+) | `cosine` 或 `exponential` | 平滑降低以穩定收斂 |

### 🔍 根據訓練問題選擇

| 問題現象 | 推薦調度器 | 配置建議 |
|---------|-----------|---------|
| 獎勵震盪劇烈 | `reduce_on_plateau` | `patience: 10-20` |
| 過早收斂（停滯） | `adaptive` | `patience: 20-30` |
| 訓練太慢 | `step` 或 `none` | 保持較高學習率 |
| 後期不穩定 | `cosine` | `T_max: 剩餘迭代數` |

---

## ⚙️ 參數調優建議

### `patience` （耐心值）

**含義**：容忍多少次迭代無改善

- **太小** (< 10)：學習率降得太快，可能過早降低
- **太大** (> 50)：反應遲鈍，浪費訓練時間
- **推薦值**：
  - 快速實驗：15-20
  - 正常訓練：25-35
  - 長期訓練：40-50

### `factor` （衰減因子）

**含義**：學習率降低的幅度

- **激進** (0.1-0.3)：大幅降低，適合確定需要減速時
- **溫和** (0.4-0.6)：中等降低，最常用
- **保守** (0.7-0.9)：小幅降低，適合微調
- **推薦值**：0.5 (減半)

### `min_lr` （最小學習率）

**含義**：學習率的下限

- **太大** (> 1e-5)：可能無法充分收斂
- **太小** (< 1e-7)：接近0，訓練停滯
- **推薦值**：
  - 快速任務：1e-5
  - 正常任務：1e-6
  - 高精度任務：1e-7

---

## 📈 監控學習率變化

訓練時會顯示當前學習率：

```
============================================================
訓練迭代 #50
============================================================
...
📊 當前學習率: 0.000250 (初始: 0.000300)
```

如果看到這個，說明調度器正在工作！

---

## 🛠️ 常見調整場景

### 場景 1：訓練開始就很慢

**問題**：獎勵上升太慢或不動

**解決方案**：
```json
{
  "type": "none"  // 暫時關閉調度器，保持高學習率
}
```
或提高初始學習率（在 `gpu_training` 中）

---

### 場景 2：訓練中期突然不動了

**問題**：獎勵停在某個值不再改善

**解決方案**：
```json
{
  "type": "adaptive",
  "patience": 20,     // 更快反應
  "factor": 0.3       // 更大幅度降低
}
```

---

### 場景 3：後期震盪劇烈

**問題**：獎勵上下震盪，無法穩定

**解決方案**：
```json
{
  "type": "cosine",
  "T_max": 200,       // 根據剩餘迭代數設定
  "eta_min": 1e-6
}
```

---

### 場景 4：想要完全控制

**問題**：調度器亂調，我想自己決定

**解決方案**：
```json
{
  "type": "none"  // 關閉自動調度
}
```
然後手動編輯 `learning_rate` 參數

---

## 🔬 實驗建議

### 第一次嘗試

```json
{
  "lr_scheduler": {
    "type": "adaptive",
    "patience": 30,
    "factor": 0.5,
    "min_lr": 1e-6,
    "improvement_threshold": 0.01
  }
}
```

### 如果效果不好（200迭代後）

試試更激進的設置：
```json
{
  "type": "adaptive",
  "patience": 20,      // 更快反應
  "factor": 0.3,       // 更大降幅
  "improvement_threshold": 0.005  // 更低門檻
}
```

### 如果訓練很穩定（500迭代後）

切換到平滑模式：
```json
{
  "type": "cosine",
  "T_max": 500,
  "eta_min": 1e-6
}
```

---

## 📊 效果對比示例

### 不使用調度器 (`none`)
```
迭代 #0:    學習率 0.000300, 獎勵 -5.0
迭代 #100:  學習率 0.000300, 獎勵 2.5
迭代 #500:  學習率 0.000300, 獎勵 8.2
迭代 #1000: 學習率 0.000300, 獎勵 8.5 (停滯，學習率太高無法精調)
```

### 使用 `adaptive`
```
迭代 #0:    學習率 0.000300, 獎勵 -5.0
迭代 #100:  學習率 0.000300, 獎勵 2.5
迭代 #500:  學習率 0.000150, 獎勵 8.2 (自動降低)
迭代 #1000: 學習率 0.000075, 獎勵 12.3 (持續優化)
```

---

## 🆘 故障排除

### 問題：學習率降得太快

**現象**：
```
迭代 #50: 學習率已達最小值，無法再降低
```

**解決**：
- 增加 `patience`：`30 → 50`
- 增加 `min_lr`：`1e-6 → 1e-5`
- 減小 `factor`：`0.5 → 0.7`

---

### 問題：學習率從未改變

**現象**：一直顯示初始學習率

**檢查**：
1. `type` 是否設為 `"none"`？
2. 是否有獎勵改善（`adaptive` 需要先有改善才會重置）？
3. 配置文件是否保存？

---

### 問題：不知道哪個效果好

**建議**：做個小實驗

1. 複製 checkpoint
2. 分別用不同調度器訓練 200 個迭代
3. 比較最終獎勵
4. 選擇最好的繼續訓練

---

## 💡 專業建議

1. **不要頻繁切換**：給每個調度器至少 100 個迭代的測試時間

2. **觀察趨勢而非瞬時值**：看 50-100 個迭代的平均獎勵趨勢

3. **記錄實驗結果**：寫下哪個配置效果最好

4. **先穩定再優化**：如果訓練崩潰，先用 `none` 穩定訓練

5. **結合其他參數**：學習率調度要配合合適的 `batch_size` 和 `ppo_epochs`

---

## 📚 延伸閱讀

- **階梯式衰減**：適合已知訓練階段的場景
- **餘弦退火**：論文證明對於 ResNet 等深度網絡效果很好
- **自適應調整**：最接近人工調參的思路
- **性能停滯降低**：PyTorch 官方推薦的通用方法

---

**祝訓練順利！記得根據實際效果調整配置。** 🚀
