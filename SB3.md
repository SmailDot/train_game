# Stable-Baselines3 (SB3) PPO 訓練詳解

本文檔詳細說明本專案使用的 **Stable-Baselines3 (SB3)** 框架及其核心演算法 **PPO (Proximal Policy Optimization)** 的訓練過程與日誌參數意義。

---

## 🧠 什麼是 PPO (Proximal Policy Optimization)?

PPO 是目前最流行的深度強化學習 (Deep Reinforcement Learning) 演算法之一，由 OpenAI 提出。它在 **訓練穩定性** 與 **樣本效率** 之間取得了極佳的平衡。

### 核心概念
1.  **Actor-Critic 架構**：
    -   **Actor (策略網路)**：負責決定動作（例如：要不要跳躍）。
    -   **Critic (價值網路)**：負責評分當前的狀態好不好（例如：現在這個位置有多大機率會死）。
2.  **Clip 機制 (裁剪)**：
    -   為了防止模型一次更新太多導致「學壞」，PPO 限制了每次更新的幅度（通常限制在 0.8 ~ 1.2 倍之間）。這是 PPO 穩定的關鍵。
3.  **On-Policy**：
    -   PPO 是 On-Policy 演算法，意味著它只能使用「當前最新策略」收集的數據來學習。舊的經驗不能重複使用，這也是為什麼我們需要並行環境來加速收集數據。

---

## 📊 訓練日誌參數詳解

在訓練過程中，控制台會輸出如下的表格。以下是每個欄位的詳細意義說明：

### 1. `env/` (環境自定義指標)
這些指標是由 `Monitor` wrapper 記錄的，直接反映 AI 在遊戲中的具體表現。

| 參數名稱 | 中文意義 | 詳細解釋 | 理想趨勢 |
| :--- | :--- | :--- | :--- |
| **`alignment_score`** | **對齊分數** | AI 通過障礙物時，是否位於空隙的正中心。<br>• `1.0`: 完美中心<br>• `0.0`: 撞牆邊緣 | 越高越好 (接近 1.0) |
| **`passed_count`** | **通過數量** | AI 平均每局能成功通過多少個障礙物。這是衡量生存能力最直觀的指標。 | 越高越好 |
| **`scroll_speed`** | **捲動速度** | 當前的遊戲速度。若啟用了課程學習 (Curriculum)，隨著 AI 變強，此速度會自動提升。 | 隨訓練階段上升 |
| **`win_rate`** | **勝率** | 在此批次中，成功達到目標分數 (如 6666 分) 的比例。 | 越高越好 |

### 2. `rollout/` (採樣/遊玩階段)
這是 PPO 在與環境互動、收集經驗時的統計數據。

| 參數名稱 | 中文意義 | 詳細解釋 | 理想趨勢 |
| :--- | :--- | :--- | :--- |
| **`ep_len_mean`** | **平均步數** | 平均每一局遊戲能堅持多少個 Frame (步數)。<br>數值越高代表 AI 活得越久。 | 越高越好 |
| **`ep_rew_mean`** | **平均獎勵** | 平均每一局遊戲獲得的總分 (Total Reward)。<br>包含存活獎勵、通過獎勵、扣分等總和。 | 越高越好 |

### 3. `time/` (系統效能)
監控訓練硬體與時間效率的指標。

| 參數名稱 | 中文意義 | 詳細解釋 |
| :--- | :--- | :--- |
| **`fps`** | **FPS** | 每秒處理的步數 (Frames Per Second)。<br>數值取決於 CPU/GPU 效能與並行環境數量。 |
| **`iterations`** | **迭代次數** | PPO 演算法完成一次完整更新循環的次數。 |
| **`time_elapsed`** | **經過時間** | 本次訓練啟動後經過的總秒數。 |
| **`total_timesteps`** | **總步數** | 累積訓練過的總步數 (包含歷史步數)。 |

### 4. `train/` (神經網路優化指標) ⚠️ 最重要
這是 PPO 核心演算法的健康檢查表，用來判斷模型是否在「正常學習」。

| 參數名稱 | 中文意義 | 詳細解釋 | 診斷標準 |
| :--- | :--- | :--- | :--- |
| **`approx_kl`** | **近似 KL 散度** | 衡量新舊策略之間的差異大小。<br>若數值過大，代表更新太劇烈，模型可能不穩定。 | **越小越好**<br>通常應 < 0.03 |
| **`clip_fraction`** | **裁剪比例** | 有多少比例的訓練樣本因為差異過大而被 PPO 的 Clip 機制「修剪」掉了。 | **適中**<br>通常 < 0.2 (20%) |
| **`clip_range`** | **裁剪範圍** | PPO 的超參數，限制更新幅度的邊界 (通常為 0.1 或 0.2)。 | 固定值 |
| **`entropy_coef`** | **熵係數** | 控制 AI 探索新動作的意願 (權重)。 | 固定或遞減 |
| **`entropy_loss`** | **熵損失** | 代表 AI 行為的隨機性 (Randomness)。<br>• 數值越大 (接近 0)：越隨機 (初期)<br>• 數值越小 (負數)：越確定 (後期) | **隨訓練下降**<br>代表 AI 越來越有自信 |
| **`explained_variance`** | **解釋變異量** | Critic (價值網路) 預測分數的準確度。<br>• `1.0`: 完美預測<br>• `0.0`: 瞎猜<br>• `< 0`: 比瞎猜還差 | **越高越好**<br>通常 > 0.5 |
| **`learning_rate`** | **學習率** | 神經網路更新步伐的大小。 | 固定或遞減 |
| **`loss`** | **總損失** | 所有損失函數的加總 (Policy + Value + Entropy)。 | 趨勢不一定 |
| **`policy_gradient_loss`** | **策略損失** | Actor 網路的損失，目標是讓好動作的機率變大。 | 趨勢不一定 |
| **`value_loss`** | **價值損失** | Critic 網路的損失，目標是讓預測的分數越準越好。 | 趨勢不一定 |

---

## 🛠️ 常見問題診斷 (Troubleshooting)

### Q1: `approx_kl` 突然變得很大 (> 0.05)？
*   **原因**：學習率 (Learning Rate) 可能設太高，導致模型每次更新步伐太大，破壞了原本的知識。
*   **解法**：降低 `learning_rate`。

### Q2: `explained_variance` 一直很低 (< 0)？
*   **原因**：價值網路 (Critic) 完全無法預測局勢。可能是 Reward 設計太隨機，或者神經網路太小。
*   **解法**：檢查 Reward Function 是否合理，或增加網路層數 (如 512x512)。

### Q3: `entropy_loss` 下降得太快？
*   **原因**：模型過早收斂 (Premature Convergence)，變成只會用同一招，不再嘗試新策略。
*   **解法**：增加 `ent_coef` (熵係數)，強迫 AI 多探索。

### Q4: `fps` 很低？
*   **原因**：並行環境數量 (`n_envs`) 太少，或者 CPU 瓶頸。
*   **解法**：增加 `n_envs` (如 8 -> 32)，但要注意記憶體用量。

---

## 🎓 什麼是課程學習 (Curriculum Learning)?

**課程學習 (Curriculum Learning)** 是一種模仿人類學習過程的訓練策略。就像我們在學校學數學，是先學「加減法」，再學「乘除法」，最後才學「微積分」。如果一開始就讓小學生算微積分，他會因為太難而完全學不會。

### 在本專案中的應用
如果一開始就讓遊戲速度極快 (例如 `ScrollSpeed = 10.0`)，AI 會在開局 0.1 秒內撞牆死亡，它根本來不及理解「按空白鍵可以跳」這個因果關係。

因此，我們設計了 **漸進式難度**：

1.  **幼兒園階段 (0 ~ 150 萬步)**：
    *   速度慢 (`MaxScrollSpeed = 3.0`)
    *   障礙物間隙大
    *   **目標**：讓 AI 先學會「不要掉下去」和「基本的跳躍」。
2.  **小學階段 (150 萬 ~ 300 萬步)**：
    *   速度中等 (`MaxScrollSpeed = 3.6`)
    *   障礙物開始變窄
    *   **目標**：讓 AI 學會適應變化的速度。
3.  **大學階段 (300 萬步以上)**：
    *   速度全開 (`MaxScrollSpeed > 4.0`)
    *   極限操作
    *   **目標**：訓練 AI 在高速下的精準反應。

透過這種方式，AI 的學習曲線會更加平滑，最終能達到的分數上限也會更高。
