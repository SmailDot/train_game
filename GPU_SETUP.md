# GPU åŠ é€Ÿè¨­ç½®æŒ‡å—

## ğŸ“‹ ç³»çµ±é…ç½®

- **GPU**: NVIDIA GeForce RTX 3060 Ti (8GB VRAM)
- **é©…å‹•ç‰ˆæœ¬**: 572.83
- **CUDA ç‰ˆæœ¬**: 12.8
- **Python ç‰ˆæœ¬**: 3.12.2

## ğŸ”§ å®‰è£æ­¥é©Ÿ

### 1. å¸è¼‰ CPU ç‰ˆæœ¬çš„ PyTorch

```bash
pip uninstall torch torchvision torchaudio -y
```

### 2. å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
```

**é è¨ˆä¸‹è¼‰å¤§å°**: ~2.9 GB  
**é è¨ˆæ™‚é–“**: 3-5 åˆ†é˜ï¼ˆå–æ±ºæ–¼ç¶²é€Ÿï¼‰

### 3. é©—è­‰å®‰è£

```bash
python test_gpu.py
```

æ‡‰è©²çœ‹åˆ°ï¼š
```
âœ… CUDA å¯ç”¨: True
ğŸ”¢ CUDA ç‰ˆæœ¬: 12.8
ğŸ–¥ï¸  GPU 0: NVIDIA GeForce RTX 3060 Ti
   ç¸½è¨˜æ†¶é«”: 8.00 GB
```

## ğŸš€ ä½¿ç”¨ GPU è¨“ç·´

### è‡ªå‹•é…ç½®ï¼ˆæ¨è–¦ï¼‰

éŠæˆ² UI æœƒè‡ªå‹•æª¢æ¸¬ GPU ä¸¦ä½¿ç”¨å„ªåŒ–çš„é…ç½®ï¼š

```python
# game/ui.py ä¸­çš„ _register_algorithms() æœƒè‡ªå‹•æª¢æ¸¬
use_cuda = torch.cuda.is_available()
if use_cuda:
    print(f"âœ… æª¢æ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}")
```

### GPU å„ªåŒ–é…ç½®

åœ¨ `utils/training_config.py` ä¸­ï¼š

```python
RTX_3060TI_CONFIG = {
    "device": "cuda",              # ä½¿ç”¨ GPU
    "batch_size": 256,             # å¢å¤§ batch size
    "ppo_epochs": 10,              # å¢åŠ  PPO æ›´æ–°æ¬¡æ•¸
    "lr": 2.5e-4,                  # å­¸ç¿’ç‡
    "horizon": 4096,               # å¢åŠ  rollout é•·åº¦
}
```

### æ‰‹å‹•æŒ‡å®šè¨­å‚™

```python
from utils.training_config import TrainingConfig

# GPU è¨“ç·´
config = TrainingConfig(use_gpu=True)
trainer = PPOTrainer(**config.get_ppo_kwargs())

# CPU è¨“ç·´ï¼ˆå‚™ç”¨ï¼‰
config = TrainingConfig(use_gpu=False)
```

## ğŸ“Š æ€§èƒ½é æœŸ

### RTX 3060 Ti æ€§èƒ½æŒ‡æ¨™

| é …ç›® | CPU | GPU | æå‡ |
|------|-----|-----|------|
| Batch Size | 64 | 256 | 4x |
| PPO Epochs | 4 | 10 | 2.5x |
| Parallel Envs | 4 | 8 | 2x |
| çŸ©é™£é‹ç®— | åŸºæº– | ~15x | 15x |
| **ç¸½é«”è¨“ç·´é€Ÿåº¦** | åŸºæº– | **~10-15x** | **10-15x** |

### è¨˜æ†¶é«”ä½¿ç”¨

- **æ¨¡å‹**: ~50 MB
- **Batch (256)**: ~200 MB
- **æ¢¯åº¦ + å„ªåŒ–å™¨**: ~100 MB
- **ç¸½è¨ˆ**: ~350 MB / 8192 MB (4% ä½¿ç”¨ç‡)

**çµè«–**: RTX 3060 Ti 8GB è¨˜æ†¶é«”ç¶½ç¶½æœ‰é¤˜ï¼

## âš™ï¸ è¨“ç·´é…ç½®å»ºè­°

### PPO è¨“ç·´ï¼ˆGPU å„ªåŒ–ï¼‰

```python
{
    "device": "cuda",
    "batch_size": 256,      # å……åˆ†åˆ©ç”¨ GPU
    "ppo_epochs": 10,       # æ›´å¤šæ›´æ–°æ¬¡æ•¸
    "lr": 2.5e-4,           # ç©©å®šå­¸ç¿’ç‡
    "gamma": 0.99,
    "lam": 0.95,
    "clip_eps": 0.2,
    "vf_coef": 0.5,
    "ent_coef": 0.01,       # é™ä½ entropy
    "horizon": 4096,        # å¤§ rollout
}
```

### ä¸¦è¡Œç’°å¢ƒé…ç½®

```python
# GPU æ¨¡å¼ï¼š8 å€‹ä¸¦è¡Œç’°å¢ƒ
n_envs = 8

# æ¯å€‹ç’°å¢ƒæ”¶é›† 512 æ­¥
# ç¸½è¨ˆï¼š8 * 512 = 4096 æ­¥/batch
```

## ğŸ” æ•…éšœæ’é™¤

### å•é¡Œ 1: CUDA ä¸å¯ç”¨

**ç—‡ç‹€**: `torch.cuda.is_available()` è¿”å› `False`

**è§£æ±ºæ–¹æ¡ˆ**:
1. ç¢ºèªå®‰è£çš„æ˜¯ CUDA ç‰ˆæœ¬: `pip show torch | findstr cu`
2. æª¢æŸ¥ NVIDIA é©…å‹•: `nvidia-smi`
3. é‡æ–°å®‰è£: 
   ```bash
   pip uninstall torch -y
   pip install torch --index-url https://download.pytorch.org/whl/cu128
   ```

### å•é¡Œ 2: è¨˜æ†¶é«”ä¸è¶³ (OOM)

**ç—‡ç‹€**: `RuntimeError: CUDA out of memory`

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é™ä½ batch_size
"batch_size": 128  # å¾ 256 é™åˆ° 128

# æˆ–é™ä½ horizon
"horizon": 2048  # å¾ 4096 é™åˆ° 2048

# æˆ–æ¸›å°‘ä¸¦è¡Œç’°å¢ƒ
n_envs = 4  # å¾ 8 é™åˆ° 4
```

### å•é¡Œ 3: GPU åˆ©ç”¨ç‡ä½

**ç—‡ç‹€**: `nvidia-smi` é¡¯ç¤º GPU ä½¿ç”¨ç‡ < 30%

**å¯èƒ½åŸå› **:
1. Batch size å¤ªå°
2. æ•¸æ“šå‚³è¼¸ç“¶é ¸
3. CPU é è™•ç†æ…¢

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# å¢å¤§ batch size
"batch_size": 512  # å¦‚æœè¨˜æ†¶é«”å…è¨±

# ä½¿ç”¨ pin_memory åŠ é€Ÿæ•¸æ“šå‚³è¼¸
# ï¼ˆåœ¨ DataLoader ä¸­è¨­ç½®ï¼‰
```

## ğŸ“ˆ ç›£æ§ GPU ä½¿ç”¨

### å¯¦æ™‚ç›£æ§

```bash
# æ¯ç§’æ›´æ–°ä¸€æ¬¡
nvidia-smi -l 1

# æˆ–ä½¿ç”¨ watch (å¦‚æœæœ‰å®‰è£)
watch -n 1 nvidia-smi
```

### Python ä»£ç¢¼ç›£æ§

```python
import torch

# ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨
allocated = torch.cuda.memory_allocated() / 1024**2  # MB
reserved = torch.cuda.memory_reserved() / 1024**2    # MB

print(f"å·²åˆ†é…: {allocated:.1f} MB")
print(f"å·²ä¿ç•™: {reserved:.1f} MB")

# GPU åˆ©ç”¨ç‡ï¼ˆéœ€è¦é¡å¤–å¥—ä»¶ï¼‰
import pynvml
pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)
util = pynvml.nvmlDeviceGetUtilizationRates(handle)
print(f"GPU åˆ©ç”¨ç‡: {util.gpu}%")
```

## ğŸ¯ æœ€ä½³å¯¦è¸

### 1. é ç†± GPU

```python
# ç¬¬ä¸€æ¬¡é‹è¡Œæ™‚é ç†±
if torch.cuda.is_available():
    dummy = torch.randn(1, 1).cuda()
    _ = dummy + dummy
    torch.cuda.synchronize()
```

### 2. æ··åˆç²¾åº¦è¨“ç·´ï¼ˆé€²éšï¼‰

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(input)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. æ¸…ç†æœªä½¿ç”¨çš„è¨˜æ†¶é«”

```python
# è¨“ç·´å¾Œæ¸…ç†
torch.cuda.empty_cache()
```

### 4. è¨­ç½®éš¨æ©Ÿç¨®å­

```python
import torch
import random
import numpy as np

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
```

## ğŸ“š åƒè€ƒè³‡æº

- [PyTorch CUDA å®˜æ–¹æ–‡æª”](https://pytorch.org/docs/stable/cuda.html)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
- [RTX 3060 Ti è¦æ ¼](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3060-3060ti/)

## âœ… æª¢æŸ¥æ¸…å–®

å®‰è£å®Œæˆå¾Œï¼Œç¢ºèªä»¥ä¸‹é …ç›®ï¼š

- [ ] `torch.cuda.is_available()` è¿”å› `True`
- [ ] `nvidia-smi` é¡¯ç¤º GPU ä¿¡æ¯
- [ ] `test_gpu.py` æ‰€æœ‰æ¸¬è©¦é€šé
- [ ] GPU è¨“ç·´é€Ÿåº¦æå‡ 10x ä»¥ä¸Š
- [ ] è¨“ç·´è¦–çª—æ­£ç¢ºé¡¯ç¤º "PPO è¨“ç·´è¦–çª— (CUDA)"

## ğŸ‰ é–‹å§‹è¨“ç·´

```bash
python run_game.py
```

é»æ“Š "AI è¨“ç·´" â†’ é¸æ“‡ PPO â†’ é–‹å§‹è¨“ç·´

äº«å— GPU åŠ é€Ÿçš„è¨“ç·´é€Ÿåº¦ï¼ğŸš€
