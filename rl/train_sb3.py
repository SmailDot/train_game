#!/usr/bin/env python3
"""
Game2048 SB3 è¨“ç·´è…³æœ¬

ä½¿ç”¨ Stable-Baselines3 è¨“ç·´ PPO ä»£ç†ï¼Œç›®æ¨™æ˜¯é”åˆ° 6666 åˆ†é€šé—œã€‚
"""

import argparse
import os
import sys
from argparse import BooleanOptionalAction
from collections import deque
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import (
    BaseCallback,
    CallbackList,
    CheckpointCallback,
    EvalCallback,
)
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.logger import (
    CSVOutputFormat,
    KVWriter,
    Logger,
    TensorBoardOutputFormat,
)
from stable_baselines3.common.vec_env import (
    DummyVecEnv,
    SubprocVecEnv,
    VecMonitor,
    VecNormalize,
)

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))


def _import_env():
    from rl.game2048_env import Game2048Env as _Game2048Env

    return _Game2048Env


Game2048Env = _import_env()


# --- Custom Logger for Chinese Support and Alignment ---

KEY_TRANSLATIONS = {
    # Environment
    "env/alignment_score": "env/alignment_score(å°é½Šåˆ†æ•¸)",
    "env/passed_count": "env/passed_count(é€šééšœç¤™ç‰©æ•¸é‡)",
    "env/scroll_speed": "env/scroll_speed(ç›®å‰æ²å‹•é€Ÿåº¦)",
    "env/win_rate": "env/win_rate(é€šé—œç‡)",
    # Rollout
    "rollout/ep_len_mean": "rollout/ep_len_mean(å¹³å‡å›åˆé•·åº¦)",
    "rollout/ep_rew_mean": "rollout/ep_rew_mean(å¹³å‡å›åˆçå‹µ)",
    # Time
    "time/fps": "time/fps(å¹€ç‡)",
    "time/iterations": "time/iterations(è¿­ä»£æ¬¡æ•¸)",
    "time/time_elapsed": "time/time_elapsed(ç¶“éæ™‚é–“)",
    "time/total_timesteps": "time/total_timesteps(ç¸½æ­¥æ•¸)",
    # Train
    "train/approx_kl": "train/approx_kl(è¿‘ä¼¼KLæ•£åº¦)",
    "train/clip_fraction": "train/clip_fraction(æ›´æ–°å¹…åº¦éå¤§æ¯”ä¾‹)",
    "train/clip_range": "train/clip_range(æ›´æ–°å¹…åº¦é™åˆ¶)",
    "train/entropy_coef": "train/entropy_coef(ç†µä¿‚æ•¸)",
    "train/entropy_loss": "train/entropy_loss(ç†µæå¤±)",
    "train/explained_variance": "train/explained_variance(åƒ¹å€¼é æ¸¬æº–ç¢ºåº¦)",
    "train/learning_rate": "train/learning_rate(å­¸ç¿’ç‡)",
    "train/loss": "train/loss(ç¸½æå¤±)",
    "train/n_updates": "train/n_updates(æ›´æ–°æ¬¡æ•¸)",
    "train/policy_gradient_loss": "train/policy_gradient_loss(ç­–ç•¥æ¢¯åº¦æå¤±)",
    "train/value_loss": "train/value_loss(åƒ¹å€¼æå¤±)",
}


def get_visual_width(s: str) -> int:
    """Calculate visual width of a string (Chinese chars = 2)."""
    width = 0
    for char in s:
        if "\u4e00" <= char <= "\u9fff" or "\uff00" <= char <= "\uffef":
            width += 2
        else:
            width += 1
    return width


class ChineseHumanOutputFormat(KVWriter):
    """Custom output format that handles Chinese character alignment correctly."""

    def __init__(self, file):
        self.file = file

    def write(
        self,
        key_values: Dict[str, Any],
        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],
        step: int = 0,
    ) -> None:
        # Create strings for printing
        kv_list = []
        for k, v in sorted(key_values.items()):
            # Ignore exclusion to ensure we print everything we have
            # if k in key_excluded: continue

            # Format value
            if isinstance(v, float):
                val_str = f"{v:.3g}"
            else:
                val_str = str(v)

            # Split key into English and Chinese parts
            # Assuming format: "english_key(chinese_translation)"
            if "(" in k and k.endswith(")"):
                parts = k.split("(")
                eng_part = parts[0]
                chn_part = "(" + parts[1]
            else:
                eng_part = k
                chn_part = ""

            kv_list.append((eng_part, chn_part, val_str))

        if not kv_list:
            return

        # Calculate max width for key column
        # We want: English Left <spaces> Chinese Right
        # So max_key_width = max(visual_width(eng) + visual_width(chn)) + padding
        max_key_width = 0
        for eng, chn, _ in kv_list:
            width = get_visual_width(eng) + get_visual_width(chn)
            if width > max_key_width:
                max_key_width = width

        # Add some minimum padding between English and Chinese
        max_key_width += 2

        max_val_len = max(get_visual_width(v) for _, _, v in kv_list)

        # Print separator
        # Format: | key_column | val |
        # Width: 2 + max_key_width + 3 + max_val_len + 2
        dash_len = max_key_width + max_val_len + 7
        self.file.write("-" * dash_len + "\n")

        for eng, chn, v in kv_list:
            # Key formatting: English + spaces + Chinese
            current_key_width = get_visual_width(eng) + get_visual_width(chn)
            padding_len = max_key_width - current_key_width
            key_str = f"{eng}{' ' * padding_len}{chn}"

            val_padding = " " * (max_val_len - get_visual_width(v))
            self.file.write(f"| {key_str} | {v}{val_padding} |\n")

        self.file.write("-" * dash_len + "\n")
        self.file.flush()


class ChineseLogger(Logger):
    """Logger that translates keys to Chinese before recording."""

    def record(
        self,
        key: str,
        value: Any,
        exclude: Optional[Union[str, Tuple[str, ...]]] = None,
    ) -> None:
        # Translate key if possible
        translated_key = KEY_TRANSLATIONS.get(key, key)
        super().record(translated_key, value, exclude)


def make_linear_schedule(start: float, end: float):
    """Create a linear schedule callable compatible with SB3."""

    def schedule(progress_remaining: float) -> float:
        return end + (start - end) * progress_remaining

    return schedule


def apply_finetune_overrides(
    model: PPO,
    finetune_lr: Optional[float] = None,
    finetune_ent: Optional[float] = None,
) -> None:
    """Optionally override learning rate or entropy for fine-tuning runs."""

    if finetune_lr is not None:
        target_lr = float(finetune_lr)

        def _fixed_lr(_progress_remaining: float) -> float:
            return target_lr

        model.lr_schedule = _fixed_lr
        model.learning_rate = target_lr
        optimizer = getattr(model.policy, "optimizer", None)
        if optimizer is not None:
            for group in optimizer.param_groups:
                group["lr"] = target_lr
        print(f"âš™ï¸ Fine-tune learning rate -> {target_lr:.2e}")

    if finetune_ent is not None:
        target_ent = float(finetune_ent)
        model.ent_coef = target_ent
        print(f"âš™ï¸ Fine-tune entropy coef -> {target_ent:.5f}")


def get_curriculum_phases(name: str) -> List[Dict[str, dict]]:
    if name != "progressive":
        return []

    return [
        {
            "threshold": 0,
            "profile": {
                "ScrollIncreasePerPass": 0.012,
                "MaxScrollSpeed": 3.0,
                "GapShrinkPerPass": 0.4,
            },
        },
        {
            "threshold": 1_500_000,
            "profile": {
                "ScrollIncreasePerPass": 0.018,
                "MaxScrollSpeed": 3.6,
                "GapShrinkPerPass": 0.6,
            },
        },
        {
            "threshold": 3_000_000,
            "profile": {
                "ScrollIncreasePerPass": 0.025,
                "MaxScrollSpeed": 4.2,
                "GapShrinkPerPass": 0.8,
            },
        },
    ]


class WinCallback(BaseCallback):
    """
    è‡ªå®šç¾©å›èª¿ï¼šç›£æ§é€šé—œäº‹ä»¶
    """

    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.wins = 0
        self.best_score = 0

    def _on_step(self) -> bool:
        # æª¢æŸ¥ infos ä¸­æ˜¯å¦æœ‰é€šé—œ
        if hasattr(self.locals, "infos"):
            for info in self.locals["infos"]:
                if info.get("win", False):
                    self.wins += 1
                    score = info.get("episode_score", 0)
                    if score > self.best_score:
                        self.best_score = score
                        if self.verbose > 0:
                            print(f"ğŸ‰ æ–°ç´€éŒ„ï¼åˆ†æ•¸: {score}")

                    if self.verbose > 0:
                        print(f"ğŸ¯ é€šé—œ #{self.wins}ï¼åˆ†æ•¸: {score}")

        return True


class EpisodeStatsCallback(BaseCallback):
    """Record custom environment metrics (e.g., passes, scroll speed)."""

    def __init__(
        self,
        prefix: str = "env",
        verbose: int = 0,
        window_size: int = 100,
        target_win_rate: Optional[float] = None,
    ):
        super().__init__(verbose)
        self.prefix = prefix
        self.win_buffer = deque(maxlen=window_size)
        self.pass_buffer = deque(maxlen=window_size)
        self.target_win_rate = target_win_rate

    def _on_step(self) -> bool:
        infos = self.locals.get("infos")
        dones = self.locals.get("dones")

        if not infos or dones is None:
            return True

        for idx, done in enumerate(dones):
            if done:
                info = infos[idx]
                # Buffer the win status (True/False -> 1.0/0.0)
                self.win_buffer.append(float(info.get("win", False)))

                if "passed_count" in info:
                    self.pass_buffer.append(float(info["passed_count"]))

        # Record the mean of the buffers (Rolling Average)
        if self.win_buffer:
            current_win_rate = np.mean(self.win_buffer)
            self.logger.record(f"{self.prefix}/win_rate", current_win_rate)

            # Check for target win rate stop condition
            if (
                self.target_win_rate is not None
                and len(self.win_buffer) >= self.win_buffer.maxlen
                and current_win_rate >= self.target_win_rate
            ):
                if self.verbose > 0:
                    print(
                        f"ğŸ‰ é”æˆç›®æ¨™é€šé—œç‡ï¼ç•¶å‰: {current_win_rate:.2f} "
                        f">= ç›®æ¨™: {self.target_win_rate:.2f}"
                    )
                return False  # Stop training

        if self.pass_buffer:
            self.logger.record(f"{self.prefix}/passed_count", np.mean(self.pass_buffer))

        # Optional: Log instantaneous metrics for debugging if needed,
        # but for TensorBoard, the rolling average is much better.

        return True


class AdaptiveEntropyCallback(BaseCallback):
    """Dynamically adjust entropy coefficient based on recent win rate."""

    def __init__(
        self,
        window_size: int = 4096,
        low_threshold: float = 0.05,
        high_threshold: float = 0.25,
        increase_step: float = 5e-4,
        decrease_step: float = 3e-4,
        min_ent: float = 0.004,
        max_ent: float = 0.012,
        verbose: int = 0,
    ):
        super().__init__(verbose)
        self.window_size = window_size
        self.low_threshold = low_threshold
        self.high_threshold = high_threshold
        self.increase_step = increase_step
        self.decrease_step = decrease_step
        self.min_ent = min_ent
        self.max_ent = max_ent
        self._buffer: deque[float] = deque(maxlen=window_size)
        self._current_ent: Optional[float] = None

    def _on_training_start(self) -> None:
        # Capture the initial entropy coefficient from the model
        self._current_ent = float(getattr(self.model, "ent_coef", 0.01))
        if self.verbose:
            print(f"ğŸ”§ è‡ªé©æ‡‰ç†µå•Ÿå‹•ï¼Œåˆå§‹ ent_coef = {self._current_ent:.5f}")

    def _set_entropy(self, value: float) -> None:
        if self._current_ent is None or abs(value - self._current_ent) < 1e-6:
            return

        self._current_ent = float(np.clip(value, self.min_ent, self.max_ent))
        self.model.ent_coef = self._current_ent
        # Log to TensorBoard for transparency
        self.logger.record("train/entropy_coef", self._current_ent)
        if self.verbose:
            print(f"âš™ï¸ èª¿æ•´ ent_coef -> {self._current_ent:.5f}")

    def _on_step(self) -> bool:
        infos = self.locals.get("infos") if hasattr(self, "locals") else None
        if not infos:
            return True

        win_flag = 1.0 if any(info.get("win", False) for info in infos) else 0.0
        self._buffer.append(win_flag)

        if len(self._buffer) < self.window_size:
            return True

        win_rate = float(np.mean(self._buffer))

        if win_rate >= self.high_threshold:
            self._set_entropy(self._current_ent - self.decrease_step)
        elif win_rate <= self.low_threshold:
            self._set_entropy(self._current_ent + self.increase_step)

        return True


class CurriculumCallback(BaseCallback):
    """Gradually ramp environment difficulty according to predefined phases."""

    def __init__(self, phases: List[Dict[str, dict]], verbose: int = 0):
        super().__init__(verbose)
        self.phases = sorted(phases, key=lambda item: item["threshold"])
        self._current_phase = -1

    def _apply_phase(self, index: int) -> None:
        if index < 0 or index >= len(self.phases):
            return

        profile = self.phases[index]["profile"]
        threshold = self.phases[index]["threshold"]
        self._current_phase = index

        if self.verbose:
            print(
                "ğŸ“ˆ èª²ç¨‹éšæ®µ"
                f" {index + 1}/{len(self.phases)} @ step {threshold:,}: {profile}"
            )

        env_method = getattr(self.training_env, "env_method", None)
        if env_method is not None:
            try:
                env_method("apply_difficulty_profile", profile)
            except AttributeError:
                if self.verbose:
                    print("âš ï¸ ç„¡æ³•å¥—ç”¨èª²ç¨‹è¨­å®šï¼Œç’°å¢ƒç¼ºå°‘ apply_difficulty_profileã€‚")

    def _on_training_start(self) -> None:
        if self.phases:
            self._apply_phase(0)

    def _on_step(self) -> bool:
        if not self.phases:
            return True

        next_index = self._current_phase + 1
        if (
            next_index < len(self.phases)
            and self.num_timesteps >= self.phases[next_index]["threshold"]
        ):
            self._apply_phase(next_index)

        return True


def create_envs(
    n_envs: int = 32,
    normalize: bool = True,
    training: bool = True,
    norm_path: Optional[str] = None,
    seed: int = 42,
    render_mode: Optional[str] = None,
):
    """
    å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ

    Args:
        n_envs: ç’°å¢ƒæ•¸é‡
        normalize: æ˜¯å¦ä½¿ç”¨ VecNormalize

    Returns:
        ç’°å¢ƒå¯¦ä¾‹
    """

    env_kwargs = {}
    if render_mode:
        env_kwargs["render_mode"] = render_mode
        # å¦‚æœå•Ÿç”¨æ¸²æŸ“ï¼Œå¼·åˆ¶ä½¿ç”¨å–®ä¸€ç’°å¢ƒèˆ‡ DummyVecEnv ä»¥é¿å…è¦–çª—è¡çªèˆ‡å´©æ½°
        n_envs = 1
        vec_env_cls = DummyVecEnv
        print(
            "âš ï¸ å•Ÿç”¨æ¸²æŸ“æ¨¡å¼ï¼šå¼·åˆ¶å°‡ç’°å¢ƒæ•¸é‡è¨­ç‚º 1 ä¸¦ä½¿ç”¨ DummyVecEnv ä»¥é¿å…è¦–çª—è¡çªã€‚"
        )
    else:
        vec_env_cls = SubprocVecEnv if n_envs > 1 else DummyVecEnv

    print(f"ğŸš€ å‰µå»º {n_envs} å€‹ä¸¦è¡Œç’°å¢ƒ (Class: {vec_env_cls.__name__})...")

    vec_env = make_vec_env(
        Game2048Env,
        n_envs=n_envs,
        env_kwargs=env_kwargs,
        seed=seed,
        vec_env_cls=vec_env_cls,
    )

    # æ·»åŠ ç›£æ§
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    vec_env = VecMonitor(vec_env, log_dir)

    # å¯é¸ï¼šæ·»åŠ æ­£è¦åŒ–
    if normalize:
        norm_reward = training
        if norm_path and os.path.exists(norm_path):
            vec_env = VecNormalize.load(norm_path, vec_env)
            print(f"ğŸ“„ VecNormalize çµ±è¨ˆè¼‰å…¥: {norm_path}")
        else:
            if norm_path and not os.path.exists(norm_path):
                print(f"âš ï¸ æ‰¾ä¸åˆ° VecNormalize æª”æ¡ˆ {norm_path}ï¼Œå°‡é‡æ–°åˆå§‹åŒ–çµ±è¨ˆã€‚")
            vec_env = VecNormalize(
                vec_env,
                norm_obs=True,
                norm_reward=norm_reward,
                clip_obs=10.0,
                clip_reward=10.0,
                gamma=0.995,
            )

        vec_env.training = training
        vec_env.norm_reward = norm_reward

    if render_mode:
        setattr(vec_env, "render_mode", render_mode)

    return vec_env


def create_callbacks(
    env,
    normalize: bool = False,
    eval_freq: int = 5000,
    save_freq: int = 10000,
    norm_path: Optional[str] = None,
    seed: int = 42,
    adaptive_entropy: bool = True,
    curriculum_phases: Optional[List[Dict[str, dict]]] = None,
    target_win_rate: Optional[float] = None,
):
    """å»ºç«‹è¨“ç·´/è©•ä¼°æ‰€éœ€çš„å›èª¿ã€‚"""

    callbacks = []

    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path="./checkpoints/",
        name_prefix="ppo_game2048",
        save_replay_buffer=True,
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)

    eval_env = create_envs(
        n_envs=4,
        normalize=normalize,
        training=False,
        norm_path=norm_path,
        seed=seed + 1,
        render_mode=None,
    )

    if (
        normalize
        and isinstance(env, VecNormalize)
        and isinstance(eval_env, VecNormalize)
    ):
        eval_env.obs_rms = env.obs_rms.copy()
        eval_env.ret_rms = env.ret_rms.copy()
        eval_env.training = False
        eval_env.norm_reward = False

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/eval/",
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        verbose=1,
    )
    callbacks.append(eval_callback)

    callbacks.append(WinCallback(verbose=1))
    callbacks.append(EpisodeStatsCallback(verbose=0, target_win_rate=target_win_rate))
    if curriculum_phases:
        callbacks.append(CurriculumCallback(curriculum_phases, verbose=1))
    if adaptive_entropy:
        callbacks.append(AdaptiveEntropyCallback(verbose=0))

    return CallbackList(callbacks)


def create_model(env, config: dict):
    """
    å‰µå»º PPO æ¨¡å‹

    Args:
        env: ç’°å¢ƒ
        config: é…ç½®å­—å…¸

    Returns:
        PPO æ¨¡å‹
    """
    print("ğŸ§  å‰µå»º PPO æ¨¡å‹...")

    learning_rate = config["learning_rate"]
    if isinstance(learning_rate, (tuple, list)) and len(learning_rate) == 2:
        learning_rate = make_linear_schedule(learning_rate[0], learning_rate[1])

    # ç¶²çµ¡æ¶æ§‹é…ç½®
    policy_kwargs = dict(
        net_arch=dict(
            pi=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Actor ç¶²çµ¡
            vf=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Critic ç¶²çµ¡
        ),
        activation_fn=torch.nn.ReLU,
    )

    # å‰µå»ºæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        policy_kwargs=policy_kwargs,
        # å­¸ç¿’åƒæ•¸
        learning_rate=learning_rate,
        gamma=config["gamma"],
        gae_lambda=config["gae_lambda"],
        # PPO åƒæ•¸
        clip_range=config["clip_range"],
        ent_coef=config["ent_coef"],
        vf_coef=config["vf_coef"],
        # è¨“ç·´æ•ˆç‡
        n_steps=config["n_steps"],
        batch_size=config["batch_size"],
        n_epochs=config["n_epochs"],
        max_grad_norm=config["max_grad_norm"],
        # æ—¥èªŒå’Œè¨­å‚™
        verbose=config["verbose"],
        tensorboard_log=config["tensorboard_log"],
        device=config["device"],
    )

    return model


def get_training_config(target: str = "6666") -> dict:
    """
    ç²å–é‡å°ç›®æ¨™çš„è¨“ç·´é…ç½®

    Args:
        target: ç›®æ¨™ ("6666" æˆ– "test")

    Returns:
        é…ç½®å­—å…¸
    """
    base_config = {
        # è¨­å‚™
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        # ç¶²çµ¡æ¶æ§‹
        "hidden_dim": 256,
        # å­¸ç¿’åƒæ•¸ (é‡å°é•·æœŸç›®æ¨™å„ªåŒ–)
        "learning_rate": 1e-4,
        "gamma": 0.98,  # ç¨å¾®é™ä½ gammaï¼Œè®“ AI æ›´å°ˆæ³¨æ–¼çœ¼å‰çš„é–ƒé¿ (0.99 -> 0.98)
        "gae_lambda": 0.95,
        # PPO åƒæ•¸
        "clip_range": 0.1,
        "ent_coef": 0.005,
        "vf_coef": 1.0,
        # è¨“ç·´æ•ˆç‡
        "n_steps": 2048,  # æ¢å¾©æ¨™æº–æ­¥æ•¸
        "batch_size": 2048,
        "n_epochs": 10,
        "max_grad_norm": 0.3,
        # æ—¥èªŒ
        "verbose": 1,
        "tensorboard_log": "./logs/tensorboard/",
    }

    if target == "6666":
        # é‡å° 6666 åˆ†çš„é…ç½®
        config_6666 = base_config.copy()
        config_6666.update(
            {
                "learning_rate": (2e-4, 5e-5),  # ç¨å¾®æé«˜åˆå§‹å­¸ç¿’ç‡
                "ent_coef": 0.01,
                "vf_coef": 1.0,
                "n_steps": 2048,
                "batch_size": 4096,
                "n_epochs": 10,
                "hidden_dim": 256,  # ç¸®å°ç¶²çµ¡å®¹é‡ (512 -> 256) ä»¥é¿å…éæ“¬åˆä¸¦åŠ å¿«æ”¶æ–‚
            }
        )
        return config_6666

    elif target == "test":
        # æ¸¬è©¦é…ç½®ï¼ˆå¿«é€Ÿé©—è­‰ï¼‰
        config_test = base_config.copy()
        config_test.update(
            {
                "learning_rate": 2e-4,
                "ent_coef": 0.02,
                "n_steps": 512,
                "batch_size": 512,
                "n_epochs": 6,
                "verbose": 2,
            }
        )
        return config_test

    return base_config


def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Game2048 SB3 è¨“ç·´")
    parser.add_argument("--n-envs", type=int, default=32, help="ä¸¦è¡Œç’°å¢ƒæ•¸é‡")
    parser.add_argument(
        "--total-timesteps", type=int, default=5_000_000, help="ç¸½è¨“ç·´æ­¥æ•¸"
    )
    parser.add_argument(
        "--target", type=str, default="6666", choices=["6666", "test"], help="è¨“ç·´ç›®æ¨™"
    )
    parser.add_argument(
        "--normalize",
        action=BooleanOptionalAction,
        default=True,
        help="å•Ÿç”¨æˆ–åœç”¨ VecNormalize (é è¨­å•Ÿç”¨)",
    )
    parser.add_argument(
        "--norm-path",
        type=str,
        help="VecNormalize çµ±è¨ˆæª”æ¡ˆè·¯å¾‘ï¼ˆå¯ç”¨æ–¼è¼‰å…¥/è¦†å¯«ï¼‰",
    )
    parser.add_argument(
        "--eval-freq",
        type=int,
        default=50_000,
        help="è©•ä¼°é »ç‡ï¼ˆä»¥ timesteps ç‚ºå–®ä½ï¼‰",
    )
    parser.add_argument(
        "--save-freq",
        type=int,
        default=25_000,
        help="æª¢æŸ¥é»ä¿å­˜é »ç‡",
    )
    parser.add_argument("--load", type=str, help="è¼‰å…¥ç¾æœ‰æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")
    parser.add_argument(
        "--finetune-lr",
        type=float,
        help="é‡å°è¼‰å…¥æ¨¡å‹è¦†å¯«å›ºå®šå­¸ç¿’ç‡ (åƒ…åœ¨ --load æ™‚ç”Ÿæ•ˆ)",
    )
    parser.add_argument(
        "--finetune-ent",
        type=float,
        help="é‡å°è¼‰å…¥æ¨¡å‹è¦†å¯«ç†µä¿‚æ•¸ (åƒ…åœ¨ --load æ™‚ç”Ÿæ•ˆ)",
    )
    parser.add_argument(
        "--adaptive-entropy",
        action=BooleanOptionalAction,
        default=True,
        help="å•Ÿç”¨è‡ªé©æ‡‰ç†µå›èª¿ (fine-tune æ™‚å¯åœç”¨)",
    )
    parser.add_argument(
        "--curriculum",
        type=str,
        default="none",
        choices=["none", "progressive"],
        help="æŒ‡å®šè¨“ç·´æ™‚æœŸæœ›ä½¿ç”¨çš„é›£åº¦èª²ç¨‹",
    )
    parser.add_argument(
        "--auto-resume",
        action="store_true",
        help="è‡ªå‹•å˜—è©¦è¼‰å…¥æœ€ä½³æˆ–æœ€æ–°çš„æ¨¡å‹ç¹¼çºŒè¨“ç·´",
    )
    parser.add_argument(
        "--render",
        action="store_true",
        help="å•Ÿç”¨æ¸²æŸ“æ¨¡å¼ (æ³¨æ„ï¼šé€™æœƒé–‹å•Ÿå¤§é‡è¦–çª—ï¼Œåƒ…ç”¨æ–¼èª¿è©¦)",
    )
    parser.add_argument(
        "--target-win-rate",
        type=float,
        help="ç•¶é€šé—œç‡é”åˆ°æ­¤å€¼æ™‚åœæ­¢è¨“ç·´ (ä¾‹å¦‚ 0.9)",
    )

    args = parser.parse_args()

    # è‡ªå‹•æ¢å¾©é‚è¼¯
    if args.auto_resume and not args.load:
        candidates = [
            f"./models/ppo_game2048_{args.target}_final.zip",
            "./best_model/best_model.zip",
        ]
        for path in candidates:
            if os.path.exists(path):
                print(f"ğŸ”„ è‡ªå‹•åµæ¸¬åˆ°ç¾æœ‰æ¨¡å‹ï¼Œæº–å‚™æ¢å¾©è¨“ç·´: {path}")
                args.load = path
                # å˜—è©¦å°‹æ‰¾å°æ‡‰çš„æ­£è¦åŒ–æª”æ¡ˆ
                norm_candidates = [
                    f"./models/vec_normalize_{args.target}.pkl",
                    path.replace(".zip", ".pkl"),
                    os.path.join(
                        os.path.dirname(path), f"vec_normalize_{args.target}.pkl"
                    ),
                ]
                for np_path in norm_candidates:
                    if os.path.exists(np_path):
                        print(f"   â””â”€â”€ ç™¼ç¾æ­£è¦åŒ–çµ±è¨ˆ: {np_path}")
                        args.norm_path = np_path
                        break
                break

    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    print("=" * 60)
    print("ğŸ® Game2048 SB3 è¨“ç·´")
    print(f"ğŸ¯ ç›®æ¨™: {args.target}")
    print(f"ğŸš€ ä¸¦è¡Œç’°å¢ƒ: {args.n_envs}")
    print(f"â±ï¸ ç¸½æ­¥æ•¸: {args.total_timesteps:,}")
    print(f"ğŸ–¥ï¸ è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)

    curriculum_phases = get_curriculum_phases(args.curriculum)
    if curriculum_phases:
        print(f"ğŸ“ˆ å•Ÿç”¨èª²ç¨‹: {args.curriculum} ({len(curriculum_phases)} éšæ®µ)")

    if not args.normalize and args.norm_path:
        print("âš ï¸ å·²åœç”¨ VecNormalizeï¼Œå¿½ç•¥ --norm-path åƒæ•¸ã€‚")

    # å‰µå»ºç’°å¢ƒ
    env = create_envs(
        args.n_envs,
        normalize=args.normalize,
        training=True,
        norm_path=args.norm_path,
        seed=args.seed,
        render_mode="human" if args.render else None,
    )

    # ç²å–é…ç½®
    config = get_training_config(args.target)

    norm_save_path = args.norm_path or f"./models/vec_normalize_{args.target}.pkl"

    # å‰µå»ºæˆ–è¼‰å…¥æ¨¡å‹
    if args.load:
        print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {args.load}")
        model = PPO.load(args.load, env=env)
    else:
        model = create_model(env, config)

    # é…ç½®è‡ªå®šç¾© Logger (æ”¯æ´ä¸­æ–‡èˆ‡å°é½Š)
    log_dir = "./logs/tensorboard/"
    output_formats = [
        ChineseHumanOutputFormat(sys.stdout),
        TensorBoardOutputFormat(log_dir),
        CSVOutputFormat(os.path.join(log_dir, "progress.csv")),
    ]
    custom_logger = ChineseLogger(folder=log_dir, output_formats=output_formats)
    model.set_logger(custom_logger)

    apply_finetune_overrides(model, args.finetune_lr, args.finetune_ent)

    # å‰µå»ºå›èª¿
    callbacks = create_callbacks(
        env,
        normalize=args.normalize,
        eval_freq=args.eval_freq,
        save_freq=args.save_freq,
        norm_path=args.norm_path,
        seed=args.seed,
        adaptive_entropy=args.adaptive_entropy,
        curriculum_phases=curriculum_phases,
        target_win_rate=args.target_win_rate,
    )

    # é–‹å§‹è¨“ç·´ï¼
    print("ğŸ¯ é–‹å§‹è¨“ç·´...")
    print("ğŸ’¡ æç¤º: é–‹å•Ÿ TensorBoard ç›£æ§è¨“ç·´é€²åº¦")
    print("   tensorboard --logdir ./logs/tensorboard/")
    print("-" * 60)

    try:
        # å¦‚æœæ˜¯è¼‰å…¥æ¨¡å‹ï¼Œå‰‡ä¸é‡ç½®æ­¥æ•¸è¨ˆæ•¸å™¨ï¼Œä»¥ä¿æŒ TensorBoard æ›²ç·šé€£çºŒ
        reset_timesteps = not bool(args.load)

        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=reset_timesteps,
        )

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_path = f"./models/ppo_game2048_{args.target}_final.zip"
        os.makedirs(os.path.dirname(final_path), exist_ok=True)
        model.save(final_path)
        print(f"âœ… è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")

        # å¦‚æœä½¿ç”¨ VecNormalizeï¼Œä¿å­˜æ­£è¦åŒ–çµ±è¨ˆ
        if args.normalize and hasattr(env, "save"):
            env.save(norm_save_path)
            print(f"âœ… VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_save_path}")

    except KeyboardInterrupt:
        print("\nâš ï¸ è¨“ç·´è¢«ä¸­æ–·")
        # ä¿å­˜ä¸­é–“çµæœ
        interrupt_path = f"./models/ppo_game2048_{args.target}_interrupted.zip"
        os.makedirs(os.path.dirname(interrupt_path), exist_ok=True)
        model.save(interrupt_path)
        print(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜åˆ°: {interrupt_path}")
        if args.normalize and hasattr(env, "save"):
            env.save(norm_save_path)
            print(f"ğŸ’¾ VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_save_path}")

    finally:
        env.close()


if __name__ == "__main__":
    main()
