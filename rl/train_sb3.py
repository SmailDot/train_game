#!/usr/bin/env python3
"""
Game2048 SB3 è¨“ç·´è…³æœ¬

ä½¿ç”¨ Stable-Baselines3 è¨“ç·´ PPO ä»£ç†ï¼Œç›®æ¨™æ˜¯é”åˆ° 6666 åˆ†é€šé—œã€‚
"""

import argparse
import os
import sys
from argparse import BooleanOptionalAction
from collections import deque
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import (
    BaseCallback,
    CallbackList,
    CheckpointCallback,
    EvalCallback,
)
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecMonitor, VecNormalize

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))


def _import_env():
    from rl.game2048_env import Game2048Env as _Game2048Env

    return _Game2048Env


Game2048Env = _import_env()


def make_linear_schedule(start: float, end: float):
    """Create a linear schedule callable compatible with SB3."""

    def schedule(progress_remaining: float) -> float:
        return end + (start - end) * progress_remaining

    return schedule


def apply_finetune_overrides(
    model: PPO,
    finetune_lr: Optional[float] = None,
    finetune_ent: Optional[float] = None,
) -> None:
    """Optionally override learning rate or entropy for fine-tuning runs."""

    if finetune_lr is not None:
        target_lr = float(finetune_lr)

        def _fixed_lr(_progress_remaining: float) -> float:
            return target_lr

        model.lr_schedule = _fixed_lr
        model.learning_rate = target_lr
        optimizer = getattr(model.policy, "optimizer", None)
        if optimizer is not None:
            for group in optimizer.param_groups:
                group["lr"] = target_lr
        print(f"âš™ï¸ Fine-tune learning rate -> {target_lr:.2e}")

    if finetune_ent is not None:
        target_ent = float(finetune_ent)
        model.ent_coef = target_ent
        print(f"âš™ï¸ Fine-tune entropy coef -> {target_ent:.5f}")


def get_curriculum_phases(name: str) -> List[Dict[str, dict]]:
    if name != "progressive":
        return []

    return [
        {
            "threshold": 0,
            "profile": {
                "ScrollIncreasePerPass": 0.012,
                "MaxScrollSpeed": 3.0,
                "GapShrinkPerPass": 0.4,
            },
        },
        {
            "threshold": 1_500_000,
            "profile": {
                "ScrollIncreasePerPass": 0.018,
                "MaxScrollSpeed": 3.6,
                "GapShrinkPerPass": 0.6,
            },
        },
        {
            "threshold": 3_000_000,
            "profile": {
                "ScrollIncreasePerPass": 0.025,
                "MaxScrollSpeed": 4.2,
                "GapShrinkPerPass": 0.8,
            },
        },
    ]


class WinCallback(BaseCallback):
    """
    è‡ªå®šç¾©å›èª¿ï¼šç›£æ§é€šé—œäº‹ä»¶
    """

    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.wins = 0
        self.best_score = 0

    def _on_step(self) -> bool:
        # æª¢æŸ¥ infos ä¸­æ˜¯å¦æœ‰é€šé—œ
        if hasattr(self.locals, "infos"):
            for info in self.locals["infos"]:
                if info.get("win", False):
                    self.wins += 1
                    score = info.get("episode_score", 0)
                    if score > self.best_score:
                        self.best_score = score
                        if self.verbose > 0:
                            print(f"ğŸ‰ æ–°ç´€éŒ„ï¼åˆ†æ•¸: {score}")

                    if self.verbose > 0:
                        print(f"ğŸ¯ é€šé—œ #{self.wins}ï¼åˆ†æ•¸: {score}")

        return True


class EpisodeStatsCallback(BaseCallback):
    """Record custom environment metrics (e.g., passes, scroll speed)."""

    def __init__(self, prefix: str = "env", verbose: int = 0):
        super().__init__(verbose)
        self.prefix = prefix

    def _on_step(self) -> bool:
        infos = self.locals.get("infos") if hasattr(self, "locals") else None
        if not infos:
            return True

        metrics = {}
        for info in infos:
            if not isinstance(info, dict):
                continue
            for key in ("passed_count", "scroll_speed", "alignment_score"):
                if key in info:
                    metrics.setdefault(key, []).append(info[key])

            # Track binary win flags to compute win rate during training
            if "win" in info:
                metrics.setdefault("win", []).append(float(bool(info["win"])))

        for key, values in metrics.items():
            if not values:
                continue

            if key == "win":
                self.logger.record(f"{self.prefix}/win_rate", float(np.mean(values)))
            else:
                self.logger.record(f"{self.prefix}/{key}", float(np.mean(values)))

        return True


class AdaptiveEntropyCallback(BaseCallback):
    """Dynamically adjust entropy coefficient based on recent win rate."""

    def __init__(
        self,
        window_size: int = 4096,
        low_threshold: float = 0.05,
        high_threshold: float = 0.25,
        increase_step: float = 5e-4,
        decrease_step: float = 3e-4,
        min_ent: float = 0.004,
        max_ent: float = 0.012,
        verbose: int = 0,
    ):
        super().__init__(verbose)
        self.window_size = window_size
        self.low_threshold = low_threshold
        self.high_threshold = high_threshold
        self.increase_step = increase_step
        self.decrease_step = decrease_step
        self.min_ent = min_ent
        self.max_ent = max_ent
        self._buffer: deque[float] = deque(maxlen=window_size)
        self._current_ent: Optional[float] = None

    def _on_training_start(self) -> None:
        # Capture the initial entropy coefficient from the model
        self._current_ent = float(getattr(self.model, "ent_coef", 0.01))
        if self.verbose:
            print(f"ğŸ”§ è‡ªé©æ‡‰ç†µå•Ÿå‹•ï¼Œåˆå§‹ ent_coef = {self._current_ent:.5f}")

    def _set_entropy(self, value: float) -> None:
        if self._current_ent is None or abs(value - self._current_ent) < 1e-6:
            return

        self._current_ent = float(np.clip(value, self.min_ent, self.max_ent))
        self.model.ent_coef = self._current_ent
        # Log to TensorBoard for transparency
        self.logger.record("train/entropy_coef", self._current_ent)
        if self.verbose:
            print(f"âš™ï¸ èª¿æ•´ ent_coef -> {self._current_ent:.5f}")

    def _on_step(self) -> bool:
        infos = self.locals.get("infos") if hasattr(self, "locals") else None
        if not infos:
            return True

        win_flag = 1.0 if any(info.get("win", False) for info in infos) else 0.0
        self._buffer.append(win_flag)

        if len(self._buffer) < self.window_size:
            return True

        win_rate = float(np.mean(self._buffer))

        if win_rate >= self.high_threshold:
            self._set_entropy(self._current_ent - self.decrease_step)
        elif win_rate <= self.low_threshold:
            self._set_entropy(self._current_ent + self.increase_step)

        return True


class CurriculumCallback(BaseCallback):
    """Gradually ramp environment difficulty according to predefined phases."""

    def __init__(self, phases: List[Dict[str, dict]], verbose: int = 0):
        super().__init__(verbose)
        self.phases = sorted(phases, key=lambda item: item["threshold"])
        self._current_phase = -1

    def _apply_phase(self, index: int) -> None:
        if index < 0 or index >= len(self.phases):
            return

        profile = self.phases[index]["profile"]
        threshold = self.phases[index]["threshold"]
        self._current_phase = index

        if self.verbose:
            print(
                "ğŸ“ˆ èª²ç¨‹éšæ®µ"
                f" {index + 1}/{len(self.phases)} @ step {threshold:,}: {profile}"
            )

        env_method = getattr(self.training_env, "env_method", None)
        if env_method is not None:
            try:
                env_method("apply_difficulty_profile", profile)
            except AttributeError:
                if self.verbose:
                    print("âš ï¸ ç„¡æ³•å¥—ç”¨èª²ç¨‹è¨­å®šï¼Œç’°å¢ƒç¼ºå°‘ apply_difficulty_profileã€‚")

    def _on_training_start(self) -> None:
        if self.phases:
            self._apply_phase(0)

    def _on_step(self) -> bool:
        if not self.phases:
            return True

        next_index = self._current_phase + 1
        if (
            next_index < len(self.phases)
            and self.num_timesteps >= self.phases[next_index]["threshold"]
        ):
            self._apply_phase(next_index)

        return True


def create_envs(
    n_envs: int = 32,
    normalize: bool = True,
    training: bool = True,
    norm_path: Optional[str] = None,
    seed: int = 42,
    render_mode: Optional[str] = None,
):
    """
    å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ

    Args:
        n_envs: ç’°å¢ƒæ•¸é‡
        normalize: æ˜¯å¦ä½¿ç”¨ VecNormalize

    Returns:
        ç’°å¢ƒå¯¦ä¾‹
    """
    print(f"ğŸš€ å‰µå»º {n_envs} å€‹ä¸¦è¡Œç’°å¢ƒ...")

    env_kwargs = {}
    if render_mode:
        env_kwargs["render_mode"] = render_mode

    vec_env = make_vec_env(Game2048Env, n_envs=n_envs, env_kwargs=env_kwargs, seed=seed)

    # æ·»åŠ ç›£æ§
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    vec_env = VecMonitor(vec_env, log_dir)

    # å¯é¸ï¼šæ·»åŠ æ­£è¦åŒ–
    if normalize:
        norm_reward = training
        if norm_path and os.path.exists(norm_path):
            vec_env = VecNormalize.load(norm_path, vec_env)
            print(f"ğŸ“„ VecNormalize çµ±è¨ˆè¼‰å…¥: {norm_path}")
        else:
            if norm_path and not os.path.exists(norm_path):
                print(f"âš ï¸ æ‰¾ä¸åˆ° VecNormalize æª”æ¡ˆ {norm_path}ï¼Œå°‡é‡æ–°åˆå§‹åŒ–çµ±è¨ˆã€‚")
            vec_env = VecNormalize(
                vec_env,
                norm_obs=True,
                norm_reward=norm_reward,
                clip_obs=10.0,
                clip_reward=10.0,
                gamma=0.995,
            )

        vec_env.training = training
        vec_env.norm_reward = norm_reward

    if render_mode:
        setattr(vec_env, "render_mode", render_mode)

    return vec_env


def create_callbacks(
    env,
    normalize: bool = False,
    eval_freq: int = 5000,
    save_freq: int = 10000,
    norm_path: Optional[str] = None,
    seed: int = 42,
    adaptive_entropy: bool = True,
    curriculum_phases: Optional[List[Dict[str, dict]]] = None,
):
    """å»ºç«‹è¨“ç·´/è©•ä¼°æ‰€éœ€çš„å›èª¿ã€‚"""

    callbacks = []

    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path="./checkpoints/",
        name_prefix="ppo_game2048",
        save_replay_buffer=True,
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)

    eval_env = create_envs(
        n_envs=4,
        normalize=normalize,
        training=False,
        norm_path=norm_path,
        seed=seed + 1,
        render_mode=None,
    )

    if (
        normalize
        and isinstance(env, VecNormalize)
        and isinstance(eval_env, VecNormalize)
    ):
        eval_env.obs_rms = env.obs_rms.copy()
        eval_env.ret_rms = env.ret_rms.copy()
        eval_env.training = False
        eval_env.norm_reward = False

    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/eval/",
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        verbose=1,
    )
    callbacks.append(eval_callback)

    callbacks.append(WinCallback(verbose=1))
    callbacks.append(EpisodeStatsCallback(verbose=0))
    if curriculum_phases:
        callbacks.append(CurriculumCallback(curriculum_phases, verbose=1))
    if adaptive_entropy:
        callbacks.append(AdaptiveEntropyCallback(verbose=0))

    return CallbackList(callbacks)


def create_model(env, config: dict):
    """
    å‰µå»º PPO æ¨¡å‹

    Args:
        env: ç’°å¢ƒ
        config: é…ç½®å­—å…¸

    Returns:
        PPO æ¨¡å‹
    """
    print("ğŸ§  å‰µå»º PPO æ¨¡å‹...")

    learning_rate = config["learning_rate"]
    if isinstance(learning_rate, (tuple, list)) and len(learning_rate) == 2:
        learning_rate = make_linear_schedule(learning_rate[0], learning_rate[1])

    # ç¶²çµ¡æ¶æ§‹é…ç½®
    policy_kwargs = dict(
        net_arch=dict(
            pi=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Actor ç¶²çµ¡
            vf=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Critic ç¶²çµ¡
        ),
        activation_fn=torch.nn.ReLU,
    )

    # å‰µå»ºæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        policy_kwargs=policy_kwargs,
        # å­¸ç¿’åƒæ•¸
        learning_rate=learning_rate,
        gamma=config["gamma"],
        gae_lambda=config["gae_lambda"],
        # PPO åƒæ•¸
        clip_range=config["clip_range"],
        ent_coef=config["ent_coef"],
        vf_coef=config["vf_coef"],
        # è¨“ç·´æ•ˆç‡
        n_steps=config["n_steps"],
        batch_size=config["batch_size"],
        n_epochs=config["n_epochs"],
        max_grad_norm=config["max_grad_norm"],
        # æ—¥èªŒå’Œè¨­å‚™
        verbose=config["verbose"],
        tensorboard_log=config["tensorboard_log"],
        device=config["device"],
    )

    return model


def get_training_config(target: str = "6666") -> dict:
    """
    ç²å–é‡å°ç›®æ¨™çš„è¨“ç·´é…ç½®

    Args:
        target: ç›®æ¨™ ("6666" æˆ– "test")

    Returns:
        é…ç½®å­—å…¸
    """
    base_config = {
        # è¨­å‚™
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        # ç¶²çµ¡æ¶æ§‹
        "hidden_dim": 256,
        # å­¸ç¿’åƒæ•¸ (é‡å°é•·æœŸç›®æ¨™å„ªåŒ–)
        "learning_rate": 1e-4,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        # PPO åƒæ•¸
        "clip_range": 0.1,
        "ent_coef": 0.005,
        "vf_coef": 1.0,
        # è¨“ç·´æ•ˆç‡
        "n_steps": 1024,
        "batch_size": 2048,
        "n_epochs": 10,
        "max_grad_norm": 0.3,
        # æ—¥èªŒ
        "verbose": 1,
        "tensorboard_log": "./logs/tensorboard/",
    }

    if target == "6666":
        # é‡å° 6666 åˆ†çš„é…ç½®
        config_6666 = base_config.copy()
        config_6666.update(
            {
                "learning_rate": (2e-4, 5e-5),  # ç¨å¾®æé«˜åˆå§‹å­¸ç¿’ç‡
                "ent_coef": 0.01,
                "vf_coef": 1.0,
                "n_steps": 4096,  # å¢åŠ  n_steps è®“æ¯æ¬¡æ›´æ–°çœ‹åˆ°æ›´é•·è»Œè·¡ (2048 -> 4096)
                "batch_size": 8192,  # å¢åŠ  batch_size (4096 -> 8192)
                "n_epochs": 10,
                "hidden_dim": 512,  # å¢åŠ ç¶²çµ¡å®¹é‡ (256 -> 512)
            }
        )
        return config_6666

    elif target == "test":
        # æ¸¬è©¦é…ç½®ï¼ˆå¿«é€Ÿé©—è­‰ï¼‰
        config_test = base_config.copy()
        config_test.update(
            {
                "learning_rate": 2e-4,
                "ent_coef": 0.02,
                "n_steps": 512,
                "batch_size": 512,
                "n_epochs": 6,
                "verbose": 2,
            }
        )
        return config_test

    return base_config


def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Game2048 SB3 è¨“ç·´")
    parser.add_argument("--n-envs", type=int, default=32, help="ä¸¦è¡Œç’°å¢ƒæ•¸é‡")
    parser.add_argument(
        "--total-timesteps", type=int, default=5_000_000, help="ç¸½è¨“ç·´æ­¥æ•¸"
    )
    parser.add_argument(
        "--target", type=str, default="6666", choices=["6666", "test"], help="è¨“ç·´ç›®æ¨™"
    )
    parser.add_argument(
        "--normalize",
        action=BooleanOptionalAction,
        default=True,
        help="å•Ÿç”¨æˆ–åœç”¨ VecNormalize (é è¨­å•Ÿç”¨)",
    )
    parser.add_argument(
        "--norm-path",
        type=str,
        help="VecNormalize çµ±è¨ˆæª”æ¡ˆè·¯å¾‘ï¼ˆå¯ç”¨æ–¼è¼‰å…¥/è¦†å¯«ï¼‰",
    )
    parser.add_argument(
        "--eval-freq",
        type=int,
        default=50_000,
        help="è©•ä¼°é »ç‡ï¼ˆä»¥ timesteps ç‚ºå–®ä½ï¼‰",
    )
    parser.add_argument(
        "--save-freq",
        type=int,
        default=25_000,
        help="æª¢æŸ¥é»ä¿å­˜é »ç‡",
    )
    parser.add_argument("--load", type=str, help="è¼‰å…¥ç¾æœ‰æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")
    parser.add_argument(
        "--finetune-lr",
        type=float,
        help="é‡å°è¼‰å…¥æ¨¡å‹è¦†å¯«å›ºå®šå­¸ç¿’ç‡ (åƒ…åœ¨ --load æ™‚ç”Ÿæ•ˆ)",
    )
    parser.add_argument(
        "--finetune-ent",
        type=float,
        help="é‡å°è¼‰å…¥æ¨¡å‹è¦†å¯«ç†µä¿‚æ•¸ (åƒ…åœ¨ --load æ™‚ç”Ÿæ•ˆ)",
    )
    parser.add_argument(
        "--adaptive-entropy",
        action=BooleanOptionalAction,
        default=True,
        help="å•Ÿç”¨è‡ªé©æ‡‰ç†µå›èª¿ (fine-tune æ™‚å¯åœç”¨)",
    )
    parser.add_argument(
        "--curriculum",
        type=str,
        default="none",
        choices=["none", "progressive"],
        help="æŒ‡å®šè¨“ç·´æ™‚æœŸæœ›ä½¿ç”¨çš„é›£åº¦èª²ç¨‹",
    )
    parser.add_argument(
        "--auto-resume",
        action="store_true",
        help="è‡ªå‹•å˜—è©¦è¼‰å…¥æœ€ä½³æˆ–æœ€æ–°çš„æ¨¡å‹ç¹¼çºŒè¨“ç·´",
    )

    args = parser.parse_args()

    # è‡ªå‹•æ¢å¾©é‚è¼¯
    if args.auto_resume and not args.load:
        candidates = [
            f"./models/ppo_game2048_{args.target}_final.zip",
            "./best_model/best_model.zip",
        ]
        for path in candidates:
            if os.path.exists(path):
                print(f"ğŸ”„ è‡ªå‹•åµæ¸¬åˆ°ç¾æœ‰æ¨¡å‹ï¼Œæº–å‚™æ¢å¾©è¨“ç·´: {path}")
                args.load = path
                # å˜—è©¦å°‹æ‰¾å°æ‡‰çš„æ­£è¦åŒ–æª”æ¡ˆ
                norm_candidates = [
                    f"./models/vec_normalize_{args.target}.pkl",
                    path.replace(".zip", ".pkl"),
                    os.path.join(
                        os.path.dirname(path), f"vec_normalize_{args.target}.pkl"
                    ),
                ]
                for np_path in norm_candidates:
                    if os.path.exists(np_path):
                        print(f"   â””â”€â”€ ç™¼ç¾æ­£è¦åŒ–çµ±è¨ˆ: {np_path}")
                        args.norm_path = np_path
                        break
                break

    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    print("=" * 60)
    print("ğŸ® Game2048 SB3 è¨“ç·´")
    print(f"ğŸ¯ ç›®æ¨™: {args.target}")
    print(f"ğŸš€ ä¸¦è¡Œç’°å¢ƒ: {args.n_envs}")
    print(f"â±ï¸ ç¸½æ­¥æ•¸: {args.total_timesteps:,}")
    print(f"ğŸ–¥ï¸ è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)

    curriculum_phases = get_curriculum_phases(args.curriculum)
    if curriculum_phases:
        print(f"ğŸ“ˆ å•Ÿç”¨èª²ç¨‹: {args.curriculum} ({len(curriculum_phases)} éšæ®µ)")

    if not args.normalize and args.norm_path:
        print("âš ï¸ å·²åœç”¨ VecNormalizeï¼Œå¿½ç•¥ --norm-path åƒæ•¸ã€‚")

    # å‰µå»ºç’°å¢ƒ
    env = create_envs(
        args.n_envs,
        normalize=args.normalize,
        training=True,
        norm_path=args.norm_path,
        seed=args.seed,
    )

    # ç²å–é…ç½®
    config = get_training_config(args.target)

    norm_save_path = args.norm_path or f"./models/vec_normalize_{args.target}.pkl"

    # å‰µå»ºæˆ–è¼‰å…¥æ¨¡å‹
    if args.load:
        print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {args.load}")
        model = PPO.load(args.load, env=env)
    else:
        model = create_model(env, config)

    apply_finetune_overrides(model, args.finetune_lr, args.finetune_ent)

    # å‰µå»ºå›èª¿
    callbacks = create_callbacks(
        env,
        normalize=args.normalize,
        eval_freq=args.eval_freq,
        save_freq=args.save_freq,
        norm_path=args.norm_path,
        seed=args.seed,
        adaptive_entropy=args.adaptive_entropy,
        curriculum_phases=curriculum_phases,
    )

    # é–‹å§‹è¨“ç·´ï¼
    print("ğŸ¯ é–‹å§‹è¨“ç·´...")
    print("ğŸ’¡ æç¤º: é–‹å•Ÿ TensorBoard ç›£æ§è¨“ç·´é€²åº¦")
    print("   tensorboard --logdir ./logs/tensorboard/")
    print("-" * 60)

    try:
        # å¦‚æœæ˜¯è¼‰å…¥æ¨¡å‹ï¼Œå‰‡ä¸é‡ç½®æ­¥æ•¸è¨ˆæ•¸å™¨ï¼Œä»¥ä¿æŒ TensorBoard æ›²ç·šé€£çºŒ
        reset_timesteps = not bool(args.load)

        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=reset_timesteps,
        )

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_path = f"./models/ppo_game2048_{args.target}_final.zip"
        os.makedirs(os.path.dirname(final_path), exist_ok=True)
        model.save(final_path)
        print(f"âœ… è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")

        # å¦‚æœä½¿ç”¨ VecNormalizeï¼Œä¿å­˜æ­£è¦åŒ–çµ±è¨ˆ
        if args.normalize and hasattr(env, "save"):
            env.save(norm_save_path)
            print(f"âœ… VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_save_path}")

    except KeyboardInterrupt:
        print("\nâš ï¸ è¨“ç·´è¢«ä¸­æ–·")
        # ä¿å­˜ä¸­é–“çµæœ
        interrupt_path = f"./models/ppo_game2048_{args.target}_interrupted.zip"
        os.makedirs(os.path.dirname(interrupt_path), exist_ok=True)
        model.save(interrupt_path)
        print(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜åˆ°: {interrupt_path}")
        if args.normalize and hasattr(env, "save"):
            env.save(norm_save_path)
            print(f"ğŸ’¾ VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_save_path}")

    finally:
        env.close()


if __name__ == "__main__":
    main()
