#!/usr/bin/env python3
"""
Game2048 SB3 è¨“ç·´è…³æœ¬

ä½¿ç”¨ Stable-Baselines3 è¨“ç·´ PPO ä»£ç†ï¼Œç›®æ¨™æ˜¯é”åˆ° 6666 åˆ†é€šé—œã€‚
"""

import argparse
import os
import sys
from pathlib import Path

import numpy as np

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import (
    BaseCallback,
    CallbackList,
    CheckpointCallback,
    EvalCallback,
)
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecMonitor, VecNormalize

from rl.game2048_env import Game2048Env


class WinCallback(BaseCallback):
    """
    è‡ªå®šç¾©å›èª¿ï¼šç›£æ§é€šé—œäº‹ä»¶
    """

    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.wins = 0
        self.best_score = 0

    def _on_step(self) -> bool:
        # æª¢æŸ¥ infos ä¸­æ˜¯å¦æœ‰é€šé—œ
        if hasattr(self.locals, "infos"):
            for info in self.locals["infos"]:
                if info.get("win", False):
                    self.wins += 1
                    score = info.get("episode_score", 0)
                    if score > self.best_score:
                        self.best_score = score
                        if self.verbose > 0:
                            print(f"ğŸ‰ æ–°ç´€éŒ„ï¼åˆ†æ•¸: {score}")

                    if self.verbose > 0:
                        print(f"ğŸ¯ é€šé—œ #{self.wins}ï¼åˆ†æ•¸: {score}")

        return True


def create_envs(n_envs: int = 32, normalize: bool = True):
    """
    å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ

    Args:
        n_envs: ç’°å¢ƒæ•¸é‡
        normalize: æ˜¯å¦ä½¿ç”¨ VecNormalize

    Returns:
        ç’°å¢ƒå¯¦ä¾‹
    """
    print(f"ğŸš€ å‰µå»º {n_envs} å€‹ä¸¦è¡Œç’°å¢ƒ...")

    # å‰µå»ºåŸºç¤ç’°å¢ƒ
    vec_env = make_vec_env(Game2048Env, n_envs=n_envs, env_kwargs={}, seed=42)

    # æ·»åŠ ç›£æ§
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    vec_env = VecMonitor(vec_env, log_dir)

    # å¯é¸ï¼šæ·»åŠ æ­£è¦åŒ–
    if normalize:
        vec_env = VecNormalize(
            vec_env,
            norm_obs=True,
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0,
            gamma=0.995,
        )

    return vec_env


def create_callbacks(eval_freq: int = 5000, save_freq: int = 10000):
    """
    å‰µå»ºè¨“ç·´å›èª¿

    Args:
        eval_freq: è©•ä¼°é »ç‡
        save_freq: ä¿å­˜é »ç‡

    Returns:
        å›èª¿åˆ—è¡¨
    """
    callbacks = []

    # æª¢æŸ¥é»å›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path="./checkpoints/",
        name_prefix="ppo_game2048",
        save_replay_buffer=True,
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)

    # è©•ä¼°å›èª¿
    eval_env = make_vec_env(Game2048Env, n_envs=4)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/eval/",
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        verbose=1,
    )
    callbacks.append(eval_callback)

    # é€šé—œç›£æ§å›èª¿
    win_callback = WinCallback(verbose=1)
    callbacks.append(win_callback)

    return CallbackList(callbacks)


def create_model(env, config: dict):
    """
    å‰µå»º PPO æ¨¡å‹

    Args:
        env: ç’°å¢ƒ
        config: é…ç½®å­—å…¸

    Returns:
        PPO æ¨¡å‹
    """
    print("ğŸ§  å‰µå»º PPO æ¨¡å‹...")

    # ç¶²çµ¡æ¶æ§‹é…ç½®
    policy_kwargs = dict(
        net_arch=dict(
            pi=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Actor ç¶²çµ¡
            vf=[
                config["hidden_dim"],
                config["hidden_dim"],
                config["hidden_dim"],
            ],  # Critic ç¶²çµ¡
        ),
        activation_fn=torch.nn.ReLU,
    )

    # å‰µå»ºæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,
        policy_kwargs=policy_kwargs,
        # å­¸ç¿’åƒæ•¸
        learning_rate=config["learning_rate"],
        gamma=config["gamma"],
        gae_lambda=config["gae_lambda"],
        # PPO åƒæ•¸
        clip_range=config["clip_range"],
        ent_coef=config["ent_coef"],
        vf_coef=config["vf_coef"],
        # è¨“ç·´æ•ˆç‡
        n_steps=config["n_steps"],
        batch_size=config["batch_size"],
        n_epochs=config["n_epochs"],
        max_grad_norm=config["max_grad_norm"],
        # æ—¥èªŒå’Œè¨­å‚™
        verbose=config["verbose"],
        tensorboard_log=config["tensorboard_log"],
        device=config["device"],
    )

    return model


def get_training_config(target: str = "6666") -> dict:
    """
    ç²å–é‡å°ç›®æ¨™çš„è¨“ç·´é…ç½®

    Args:
        target: ç›®æ¨™ ("6666" æˆ– "test")

    Returns:
        é…ç½®å­—å…¸
    """
    base_config = {
        # è¨­å‚™
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        # ç¶²çµ¡æ¶æ§‹
        "hidden_dim": 256,
        # å­¸ç¿’åƒæ•¸ (é‡å°é•·æœŸç›®æ¨™å„ªåŒ–)
        "learning_rate": 5e-5,  # ç©©å®šä½†ä¸å¤ªæ…¢
        "gamma": 0.995,  # é«˜æŠ˜æ‰£å› å­ï¼ˆé‡è¦–é•·æœŸçå‹µï¼‰
        "gae_lambda": 0.97,  # é«˜ GAE lambda
        # PPO åƒæ•¸
        "clip_range": 0.15,  # é©ä¸­çš„ clip ç¯„åœ
        "ent_coef": 0.05,  # é«˜ entropyï¼ˆæ¢ç´¢ï¼‰
        "vf_coef": 1.5,  # å¼· critic è¨“ç·´
        # è¨“ç·´æ•ˆç‡
        "n_steps": 2048,  # æ¯å€‹ç’°å¢ƒæ”¶é›† 2048 æ­¥
        "batch_size": 512,  # å¤§ batch size
        "n_epochs": 15,  # æ¯æ¬¡æ›´æ–° 15 è¼ª
        "max_grad_norm": 0.5,
        # æ—¥èªŒ
        "verbose": 1,
        "tensorboard_log": "./logs/tensorboard/",
    }

    if target == "6666":
        # é‡å° 6666 åˆ†çš„é…ç½®
        config_6666 = base_config.copy()
        config_6666.update(
            {
                "learning_rate": 3e-5,  # æ›´æ…¢ä½†æ›´ç©©å®š
                "ent_coef": 0.03,  # ç¨å¾®æ¸›å°‘æ¢ç´¢
                "vf_coef": 2.0,  # æ›´å¼·çš„ critic
                "n_steps": 4096,  # æ”¶é›†æ›´å¤šæ•¸æ“š
                "batch_size": 1024,  # æ›´å¤§çš„ batch
                "n_epochs": 20,  # æ›´å¤šæ›´æ–°è¼ªæ¬¡
            }
        )
        return config_6666

    elif target == "test":
        # æ¸¬è©¦é…ç½®ï¼ˆå¿«é€Ÿé©—è­‰ï¼‰
        config_test = base_config.copy()
        config_test.update(
            {
                "learning_rate": 1e-4,  # æ›´å¿«å­¸ç¿’
                "ent_coef": 0.1,  # æ›´å¤šæ¢ç´¢
                "n_steps": 1024,  # å°‘é‡æ•¸æ“š
                "batch_size": 256,  # å° batch
                "n_epochs": 5,  # å°‘é‡æ›´æ–°
                "verbose": 2,  # æ›´å¤šè¼¸å‡º
            }
        )
        return config_test

    return base_config


def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Game2048 SB3 è¨“ç·´")
    parser.add_argument("--n-envs", type=int, default=32, help="ä¸¦è¡Œç’°å¢ƒæ•¸é‡")
    parser.add_argument(
        "--total-timesteps", type=int, default=5_000_000, help="ç¸½è¨“ç·´æ­¥æ•¸"
    )
    parser.add_argument(
        "--target", type=str, default="6666", choices=["6666", "test"], help="è¨“ç·´ç›®æ¨™"
    )
    parser.add_argument("--normalize", action="store_true", help="ä½¿ç”¨ VecNormalize")
    parser.add_argument("--load", type=str, help="è¼‰å…¥ç¾æœ‰æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")

    args = parser.parse_args()

    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    print("=" * 60)
    print("ğŸ® Game2048 SB3 è¨“ç·´")
    print(f"ğŸ¯ ç›®æ¨™: {args.target}")
    print(f"ğŸš€ ä¸¦è¡Œç’°å¢ƒ: {args.n_envs}")
    print(f"â±ï¸ ç¸½æ­¥æ•¸: {args.total_timesteps:,}")
    print(f"ğŸ–¥ï¸ è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)

    # å‰µå»ºç’°å¢ƒ
    env = create_envs(args.n_envs, args.normalize)

    # ç²å–é…ç½®
    config = get_training_config(args.target)

    # å‰µå»ºæˆ–è¼‰å…¥æ¨¡å‹
    if args.load:
        print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {args.load}")
        model = PPO.load(args.load, env=env)
    else:
        model = create_model(env, config)

    # å‰µå»ºå›èª¿
    callbacks = create_callbacks()

    # é–‹å§‹è¨“ç·´ï¼
    print("ğŸ¯ é–‹å§‹è¨“ç·´...")
    print("ğŸ’¡ æç¤º: é–‹å•Ÿ TensorBoard ç›£æ§è¨“ç·´é€²åº¦")
    print("   tensorboard --logdir ./logs/tensorboard/")
    print("-" * 60)

    try:
        model.learn(
            total_timesteps=args.total_timesteps, callback=callbacks, progress_bar=True
        )

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_path = f"./models/ppo_game2048_{args.target}_final.zip"
        os.makedirs(os.path.dirname(final_path), exist_ok=True)
        model.save(final_path)
        print(f"âœ… è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")

        # å¦‚æœä½¿ç”¨ VecNormalizeï¼Œä¿å­˜æ­£è¦åŒ–çµ±è¨ˆ
        if args.normalize and hasattr(env, "save"):
            norm_path = f"./models/vec_normalize_{args.target}.pkl"
            env.save(norm_path)
            print(f"âœ… VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_path}")

    except KeyboardInterrupt:
        print("\nâš ï¸ è¨“ç·´è¢«ä¸­æ–·")
        # ä¿å­˜ä¸­é–“çµæœ
        interrupt_path = f"./models/ppo_game2048_{args.target}_interrupted.zip"
        os.makedirs(os.path.dirname(interrupt_path), exist_ok=True)
        model.save(interrupt_path)
        print(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜åˆ°: {interrupt_path}")

    finally:
        env.close()


if __name__ == "__main__":
    main()
