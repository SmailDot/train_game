"""PPO trainer using PyTorch.

This file implements a compact, readable PPO training loop with checkpointing
and TensorBoard logging. It expects `agents.networks.ActorCritic` to be a
PyTorch nn.Module (the file provides a fallback but for training you must
install torch).

Notes:
- This implementation is intentionally clear rather than highly-optimized.
- For faster training use vectorized envs (multiprocessing) and larger batch sizes.
"""

import json
import os
from pathlib import Path

import numpy as np

try:
    import torch
    import torch.nn.functional as F
    from torch.utils.tensorboard import SummaryWriter

    from agents.networks import ActorCritic
    from agents.ppo_agent import PPOAgent
    from game.environment import GameEnv

    class PPOTrainer:
        def __init__(
            self,
            save_dir="checkpoints",
            lr=3e-4,
            gamma=0.99,
            lam=0.95,
            clip_eps=0.2,
            vf_coef=0.5,
            ent_coef=0.05,
            batch_size=64,
            ppo_epochs=4,
            device=None,
        ):
            self.device = device or (
                torch.device("cuda")
                if torch.cuda.is_available()
                else torch.device("cpu")
            )
            self.net = ActorCritic().to(self.device)

            # å­˜å„²åˆå§‹åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ›´æ–°ï¼‰
            self.lr = lr
            self.gamma = gamma
            self.lam = lam
            self.clip_eps = clip_eps
            self.vf_coef = vf_coef
            self.ent_coef = ent_coef
            self.batch_size = batch_size
            self.ppo_epochs = ppo_epochs

            self.opt = torch.optim.Adam(self.net.parameters(), lr=lr)

            # é…ç½®æ–‡ä»¶è·¯å¾‘
            self.config_path = Path(__file__).parent.parent / "training_config.json"
            self._last_config_check = 0

            # å­¸ç¿’ç‡èª¿åº¦å™¨é…ç½®
            self.initial_lr = lr
            self.scheduler_config = self._load_scheduler_config()
            self._setup_lr_scheduler()

            print(f"ğŸ’¾ é…ç½®æ–‡ä»¶è·¯å¾‘: {self.config_path}")
            print("   å¯åœ¨è¨“ç·´éç¨‹ä¸­ä¿®æ”¹æ­¤æ–‡ä»¶ä¾†èª¿æ•´åƒæ•¸")
            print(f"ğŸ¯ å­¸ç¿’ç‡èª¿åº¦å™¨: {self.scheduler_config.get('type', 'none')}")
            self.save_dir = save_dir
            os.makedirs(save_dir, exist_ok=True)
            self.writer = SummaryWriter(log_dir=os.path.join(save_dir, "tb"))

        def _load_scheduler_config(self):
            """å¾é…ç½®æ–‡ä»¶åŠ è¼‰å­¸ç¿’ç‡èª¿åº¦å™¨è¨­ç½®"""
            try:
                if self.config_path.exists():
                    with open(self.config_path, "r", encoding="utf-8") as f:
                        config = json.load(f)
                    return config.get("lr_scheduler", {"type": "none"})
            except Exception:
                pass
            return {"type": "none"}

        def _setup_lr_scheduler(self):
            """è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨"""
            scheduler_type = self.scheduler_config.get("type", "none")

            # æ€§èƒ½è¿½è¹¤ï¼ˆç”¨æ–¼è‡ªé©æ‡‰èª¿åº¦ï¼‰
            self.best_reward = float("-inf")
            self.best_max_reward = float("-inf")  # è¿½è¹¤æœ€é«˜å–®å›åˆåˆ†æ•¸
            self.best_min_reward = float("-inf")  # è¿½è¹¤æœ€å¥½çš„æœ€ä½åˆ†ï¼ˆä¸‹é™æå‡ï¼‰
            self.patience_counter = 0
            self.lr_history = [self.initial_lr]

            if scheduler_type == "step":
                # éšæ¢¯å¼è¡°æ¸›ï¼šæ¯ N å€‹è¿­ä»£é™ä½å­¸ç¿’ç‡
                step_size = self.scheduler_config.get("step_size", 100)
                gamma = self.scheduler_config.get("gamma", 0.9)
                self.lr_scheduler = torch.optim.lr_scheduler.StepLR(
                    self.opt, step_size=step_size, gamma=gamma
                )
                print(f"   æ¯ {step_size} è¿­ä»£å­¸ç¿’ç‡ Ã—{gamma} (éšæ¢¯å¼è¡°æ¸›)")

            elif scheduler_type == "exponential":
                # æŒ‡æ•¸è¡°æ¸›ï¼šæ¯å€‹è¿­ä»£éƒ½è¡°æ¸›
                gamma = self.scheduler_config.get("gamma", 0.999)
                self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
                    self.opt, gamma=gamma
                )
                print(f"   æ¯è¿­ä»£å­¸ç¿’ç‡ Ã—{gamma} (æŒ‡æ•¸è¡°æ¸›)")

            elif scheduler_type == "reduce_on_plateau":
                # åŸºæ–¼æ€§èƒ½ï¼šçå‹µåœæ»¯æ™‚é™ä½å­¸ç¿’ç‡
                patience = self.scheduler_config.get("patience", 20)
                factor = self.scheduler_config.get("factor", 0.5)
                self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    self.opt,
                    mode="max",
                    factor=factor,
                    patience=patience,
                    verbose=True,
                )
                print(f"   çå‹µåœæ»¯ {patience} æ¬¡å¾Œå­¸ç¿’ç‡ Ã—{factor} (æ€§èƒ½è‡ªé©æ‡‰)")

            elif scheduler_type == "cosine":
                # é¤˜å¼¦é€€ç«ï¼šå¹³æ»‘è¡°æ¸›åˆ°æœ€å°å€¼
                T_max = self.scheduler_config.get("T_max", 500)
                eta_min = self.scheduler_config.get("eta_min", 1e-6)
                self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                    self.opt, T_max=T_max, eta_min=eta_min
                )
                print(f"   {T_max} è¿­ä»£å…§é¤˜å¼¦è¡°æ¸›è‡³ {eta_min} (é¤˜å¼¦é€€ç«)")

            elif scheduler_type == "adaptive":
                # è‡ªå®šç¾©è‡ªé©æ‡‰ç­–ç•¥ï¼ˆä¸ä½¿ç”¨ PyTorch å…§å»ºï¼‰
                self.lr_scheduler = None
                patience = self.scheduler_config.get("patience", 30)
                factor = self.scheduler_config.get("factor", 0.5)
                min_lr = self.scheduler_config.get("min_lr", 1e-6)
                print(
                    f"   è‡ªé©æ‡‰èª¿æ•´ï¼š{patience}æ¬¡ç„¡æ”¹å–„â†’å­¸ç¿’ç‡Ã—{factor}ï¼Œæœ€ä½{min_lr}"
                )

            else:
                # ä¸ä½¿ç”¨èª¿åº¦å™¨
                self.lr_scheduler = None
                print("   ä¸ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦")

        def _update_lr_adaptive(self, mean_reward, max_reward, min_reward, iteration):
            """è‡ªå®šç¾©è‡ªé©æ‡‰å­¸ç¿’ç‡æ›´æ–°é‚è¼¯ï¼ˆä¸‰æŒ‡æ¨™ç³»çµ±ï¼‰

            Args:
                mean_reward: å¹³å‡çå‹µï¼ˆè©•ä¼°æ•´é«”ç©©å®šæ€§ï¼‰
                max_reward: æœ€é«˜å–®å›åˆçå‹µï¼ˆè©•ä¼°æ½›åŠ›ä¸Šé™ï¼‰
                min_reward: æœ€ä½å–®å›åˆçå‹µï¼ˆè©•ä¼°ç©©å®šæ€§ä¸‹é™ï¼‰
                iteration: ç•¶å‰è¿­ä»£æ•¸

            ç­–ç•¥ï¼š
                - å¹³å‡åˆ†æå‡ â†’ æ•´é«”é€²æ­¥ï¼Œé‡ç½® patience
                - æœ€é«˜åˆ†çªç ´ â†’ ç™¼ç¾æ½›åŠ›ï¼Œæ¸›å°‘ patienceï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
                - æœ€ä½åˆ†æå‡ â†’ ä¸‹é™æ”¹å–„ï¼Œæ¸›å°‘ patienceï¼ˆç©©å®šæ€§æå‡ï¼‰
                - æœ€ä½åˆ†æƒ¡åŒ– â†’ å¢åŠ  patienceï¼ˆè­¦å‘Šï¼šç­–ç•¥ä¸ç©©å®šï¼‰
            """
            if self.scheduler_config.get("type") != "adaptive":
                return

            if mean_reward is None:
                return

            patience = self.scheduler_config.get("patience", 30)
            factor = self.scheduler_config.get("factor", 0.5)
            min_lr = self.scheduler_config.get("min_lr", 1e-6)
            improvement_threshold = self.scheduler_config.get(
                "improvement_threshold", 0.01
            )

            # æª¢æŸ¥ä¸‰å€‹æŒ‡æ¨™çš„æ”¹å–„æƒ…æ³
            mean_improved = mean_reward > self.best_reward * (1 + improvement_threshold)
            max_improved = (
                max_reward is not None
                and max_reward > self.best_max_reward * (1 + improvement_threshold / 2)
            )

            # æœ€ä½åˆ†æ”¹å–„ï¼šä½¿ç”¨æ›´å¯¬é¬†çš„é–¾å€¼ï¼ˆ0.5%ï¼‰ï¼Œå› ç‚ºè² åˆ†æå‡å¾ˆå›°é›£
            min_improved = (
                min_reward is not None
                and min_reward > self.best_min_reward * (1 + improvement_threshold / 2)
            )

            # æœ€ä½åˆ†æƒ¡åŒ–æª¢æ¸¬ï¼šå¦‚æœæœ€ä½åˆ†ä¸‹é™è¶…é5%ï¼Œèªªæ˜ç­–ç•¥è®Šä¸ç©©å®š
            min_degraded = (
                min_reward is not None
                and self.best_min_reward > float("-inf")
                and min_reward < self.best_min_reward * (1 - improvement_threshold * 5)
            )

            # æ›´æ–°æœ€ä½³è¨˜éŒ„
            if mean_improved:
                self.best_reward = mean_reward
                self.patience_counter = 0
                print(f"   ğŸ“ˆ æ–°æœ€ä½³å¹³å‡çå‹µ: {mean_reward:.2f}")

            if max_improved:
                self.best_max_reward = max_reward
                self.patience_counter = max(0, self.patience_counter - 5)
                print(f"   ğŸŒŸ æ–°æœ€é«˜å–®å›åˆåˆ†æ•¸: {max_reward:.2f}ï¼ˆæ¸›å°‘5æ¬¡patienceï¼‰")

            if min_improved:
                self.best_min_reward = min_reward
                self.patience_counter = max(0, self.patience_counter - 3)
                print(
                    f"   â¬†ï¸ æœ€ä½åˆ†æå‡: {min_reward:.2f}ï¼ˆæ¸›å°‘3æ¬¡patienceï¼Œç©©å®šæ€§æ”¹å–„ï¼‰"
                )

            # è­¦å‘Šï¼šæœ€ä½åˆ†æƒ¡åŒ–
            if min_degraded:
                self.patience_counter += 2  # å¢åŠ 2æ¬¡patienceï¼Œæ›´å¿«è§¸ç™¼LRé™ä½
                print(
                    f"   âš ï¸ æœ€ä½åˆ†æƒ¡åŒ–: {min_reward:.2f}ï¼ˆå¢åŠ 2æ¬¡patienceï¼Œç­–ç•¥ä¸ç©©å®šï¼‰"
                )

            # å¦‚æœæ²’æœ‰ä»»ä½•æ”¹å–„
            if not mean_improved and not max_improved and not min_improved:
                self.patience_counter += 1

            # å¦‚æœåœæ»¯å¤ªä¹…ï¼Œé™ä½å­¸ç¿’ç‡
            if self.patience_counter >= patience:
                current_lr = self.opt.param_groups[0]["lr"]
                new_lr = max(current_lr * factor, min_lr)

                if new_lr != current_lr:
                    for param_group in self.opt.param_groups:
                        param_group["lr"] = new_lr
                    self.lr = new_lr
                    self.lr_history.append(new_lr)
                    print(f"\nğŸ“‰ å­¸ç¿’ç‡è‡ªé©æ‡‰èª¿æ•´: {current_lr:.6f} â†’ {new_lr:.6f}")
                    print(f"   åŸå› : {patience} æ¬¡è¿­ä»£ç„¡é¡¯è‘—æ”¹å–„")
                    print(
                        f"   ğŸ“Š ç•¶å‰æœ€ä½³ - å¹³å‡: {self.best_reward:.2f} "
                        f"| æœ€é«˜: {self.best_max_reward:.2f} "
                        f"| æœ€ä½: {self.best_min_reward:.2f}"
                    )
                    self.patience_counter = 0
                else:
                    print(f"\nâš ï¸ å­¸ç¿’ç‡å·²é”æœ€å°å€¼ {min_lr:.6f}ï¼Œç„¡æ³•å†é™ä½")

        def _check_performance_degradation(
            self, mean_reward, max_reward, min_reward, iteration
        ):
            """æª¢æ¸¬æ€§èƒ½åš´é‡é€€åŒ–ä¸¦å›æª”åˆ°æœ€ä½³æª¢æŸ¥é»"""
            # åªæœ‰åœ¨æœ‰è¶³å¤ è¨“ç·´æ­·å²æ™‚æ‰æª¢æŸ¥ï¼ˆè‡³å°‘ 100 æ¬¡è¿­ä»£ï¼‰
            if iteration < 100:
                return False

            # åªæœ‰åœ¨æ‰€æœ‰çå‹µéƒ½æœ‰æ•ˆæ™‚æ‰æª¢æŸ¥
            if (
                mean_reward is None
                or max_reward is None
                or min_reward is None
                or self.best_reward <= 0
            ):
                return False

            # è¨ˆç®—å„æŒ‡æ¨™çš„ä¸‹é™æ¯”ä¾‹
            mean_drop = (self.best_reward - mean_reward) / abs(self.best_reward)
            max_drop = (
                (self.best_max_reward - max_reward) / abs(self.best_max_reward)
                if self.best_max_reward > 0
                else 0
            )
            min_drop = (
                (self.best_min_reward - min_reward) / abs(self.best_min_reward)
                if self.best_min_reward > 0
                else 0
            )

            # åš´æ ¼çš„é€€åŒ–é–¾å€¼ï¼šä»»ä¸€æŒ‡æ¨™ä¸‹é™è¶…é 40% å³è¦–ç‚ºå´©æ½°
            degradation_threshold = 0.40

            # æª¢æ¸¬å´©æ½°æ¢ä»¶ï¼ˆä»»ä¸€æŒ‡æ¨™åš´é‡ä¸‹é™ï¼‰
            is_catastrophic = (
                mean_drop > degradation_threshold
                or max_drop > degradation_threshold
                or (
                    min_drop > degradation_threshold and self.best_min_reward > 10
                )  # æœ€ä½åˆ†åªæœ‰åœ¨åŸæœ¬è¼ƒé«˜æ™‚æ‰é—œæ³¨
            )

            if is_catastrophic:
                print(f"\n{'='*60}")
                print("âš ï¸âš ï¸âš ï¸ æª¢æ¸¬åˆ°æ€§èƒ½å´©æ½°ï¼âš ï¸âš ï¸âš ï¸")
                print(f"{'='*60}")
                print("ğŸ“‰ ç•¶å‰æŒ‡æ¨™ vs æœ€ä½³è¨˜éŒ„ï¼š")
                print(
                    f"   å¹³å‡åˆ†: {mean_reward:.2f} (æœ€ä½³: {self.best_reward:.2f}) "
                    f"â†“ {mean_drop*100:.1f}%"
                )
                print(
                    f"   æœ€é«˜åˆ†: {max_reward:.2f} (æœ€ä½³: {self.best_max_reward:.2f}) "
                    f"â†“ {max_drop*100:.1f}%"
                )
                print(
                    f"   æœ€ä½åˆ†: {min_reward:.2f} (æœ€ä½³: {self.best_min_reward:.2f}) "
                    f"â†“ {min_drop*100:.1f}%"
                )
                print("\nğŸ”„ æ­£åœ¨å›æª”åˆ°æœ€ä½³æª¢æŸ¥é»...")

                # åŸ·è¡Œå›æª”
                success = self._rollback_to_best_checkpoint()

                if success:
                    print("âœ… æˆåŠŸå›æª”ï¼ç¹¼çºŒè¨“ç·´...")
                    print(f"{'='*60}\n")
                    return True
                else:
                    print("âŒ å›æª”å¤±æ•—ï¼Œç¹¼çºŒç•¶å‰è¨“ç·´...")
                    print(f"{'='*60}\n")
                    return False

            return False

        def _rollback_to_best_checkpoint(self):
            """å›æª”åˆ°æœ€ä½³æª¢æŸ¥é»"""
            try:
                # å°‹æ‰¾æœ€ä½³æª¢æŸ¥é»ï¼ˆåŸºæ–¼è¿­ä»£æ¬¡æ•¸ï¼‰
                checkpoints = []
                for file in os.listdir(self.save_dir):
                    if file.startswith("checkpoint_") and file.endswith(".pt"):
                        try:
                            step = int(
                                file.replace("checkpoint_", "").replace(".pt", "")
                            )
                            checkpoints.append((step, file))
                        except ValueError:
                            continue

                if not checkpoints:
                    print("   âš ï¸ æ‰¾ä¸åˆ°å¯ç”¨çš„æª¢æŸ¥é»")
                    return False

                # æŒ‰è¿­ä»£æ¬¡æ•¸æ’åºï¼Œå–æœ€æ–°çš„æª¢æŸ¥é»
                checkpoints.sort(reverse=True)

                # å˜—è©¦è¼‰å…¥æœ€è¿‘çš„å¹¾å€‹æª¢æŸ¥é»ï¼ˆè·³éç•¶å‰è¿­ä»£ï¼‰
                for step, filename in checkpoints[:5]:  # å˜—è©¦æœ€è¿‘ 5 å€‹æª¢æŸ¥é»
                    checkpoint_path = os.path.join(self.save_dir, filename)

                    try:
                        print(f"   ğŸ“‚ å˜—è©¦è¼‰å…¥æª¢æŸ¥é»: {filename}")
                        checkpoint = torch.load(
                            checkpoint_path, map_location=self.device
                        )

                        # è¼‰å…¥æ¨¡å‹ç‹€æ…‹
                        if "model_state" in checkpoint:
                            self.net.load_state_dict(checkpoint["model_state"])
                            print("      âœ“ æ¨¡å‹åƒæ•¸å·²è¼‰å…¥")
                        else:
                            print("      âœ— æª¢æŸ¥é»æ ¼å¼éŒ¯èª¤")
                            continue

                        # è¼‰å…¥å„ªåŒ–å™¨ç‹€æ…‹ï¼ˆé‡ç½®å­¸ç¿’å‹•é‡ï¼‰
                        if "optimizer_state" in checkpoint:
                            self.opt.load_state_dict(checkpoint["optimizer_state"])
                            print("      âœ“ å„ªåŒ–å™¨ç‹€æ…‹å·²è¼‰å…¥")

                        # é‡ç½® patience è¨ˆæ•¸å™¨
                        self.patience_counter = 0

                        # é‡ç½®å­¸ç¿’ç‡ç‚ºåˆå§‹å€¼æˆ–ç•¥ä½çš„å€¼
                        rollback_lr = self.initial_lr * 0.5  # ä½¿ç”¨ç¨ä½çš„å­¸ç¿’ç‡
                        for param_group in self.opt.param_groups:
                            param_group["lr"] = rollback_lr
                        print(f"      âœ“ å­¸ç¿’ç‡é‡ç½®ç‚º: {rollback_lr:.6f}")

                        print(f"\n   âœ… æˆåŠŸå¾è¿­ä»£ #{step} å›æª”ï¼")
                        return True

                    except Exception as e:
                        print(f"      âœ— è¼‰å…¥å¤±æ•—: {e}")
                        continue

                print("   âŒ æ‰€æœ‰æª¢æŸ¥é»éƒ½ç„¡æ³•è¼‰å…¥")
                return False

            except Exception as e:
                print(f"   âŒ å›æª”éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                import traceback

                traceback.print_exc()
                return False

        def _load_dynamic_config(self, iteration):
            """æ¯10å€‹è¿­ä»£æª¢æŸ¥ä¸¦åŠ è¼‰é…ç½®æ–‡ä»¶æ›´æ–°"""
            if iteration % 10 != 0:
                return False

            if not self.config_path.exists():
                return False

            try:
                with open(self.config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)

                # æ ¹æ“šè¨­å‚™é¡å‹é¸æ“‡é…ç½®
                device_type = (
                    self.device.type
                    if hasattr(self.device, "type")
                    else str(self.device)
                )
                mode = "gpu_training" if device_type == "cuda" else "cpu_training"
                params = config.get(mode, {})

                updated = False
                updates = []

                # æª¢æŸ¥ä¸¦æ›´æ–°å­¸ç¿’ç‡
                new_lr = params.get("learning_rate")
                if new_lr and abs(new_lr - self.lr) > 1e-9:
                    self.lr = new_lr
                    for param_group in self.opt.param_groups:
                        param_group["lr"] = new_lr
                    updates.append(f"å­¸ç¿’ç‡: {new_lr}")
                    updated = True

                # æ›´æ–°å…¶ä»–åƒæ•¸
                if "gamma" in params and params["gamma"] != self.gamma:
                    self.gamma = params["gamma"]
                    updates.append(f"gamma: {self.gamma}")
                    updated = True

                if "gae_lambda" in params and params["gae_lambda"] != self.lam:
                    self.lam = params["gae_lambda"]
                    updates.append(f"lambda: {self.lam}")
                    updated = True

                if "clip_range" in params and params["clip_range"] != self.clip_eps:
                    self.clip_eps = params["clip_range"]
                    updates.append(f"clip: {self.clip_eps}")
                    updated = True

                if "vf_coef" in params and params["vf_coef"] != self.vf_coef:
                    self.vf_coef = params["vf_coef"]
                    updates.append(f"vf_coef: {self.vf_coef}")
                    updated = True

                if "ent_coef" in params and params["ent_coef"] != self.ent_coef:
                    self.ent_coef = params["ent_coef"]
                    updates.append(f"ent_coef: {self.ent_coef}")
                    updated = True

                if "batch_size" in params and params["batch_size"] != self.batch_size:
                    self.batch_size = params["batch_size"]
                    updates.append(f"batch_size: {self.batch_size}")
                    updated = True

                if "ppo_epochs" in params and params["ppo_epochs"] != self.ppo_epochs:
                    self.ppo_epochs = params["ppo_epochs"]
                    updates.append(f"ppo_epochs: {self.ppo_epochs}")
                    updated = True

                if updated:
                    print("\nâš™ï¸ åƒæ•¸å·²å¾é…ç½®æ–‡ä»¶æ›´æ–°:")
                    for update in updates:
                        print(f"   â€¢ {update}")
                    print()

                return updated

            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è®€å–é…ç½®æ–‡ä»¶: {e}")
                return False

        def build_agent(self):
            agent = PPOAgent()
            agent.net = self.net
            agent.opt = self.opt
            agent.device = self.device
            return agent

        def collect_trajectory(self, envs=None, horizon=2048, stop_event=None):
            """Collect a `horizon`-length trajectory across one or more environments.

            Supports both list of environments (sequential) and vectorized environments.
            """
            from game.vec_env import SubprocVecEnv

            if envs is None:
                envs = [GameEnv()]
            elif isinstance(envs, GameEnv):
                envs = [envs]

            # æª¢æŸ¥æ˜¯å¦ç‚ºå‘é‡åŒ–ç’°å¢ƒ
            is_vec_env = isinstance(envs, SubprocVecEnv)

            if is_vec_env:
                # ä½¿ç”¨çœŸæ­£çš„ä¸¦è¡Œç’°å¢ƒ
                return self._collect_trajectory_vectorized(envs, horizon, stop_event)
            else:
                # ä½¿ç”¨ä¸²è¡Œç’°å¢ƒï¼ˆåŸæœ‰é‚è¼¯ï¼‰
                envs = list(envs) or [GameEnv()]
                return self._collect_trajectory_sequential(envs, horizon, stop_event)

        def _collect_trajectory_vectorized(self, vec_env, horizon, stop_event=None):
            """ä½¿ç”¨å‘é‡åŒ–ç’°å¢ƒä¸¦è¡Œæ”¶é›†è»Œè·¡"""
            n_envs = len(vec_env)
            print(f"ğŸš€ ä½¿ç”¨ {n_envs} å€‹ä¸¦è¡Œç’°å¢ƒæ”¶é›†æ•¸æ“š...")
            states = vec_env.reset()  # shape: (n_envs, state_dim)
            episode_returns = [0.0 for _ in range(n_envs)]

            batch_states = []
            actions, rewards, dones, values, logps, next_values = [], [], [], [], [], []
            ep_rewards = []

            steps = 0
            while steps < horizon:
                if (
                    stop_event is not None
                    and getattr(stop_event, "is_set", lambda: False)()
                ):
                    break

                # æ‰¹æ¬¡è™•ç†æ‰€æœ‰ç’°å¢ƒçš„ç‹€æ…‹
                s_batch = torch.tensor(
                    states, dtype=torch.float32, device=self.device
                )  # (n_envs, state_dim)

                with torch.no_grad():
                    logits, vals = self.net(s_batch)  # (n_envs, 1), (n_envs, 1)
                    probs = torch.sigmoid(logits)
                    dist = torch.distributions.Bernoulli(probs=probs)
                    action_tensors = dist.sample()  # (n_envs, 1)
                    logp = dist.log_prob(action_tensors)  # (n_envs, 1)

                actions_np = action_tensors.cpu().numpy().flatten().astype(int)

                # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ç’°å¢ƒ
                next_states, rews, dones_arr, infos = vec_env.step(actions_np)

                # è¨˜éŒ„æ•¸æ“š
                for i in range(n_envs):
                    batch_states.append(states[i])
                    actions.append(actions_np[i])
                    rewards.append(rews[i])
                    dones.append(dones_arr[i])
                    values.append(vals[i].item())
                    logps.append(logp[i].item())

                    episode_returns[i] += float(rews[i])

                    if dones_arr[i]:
                        ep_rewards.append(episode_returns[i])
                        episode_returns[i] = 0.0
                        # è¨ˆç®— next_value (é‡ç½®å¾Œç‚º 0)
                        next_values.append(0.0)
                    else:
                        # è¨ˆç®— next_value
                        with torch.no_grad():
                            s_next_t = torch.tensor(
                                next_states[i], dtype=torch.float32, device=self.device
                            ).unsqueeze(0)
                            _, next_value = self.net(s_next_t)
                            next_values.append(float(next_value.item()))

                states = next_states
                steps += n_envs

            if not batch_states:
                empty = torch.empty((0, 5), dtype=torch.float32, device=self.device)
                zero = torch.empty((0, 1), dtype=torch.float32, device=self.device)
                return (
                    {
                        "states": empty,
                        "actions": zero,
                        "logps": zero,
                        "returns": zero,
                        "advs": zero,
                    },
                    ep_rewards,
                )

            # è¨ˆç®— GAE å„ªå‹¢
            if len(next_values) < len(rewards):
                next_values.extend([0.0] * (len(rewards) - len(next_values)))

            advs = []
            gae = 0.0
            for i in reversed(range(len(rewards))):
                delta = (
                    rewards[i]
                    + self.gamma * next_values[i] * (1 - dones[i])
                    - values[i]
                )
                gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae
                advs.insert(0, gae)

            returns = [adv + val for adv, val in zip(advs, values)]

            return (
                {
                    "states": torch.tensor(
                        batch_states, dtype=torch.float32, device=self.device
                    ),
                    "actions": torch.tensor(
                        actions, dtype=torch.float32, device=self.device
                    ).unsqueeze(1),
                    "logps": torch.tensor(
                        logps, dtype=torch.float32, device=self.device
                    ).unsqueeze(1),
                    "returns": torch.tensor(
                        returns, dtype=torch.float32, device=self.device
                    ).unsqueeze(1),
                    "advs": torch.tensor(
                        advs, dtype=torch.float32, device=self.device
                    ).unsqueeze(1),
                },
                ep_rewards,
            )

        def _collect_trajectory_sequential(self, envs, horizon, stop_event=None):
            """ä½¿ç”¨ä¸²è¡Œç’°å¢ƒæ”¶é›†è»Œè·¡ï¼ˆåŸæœ‰é‚è¼¯ï¼‰"""

            states = [env.reset() for env in envs]
            episode_returns = [0.0 for _ in envs]

            batch_states = []
            actions, rewards, dones, values, logps, next_values = [], [], [], [], [], []
            ep_rewards = []

            for t in range(horizon):
                if (
                    stop_event is not None
                    and getattr(stop_event, "is_set", lambda: False)()
                ):
                    break

                env_idx = t % len(envs)
                env = envs[env_idx]
                s = states[env_idx]

                s_t = torch.tensor(
                    s, dtype=torch.float32, device=self.device
                ).unsqueeze(0)
                logits, value = self.net(s_t)
                prob = torch.sigmoid(logits)
                dist = torch.distributions.Bernoulli(probs=prob)
                action_tensor = dist.sample()
                action = int(action_tensor.item())
                logp = dist.log_prob(action_tensor)

                s_next, r, done, _ = env.step(action)

                batch_states.append(s)
                actions.append(action)
                rewards.append(r)
                dones.append(done)
                values.append(value.item())
                logps.append(logp.item())

                episode_returns[env_idx] += float(r)

                if done:
                    ep_rewards.append(episode_returns[env_idx])
                    episode_returns[env_idx] = 0.0
                    states[env_idx] = env.reset()
                    next_values.append(0.0)
                else:
                    states[env_idx] = s_next
                    with torch.no_grad():
                        s_next_t = torch.tensor(
                            s_next, dtype=torch.float32, device=self.device
                        ).unsqueeze(0)
                        _, next_value = self.net(s_next_t)
                        next_values.append(float(next_value.item()))

            if not batch_states:
                empty = torch.empty((0, 5), dtype=torch.float32, device=self.device)
                zero = torch.empty((0, 1), dtype=torch.float32, device=self.device)
                return (
                    {
                        "states": empty,
                        "actions": zero,
                        "logps": zero,
                        "returns": zero,
                        "advs": zero,
                    },
                    ep_rewards,
                )

            if len(next_values) < len(rewards):
                next_values.extend([0.0] * (len(rewards) - len(next_values)))

            advs = []
            gae = 0.0
            for i in reversed(range(len(rewards))):
                delta = (
                    rewards[i]
                    + self.gamma * next_values[i] * (1 - dones[i])
                    - values[i]
                )
                gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae
                advs.insert(0, gae)

            returns = [adv + val for adv, val in zip(advs, values)]

            batch = {
                "states": torch.tensor(
                    np.array(batch_states), dtype=torch.float32, device=self.device
                ),
                "actions": torch.tensor(
                    actions, dtype=torch.float32, device=self.device
                ).unsqueeze(1),
                "logps": torch.tensor(
                    logps, dtype=torch.float32, device=self.device
                ).unsqueeze(1),
                "returns": torch.tensor(
                    returns, dtype=torch.float32, device=self.device
                ).unsqueeze(1),
                "advs": torch.tensor(
                    advs, dtype=torch.float32, device=self.device
                ).unsqueeze(1),
            }

            batch["advs"] = (batch["advs"] - batch["advs"].mean()) / (
                batch["advs"].std() + 1e-8
            )

            return batch, ep_rewards

        def ppo_update(self, batch):
            N = batch["states"].size(0)
            idxs = np.arange(N)
            for _ in range(self.ppo_epochs):
                np.random.shuffle(idxs)
                for start in range(0, N, self.batch_size):
                    mb_idx = idxs[start : start + self.batch_size]
                    s = batch["states"][mb_idx]
                    a = batch["actions"][mb_idx]
                    old_logp = batch["logps"][mb_idx]
                    ret = batch["returns"][mb_idx]
                    adv = batch["advs"][mb_idx]

                    logits, value = self.net(s)
                    prob = torch.sigmoid(logits)
                    m = torch.distributions.Bernoulli(probs=prob)
                    new_logp = m.log_prob(a)
                    entropy = m.entropy().mean()

                    ratio = torch.exp(new_logp - old_logp)
                    surr1 = ratio * adv
                    surr2 = (
                        torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps)
                        * adv
                    )
                    policy_loss = -torch.min(surr1, surr2).mean()

                    value_loss = F.mse_loss(value, ret)

                    loss = (
                        policy_loss
                        + self.vf_coef * value_loss
                        - self.ent_coef * entropy
                    )

                    self.opt.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)
                    self.opt.step()

            return loss.item(), policy_loss.item(), value_loss.item(), entropy.item()

        def save(self, step):
            path = os.path.join(self.save_dir, f"checkpoint_{step}.pt")
            torch.save(
                {
                    "model_state": self.net.state_dict(),
                    "optimizer_state": self.opt.state_dict(),
                },
                path,
            )
            return path

        def train(
            self,
            total_timesteps=None,
            env=None,
            envs=None,
            log_interval=1,
            metrics_callback=None,
            stop_event=None,
            initial_iteration=0,
        ):
            """Main training loop.

            metrics_callback: optional callable(metrics: dict) called after each
            PPO update with keys: it, loss, policy_loss, value_loss, entropy,
            timesteps, mean_reward, episode_count
            """
            if envs is not None:
                env_list = list(envs) or [GameEnv()]
            elif env is not None:
                env_list = env if isinstance(env, (list, tuple)) else [env]
            else:
                env_list = [GameEnv()]

            env_list = [e if isinstance(e, GameEnv) else GameEnv() for e in env_list]

            timesteps = 0
            it = initial_iteration

            while True:
                # æª¢æŸ¥ä¸¦æ›´æ–°é…ç½®ï¼ˆæ¯10æ¬¡è¿­ä»£ï¼‰
                self._load_dynamic_config(it)

                # honor external stop request
                if (
                    stop_event is not None
                    and getattr(stop_event, "is_set", lambda: False)()
                ):
                    break

                batch, ep_rewards = self.collect_trajectory(
                    env_list, stop_event=stop_event
                )
                if batch["states"].numel() == 0:
                    continue
                timesteps += batch["states"].size(0)
                loss, ploss, vloss, ent = self.ppo_update(batch)
                it += 1
                # log
                self.writer.add_scalar("loss/total", loss, it)
                self.writer.add_scalar("loss/policy", ploss, it)
                self.writer.add_scalar("loss/value", vloss, it)
                self.writer.add_scalar("policy/entropy", ent, it)

                mean_reward = float(np.mean(ep_rewards)) if ep_rewards else None
                max_reward = float(np.max(ep_rewards)) if ep_rewards else None
                min_reward = float(np.min(ep_rewards)) if ep_rewards else None
                episode_count = len(ep_rewards)

                # è¨˜éŒ„çå‹µçµ±è¨ˆåˆ° TensorBoard
                if mean_reward is not None:
                    self.writer.add_scalar("reward/mean", mean_reward, it)
                    self.writer.add_scalar("reward/max", max_reward, it)
                    self.writer.add_scalar("reward/min", min_reward, it)

                    # æª¢æ¸¬æ€§èƒ½é€€åŒ–ï¼ˆæ¯10æ¬¡è¿­ä»£æ‰æª¢æŸ¥ï¼Œé¿å…éåº¦æ•æ„Ÿï¼‰
                    if it % 10 == 0:
                        self._check_performance_degradation(
                            mean_reward, max_reward, min_reward, it
                        )

                # å„²å­˜æ­·å²æ•¸æ“šç”¨æ–¼æ¯”è¼ƒ
                if not hasattr(self, "_history"):
                    self._history = {
                        "loss": [],
                        "policy_loss": [],
                        "value_loss": [],
                        "entropy": [],
                        "mean_reward": [],
                        "max_reward": [],
                        "min_reward": [],
                        "weight_mean": [],
                        "weight_std": [],
                        "grad_norm": [],
                    }

                self._history["loss"].append(loss)
                self._history["policy_loss"].append(ploss)
                self._history["value_loss"].append(vloss)
                self._history["entropy"].append(ent)
                if mean_reward is not None:
                    self._history["mean_reward"].append(mean_reward)
                    self._history["max_reward"].append(max_reward)
                    self._history["min_reward"].append(min_reward)

                # æ‰“å°è©³ç´°çš„è¨“ç·´è¨ºæ–·ä¿¡æ¯ï¼ˆæ¯10æ¬¡è¿­ä»£ï¼‰
                if it % 10 == 0:
                    print(f"\n{'='*60}")
                    print(f"è¨“ç·´è¿­ä»£ #{it}")
                    print(f"{'='*60}")
                    print("ğŸ“Š Loss æŒ‡æ¨™:")
                    print(f"  ç¸½æå¤±: {loss:.4f}")
                    print(f"  ç­–ç•¥æå¤±: {ploss:.4f}")
                    print(f"  åƒ¹å€¼æå¤±: {vloss:.4f}")
                    print(f"  ç†µå€¼: {ent:.4f}")
                    print("\nğŸ® è¨“ç·´æ•ˆæœ:")
                    if mean_reward is not None:
                        print(f"  å¹³å‡çå‹µ: {mean_reward:.2f}")
                        print(f"  æœ€é«˜çå‹µ: {max_reward:.2f}")
                        print(f"  æœ€ä½çå‹µ: {min_reward:.2f}")
                    else:
                        print("  å¹³å‡çå‹µ: N/A (å°šæœªå®Œæˆä»»ä½•å›åˆ)")
                    print(f"  å®Œæˆå›åˆæ•¸: {episode_count}")
                    print(f"  ç¸½æ™‚é–“æ­¥: {timesteps}")

                    # é¡¯ç¤ºä¸¦è¡Œç’°å¢ƒä¿¡æ¯
                    if hasattr(env_list, "__len__") and len(env_list) > 1:
                        print("\nğŸ”„ ä¸¦è¡Œç’°å¢ƒ:")
                        print(f"  ç’°å¢ƒæ•¸é‡: {len(env_list)}")
                        print(f"  ç†è«–åŠ é€Ÿ: {len(env_list)}x")

                    print("\nâš™ï¸ ç¶²è·¯ç‹€æ…‹:")
                    # æª¢æŸ¥ç¶²è·¯æ¬Šé‡æ˜¯å¦åœ¨æ›´æ–°
                    current_w_mean = 0.0
                    current_w_std = 0.0
                    try:
                        w = self.net.get_weight_matrix()
                        if w is not None:
                            current_w_mean = float(np.mean(np.abs(w)))
                            current_w_std = float(np.std(w))
                            print(f"  æ¬Šé‡å¹³å‡å€¼: {current_w_mean:.6f}")
                            print(f"  æ¬Šé‡æ¨™æº–å·®: {current_w_std:.6f}")

                            # å„²å­˜æ¬Šé‡æ­·å²
                            self._history["weight_mean"].append(current_w_mean)
                            self._history["weight_std"].append(current_w_std)
                        else:
                            print("  æ¬Šé‡: ç„¡æ³•ç²å–")
                    except Exception as e:
                        print(f"  æ¬Šé‡: ç²å–å¤±æ•— ({e})")

                    # æª¢æŸ¥æ¢¯åº¦
                    grad_norms = []
                    for param in self.net.parameters():
                        if param.grad is not None:
                            grad_norms.append(float(param.grad.norm().item()))
                    if grad_norms:
                        avg_grad = np.mean(grad_norms)
                        print(f"  å¹³å‡æ¢¯åº¦ç¯„æ•¸: {avg_grad:.6f}")
                        self._history["grad_norm"].append(avg_grad)

                        if avg_grad < 1e-6:
                            print("  âš ï¸ è­¦å‘Š: æ¢¯åº¦éå°ï¼Œæ¬Šé‡å¯èƒ½æœªæ­£ç¢ºæ›´æ–°ï¼")
                        elif avg_grad > 0.001:
                            print("  âœ… æ¢¯åº¦æ­£å¸¸ï¼Œæ¬Šé‡æ­£åœ¨æ›´æ–°")
                    else:
                        print("  æ¢¯åº¦: ç„¡")

                    # èˆ‡ä¸Šæ¬¡è¿­ä»£æ¯”è¼ƒ (å¦‚æœæœ‰æ­·å²æ•¸æ“š)
                    if len(self._history["loss"]) >= 2:
                        print(f"\nğŸ“ˆ èˆ‡ä¸Šæ¬¡æ¯”è¼ƒ (è¿­ä»£ #{it-10}):")

                        loss_change = loss - self._history["loss"][-2]
                        loss_arrow = "ğŸ“‰" if loss_change < 0 else "ğŸ“ˆ"
                        print(f"  ç¸½æå¤±: {loss_change:+.4f} {loss_arrow}")

                        ploss_change = ploss - self._history["policy_loss"][-2]
                        print(f"  ç­–ç•¥æå¤±: {ploss_change:+.4f}")

                        vloss_change = vloss - self._history["value_loss"][-2]
                        print(f"  åƒ¹å€¼æå¤±: {vloss_change:+.4f}")

                        ent_change = ent - self._history["entropy"][-2]
                        print(f"  ç†µå€¼: {ent_change:+.4f}")

                        if len(self._history["weight_mean"]) >= 2:
                            w_mean_change = (
                                current_w_mean - self._history["weight_mean"][-2]
                            )
                            w_std_change = (
                                current_w_std - self._history["weight_std"][-2]
                            )
                            print(f"  æ¬Šé‡å¹³å‡: {w_mean_change:+.6f}")
                            print(f"  æ¬Šé‡æ¨™æº–å·®: {w_std_change:+.6f}")

                            if abs(w_mean_change) < 1e-6 and abs(w_std_change) < 1e-6:
                                print("  âš ï¸ æ¬Šé‡å¹¾ä¹æ²’æœ‰è®ŠåŒ–ï¼")
                            else:
                                print("  âœ… æ¬Šé‡æ­£åœ¨æ›´æ–°")

                        if len(self._history["mean_reward"]) >= 2:
                            reward_change = (
                                self._history["mean_reward"][-1]
                                - self._history["mean_reward"][-2]
                            )
                            reward_arrow = "ğŸ“ˆ" if reward_change > 0 else "ğŸ“‰"
                            print(f"  å¹³å‡çå‹µ: {reward_change:+.2f} {reward_arrow}")

                    # å­¸ç¿’é€²åº¦è©•ä¼°
                    if mean_reward is not None:
                        if mean_reward > 20:
                            print("\nâœ… å­¸ç¿’é€²åº¦: å„ªç§€ (çå‹µ > 20)")
                        elif mean_reward > 10:
                            print("\nğŸ“ˆ å­¸ç¿’é€²åº¦: è‰¯å¥½ (çå‹µ > 10)")
                        elif mean_reward > 5:
                            print("\nâš¡ å­¸ç¿’é€²åº¦: é€²æ­¥ä¸­ (çå‹µ > 5)")
                        elif mean_reward > 0:
                            print("\nğŸ”„ å­¸ç¿’é€²åº¦: ç·©æ…¢ (çå‹µ > 0)")
                        else:
                            print("\nâš ï¸ å­¸ç¿’é€²åº¦: éœ€è¦èª¿æ•´ (çå‹µ < 0)")
                            print("   å»ºè­°: æª¢æŸ¥çå‹µå‡½æ•¸ã€é™ä½å­¸ç¿’ç‡æˆ–èª¿æ•´ç¶²è·¯çµæ§‹")
                    else:
                        # å³ä½¿æ²’æœ‰å®Œæˆå›åˆï¼Œä¹Ÿé¡¯ç¤ºå­¸ç¿’ç‹€æ…‹
                        print("\nğŸ”„ å­¸ç¿’ç‹€æ…‹:")
                        if loss < 0.05:
                            print(f"  æå¤±å¾ˆä½ ({loss:.4f})ï¼Œä½†æ²’æœ‰å®Œæˆå›åˆ")
                            print("  å¯èƒ½åŸå› : éŠæˆ²å¤ªé›£ã€çå‹µå‡½æ•¸å•é¡Œ")
                        elif ent < 0.05:
                            print(f"  ç†µå€¼éä½ ({ent:.4f})ï¼Œç­–ç•¥å¯èƒ½éæ—©æ”¶æ–‚")
                            print("  å»ºè­°: å¢åŠ  ent_coef æˆ–é‡ç½®è¨“ç·´")
                        else:
                            print("  ä»åœ¨å­¸ç¿’ä¸­ï¼Œç¹¼çºŒè¨“ç·´...")

                    print(f"{'='*60}\n")

                # æ›´æ–°å­¸ç¿’ç‡èª¿åº¦å™¨
                if self.lr_scheduler is not None:
                    scheduler_type = self.scheduler_config.get("type", "none")
                    if (
                        scheduler_type == "reduce_on_plateau"
                        and mean_reward is not None
                    ):
                        # ReduceLROnPlateau éœ€è¦ç›£æ§æŒ‡æ¨™
                        self.lr_scheduler.step(mean_reward)
                    elif scheduler_type in ["step", "exponential", "cosine"]:
                        # å…¶ä»–èª¿åº¦å™¨åŸºæ–¼è¿­ä»£æ¬¡æ•¸
                        self.lr_scheduler.step()

                # è‡ªå®šç¾©è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´
                if it % 10 == 0:  # æ¯10æ¬¡è¿­ä»£æª¢æŸ¥ä¸€æ¬¡
                    self._update_lr_adaptive(mean_reward, max_reward, min_reward, it)

                    # é¡¯ç¤ºç•¶å‰å­¸ç¿’ç‡
                    current_lr = self.opt.param_groups[0]["lr"]
                    if abs(current_lr - self.initial_lr) > 1e-9:
                        print(
                            f"ğŸ“Š ç•¶å‰å­¸ç¿’ç‡: {current_lr:.6f} "
                            f"(åˆå§‹: {self.initial_lr:.6f})"
                        )

                # callback for UI or external monitor
                try:
                    if metrics_callback is not None:
                        # ç²å–ç¶²è·¯æ¬Šé‡ç”¨æ–¼è¦–è¦ºåŒ–
                        weight_matrix = None
                        try:
                            weight_matrix = self.net.get_weight_matrix()
                        except Exception:
                            pass

                        metrics_callback(
                            {
                                "it": it,
                                "loss": float(loss),
                                "policy_loss": float(ploss),
                                "value_loss": float(vloss),
                                "entropy": float(ent),
                                "timesteps": int(timesteps),
                                "mean_reward": mean_reward,
                                "episode_count": episode_count,
                                "weights": weight_matrix,
                            }
                        )
                except Exception:
                    # metrics callback must not break training
                    pass

                if it % 10 == 0:
                    cp = self.save(it)
                    print(f"Saved checkpoint {cp}")

                if total_timesteps is not None and timesteps >= total_timesteps:
                    break

                # allow stopping after update
                if (
                    stop_event is not None
                    and getattr(stop_event, "is_set", lambda: False)()
                ):
                    break

            self.writer.close()

except Exception:
    # Torch not available: keep file importable but trainer unavailable
    pass
