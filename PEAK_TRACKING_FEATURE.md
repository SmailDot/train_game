# 🌟 三指標追蹤系統 (Triple-Metric Tracking)

## 問題背景

在訓練過程中發現，有時候 AI 會達到 170+ 的高分，但後續回合可能只有 50-60 分左右，甚至出現負分。原本的系統只使用**平均分數**來評估訓練效果，這會導致：

- 高分回合的重要信息被平均值稀釋
- 低分災難（-50, -100）被忽略
- 學習率調整器無法識別「AI 偶爾表現出色但不穩定」的信號
- 珍貴的高分軌跡數據的價值被忽略
- 策略退化（最低分持續惡化）被掩蓋

**範例**：
```
回合分數: [170, 50, 60, -30, 45, 50, 65]

❌ 只看平均分數: 58.6  
   → 看起來還行，但忽略了災難性的 -30 分

✅ 三指標分析:
   平均: 58.6  ← 整體表現中等
   最高: 170   ← AI 有很高潛力！
   最低: -30   ← 但存在嚴重失敗！
   
結論：策略不穩定，既能打出高分又會災難性失敗
      需要穩定訓練，提升下限
```

## 解決方案

### 1. 三指標追蹤系統

現在系統會同時追蹤三個關鍵指標：

| 指標 | 用途 | 意義 | 改善標準 |
|------|------|------|----------|
| **平均獎勵** (mean_reward) | 評估穩定性 | AI 的平均表現水準 | > 1% 提升 |
| **最高獎勵** (max_reward) | 評估潛力 | AI 的最佳表現能力 | > 0.5% 提升 |
| **最低獎勵** (min_reward) | 評估下限 | AI 的最差表現情況 | > 0.5% 提升 |

### 2. 智能學習率調整（正反教材系統）

#### 傳統方式（僅考慮平均值）
```python
if mean_reward > best_reward * (1 + 0.01):  # 1% 改進門檻
    update_learning_rate()
```

#### 新方式（三指標評估）
```python
# 平均分數改進：使用標準門檻（1%）
mean_improved = mean_reward > best_reward * 1.01

# 峰值分數改進：使用較寬鬆門檻（0.5%）← 正面教材
max_improved = max_reward > best_max_reward * 1.005

# 最低分數改進：使用較寬鬆門檻（0.5%）← 反面教材消除
min_improved = min_reward > best_min_reward * 1.005

# 最低分數惡化：檢測策略退化 ← 反面教材出現
min_degraded = min_reward < best_min_reward * 0.95  # -5%

if mean_improved:
    # 整體進步，完全重置 patience
    patience_counter = 0
    
if max_improved:
    # 發現新高峰，減少 5 次 patience（鼓勵探索）
    patience_counter = max(0, patience_counter - 5)
    print("🌟 發現新最高分！延緩學習率降低")
    
if min_improved:
    # 下限提升，減少 3 次 patience（穩定性改善）
    patience_counter = max(0, patience_counter - 3)
    print("⬆️ 最低分提升！策略更穩定")
    
if min_degraded:
    # 下限惡化，增加 2 次 patience（加速 LR 降低）
    patience_counter += 2
    print("⚠️ 最低分惡化！策略不穩定")
```

### 3. Patience 調整機制（正反教材響應）

當發現不同信號時採取不同策略：

| 事件 | Patience 調整 | 理由 |
|------|--------------|------|
| **平均分提升** | 重置為 0 | 整體進步，繼續當前策略 |
| **最高分突破** | -5 | 發現潛力，鼓勵探索 |
| **最低分提升** | -3 | 下限改善，穩定性提升 |
| **最低分惡化** | +2 | 策略退化，加速降低 LR |
| **無改善** | +1 | 正常累積 |

**範例情境**：
```
迭代 100: 平均 65，最高 170，最低 -20
  → 發現新峰值！patience_counter -= 5
  → "AI 展現了高分潛力，維持當前學習策略"

迭代 110: 平均 68，最高 165，最低 -15
  → 平均分提升 + 最低分提升！
  → patience_counter = 0 (平均分改善)
  → patience_counter -= 3 (最低分改善)
  → "整體進步且穩定性提升"

迭代 120: 平均 70，最高 160，最低 -40  ⚠️
  → 平均分提升但最低分惡化！
  → patience_counter = 0 (平均分)
  → patience_counter += 2 (最低分惡化)
  → "警告：雖然平均分提升，但出現災難性失敗"

迭代 130: 平均 72，最高 180，最低 -10
  → 新峰值 + 最低分大幅提升！
  → patience_counter -= 5 (峰值)
  → patience_counter -= 3 (最低分)
  → "完美進步：潛力提升且穩定性改善"
```

## 實際效果

### 訓練輸出改進

**修改前**：
```
🎮 訓練效果:
  平均獎勵: 70.71
  完成回合數: 7
```

**修改後**：
```
🎮 訓練效果:
  平均獎勵: 70.71
  最高獎勵: 170.00  ← 新增
  最低獎勵: 45.00   ← 新增
  完成回合數: 7
```

### 學習率調整通知

當發現新峰值時會顯示：
```
🌟 新最高單回合分數: 170.00
   最佳平均分數: 70.71
   (減少 5 次 patience 計數)
```

### TensorBoard 視覺化

現在可以在 TensorBoard 中看到三條曲線：
- `reward/mean` - 平均獎勵趨勢
- `reward/max` - 峰值獎勵趨勢
- `reward/min` - 最低獎勵趨勢

可以觀察：
- 峰值是否持續提升（潛力成長）
- 平均值是否逐漸接近峰值（穩定性提升）
- 最低值是否提高（下限改善）

## 技術細節

### 修改的文件

**agents/pytorch_trainer.py**

#### 1. 新增追蹤變量（Line 93-96）
```python
self.best_reward = float("-inf")       # 追蹤最佳平均分數
self.best_max_reward = float("-inf")   # 追蹤最佳峰值分數（新增）
self.best_min_reward = float("-inf")   # 追蹤最好的最低分（下限提升，新增）
self.patience_counter = 0
self.lr_reduce_count = 0
```

#### 2. 更新學習率調整方法（Lines 153-238）
```python
def _update_lr_adaptive(self, mean_reward, max_reward, min_reward, iteration):
    """
    三指標自適應學習率調整（正反教材系統）
    
    Args:
        mean_reward: 平均獎勵（評估整體穩定性）
        max_reward: 最高獎勵（評估潛力上限）← 正面教材
        min_reward: 最低獎勵（評估穩定性下限）← 反面教材
        iteration: 當前迭代次數
        
    策略：
        - 平均分提升 → 整體進步，重置 patience
        - 最高分突破 → 發現潛力，減少 patience（鼓勵探索）
        - 最低分提升 → 下限改善，減少 patience（穩定性提升）
        - 最低分惡化 → 增加 patience（警告：策略不穩定）
    """
    # ... 三指標評估邏輯 ...
```

#### 3. 計算峰值和最低值（Line 652-654）
```python
mean_reward = float(np.mean(ep_rewards)) if ep_rewards else None
max_reward = float(np.max(ep_rewards)) if ep_rewards else None   # 新增
min_reward = float(np.min(ep_rewards)) if ep_rewards else None   # 新增
```

#### 4. 記錄到 TensorBoard（Lines 658-662）
```python
if mean_reward is not None:
    self.writer.add_scalar("reward/mean", mean_reward, it)
    self.writer.add_scalar("reward/max", max_reward, it)    # 新增
    self.writer.add_scalar("reward/min", min_reward, it)    # 新增
```

#### 5. 歷史數據記錄（Lines 667-676）
```python
self._history = {
    "loss": [],
    "policy_loss": [],
    "value_loss": [],
    "entropy": [],
    "mean_reward": [],
    "max_reward": [],    # 新增
    "min_reward": [],    # 新增
    "weight_mean": [],
    "weight_std": [],
    "grad_norm": [],
}
```

#### 6. 更新調用（Line 886）
```python
# 修改前
self._update_lr_adaptive(mean_reward, it)

# 修改後
self._update_lr_adaptive(mean_reward, max_reward, min_reward, it)
```

## 設計理念

### 為什麼需要三指標系統？

**單一平均值的問題**：
- ❌ 無法識別高潛力但不穩定的策略
- ❌ 災難性失敗被平均值掩蓋
- ❌ 不知道改善是來自上限提升還是下限提升

**三指標的優勢**：
- ✅ **最高分（正面教材）**：識別成功策略的特徵
- ✅ **最低分（反面教材）**：識別失敗場景並避免
- ✅ **平均分（整體評估）**：平衡潛力與穩定性

### 為什麼採用正反教材方法？

**核心哲學**：
> 機器學習不僅要學習「什麼是好的」，  
> 更要學習「什麼是災難性的」並避免重複。

**實際場景**：
```
場景 A：分數 [170, 165, 160, 155, 150]
  最高: 170  平均: 160  最低: 150
  → 穩定的高分表現，繼續當前策略 ✅

場景 B：分數 [170, 100, 50, -20, -50]
  最高: 170  平均: 50   最低: -50
  → 不穩定！雖有高分但存在災難性失敗 ⚠️
  → 需要降低學習率，穩定策略

場景 C：分數 [100, 90, 85, 80, 75]
  最高: 100  平均: 86   最低: 75
  → 沒有突破但很穩定，可以嘗試提高探索 🔍
```

### 正反教材的權重設計

| 信號 | Patience 調整 | 權重理由 |
|------|--------------|---------|
| 平均分 +1% | 重置為 0 | 最重要：整體進步 |
| 最高分 +0.5% | -5 | 次重要：潛力信號 |
| 最低分 +0.5% | -3 | 重要：穩定性信號 |
| 最低分 -5% | +2 | 警告信號：雙倍懲罰 |

**設計原則**：
1. 平均分改善 → 完全信任，重置 patience
2. 最高分突破 → 適度獎勵（-5），鼓勵探索
3. 最低分提升 → 中度獎勵（-3），鼓勵穩定
4. 最低分惡化 → 加速響應（+2），快速降低 LR

### 為什麼不使用優先經驗回放？

**優先經驗回放（Prioritized Experience Replay）**是另一種利用高分軌跡的方法，但：
- ❌ 實現複雜度高
- ❌ 需要大量額外記憶體
- ❌ 可能導致過度擬合高分場景

### 為什麼採用信號檢測方法？

✅ **三指標信號檢測**的優勢：
- 簡單直接，不改變核心訓練流程
- 所有軌跡仍然包含在 32,768 步的 rollout 中用於梯度更新
- 僅改變**評估和學習率策略**部分
- 低計算開銷
- **正反教材雙向學習**：既追求卓越又避免災難

### 核心哲學

> 如果 AI 偶爾能達到 170 分，說明它**已經具備這個能力**；  
> 如果 AI 偶爾會得 -50 分，說明它**存在致命弱點**。
> 
> 訓練的目標不僅是：  
> - ✅ 提升上限（學習成功經驗）  
> - ✅ 提升下限（避免災難性失敗）  
> - ✅ 縮小範圍（提高穩定性）

這就像培養學生：
- ❌ 不應該說「偶爾考 100 分但也考過 -10 分，平均 45 分還行」
- ✅ 應該說「你有考 100 分的能力（保持），但也有不及格的問題（改善）」

**訓練策略演化**：
```
階段 1（早期）：[50, 40, 30, -10, 20]
  → 整體表現差，最低分很低
  → 策略：快速學習，高學習率

階段 2（中期）：[120, 80, 70, 20, 60]
  → 出現高分，但最低分仍不穩定
  → 策略：鼓勵探索，維持學習率

階段 3（後期）：[150, 140, 135, 120, 130]
  → 高分穩定，最低分提升明顯
  → 策略：精細調整，降低學習率

階段 4（成熟）：[170, 165, 168, 162, 169]
  → 高度穩定，範圍極小
  → 策略：微調優化，最低學習率
```

## 預期效果

### 短期效果
- ✅ 更清楚地觀察 AI 的表現範圍
- ✅ 避免因偶爾低分而過早放棄好策略
- ✅ 學習率調整更符合實際訓練動態

### 長期效果
- ✅ 更快速地穩定高分表現
- ✅ 減少訓練過程中的「退步」現象
- ✅ 更好地利用訓練過程中的所有信息

## 使用建議

### 觀察重點

1. **峰值趨勢**：觀察 `reward/max` 是否持續提升
   - 如果提升 → AI 在發現更好的策略
   - 如果停滯 → 可能需要調整其他參數

2. **平均值接近峰值**：觀察 `reward/mean` 是否逐漸接近 `reward/max`
   - 差距縮小 → AI 在穩定高分表現 ✅
   - 差距擴大 → AI 表現不穩定，需要調查

3. **最低值提升**：觀察 `reward/min` 是否提升
   - 提升 → AI 的下限在改善，整體能力提升
   - 持續很低 → 可能存在特定失敗場景

### 調整建議

如果發現：
- **峰值很高但平均很低**：維持當前設置，給予更多訓練時間
- **峰值停滯不前**：考慮調整探索參數（entropy_coef）
- **所有指標都停滯**：可能需要調整其他超參數（學習率、batch size 等）

## 相容性

- ✅ 完全向後相容
- ✅ 不影響現有檢查點載入
- ✅ 不改變核心 PPO 算法
- ✅ 可以隨時切換回只看平均值的模式（只需修改代碼中的判斷邏輯）

## 驗證

編譯測試：
```bash
python -m py_compile agents/pytorch_trainer.py
# ✅ 通過
```

建議執行短期訓練測試：
```bash
python run_game.py --mode=train --iterations=100
```

觀察輸出中是否顯示：
- 最高獎勵和最低獎勵
- 當發現新峰值時的提示訊息
- TensorBoard 中的三條獎勵曲線

---

**實現日期**：2024  
**修改者**：AI Assistant  
**測試狀態**：代碼已編譯通過，待實際訓練驗證
