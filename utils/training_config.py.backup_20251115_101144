"""
PPO è¨“ç·´å„ªåŒ–é…ç½®
é‡å° RTX 3060 Ti å„ªåŒ–çš„è¶…åƒæ•¸å’Œè¨“ç·´ç­–ç•¥
"""

import os
from typing import Any, Dict, Optional

# RTX 3060 Ti å„ªåŒ–é…ç½®
RTX_3060TI_CONFIG = {
    "device": "cuda",  # ä½¿ç”¨ GPU
    "batch_size": 256,  # å¢å¤§ batch size åˆ©ç”¨ GPU
    "ppo_epochs": 10,  # å¢åŠ  PPO æ›´æ–°æ¬¡æ•¸
    "lr": 2.5e-4,  # é™ä½å­¸ç¿’ç‡ç¢ºä¿ç©©å®š
    "gamma": 0.99,  # æŠ˜æ‰£å› å­
    "lam": 0.95,  # GAE lambda
    "clip_eps": 0.2,  # PPO clip ç¯„åœ
    "vf_coef": 0.5,  # Value function ä¿‚æ•¸
    "ent_coef": 0.01,  # é™ä½ entropy é¼“å‹µæ›´ç¢ºå®šçš„ç­–ç•¥
    "max_grad_norm": 0.5,  # æ¢¯åº¦è£å‰ª
    "horizon": 4096,  # å¢åŠ  rollout é•·åº¦
}

# CPU è¨“ç·´é…ç½®ï¼ˆè¼ƒä¿å®ˆï¼‰
CPU_CONFIG = {
    "device": "cpu",
    "batch_size": 64,
    "ppo_epochs": 4,
    "lr": 3e-4,
    "gamma": 0.99,
    "lam": 0.95,
    "clip_eps": 0.2,
    "vf_coef": 0.5,
    "ent_coef": 0.05,
    "max_grad_norm": 0.5,
    "horizon": 2048,
}

# æ”¹é€²çš„çå‹µå¡‘é€ 
REWARD_SHAPING_CONFIG = {
    "pass_obstacle": 10.0,  # å¢åŠ é€šéçå‹µ
    "collision": -10.0,  # å¢åŠ ç¢°æ’æ‡²ç½°
    "survive_step": 0.1,  # æ¯æ­¥å­˜æ´»å°çå‹µ
    "height_penalty": 0.05,  # æ‡²ç½°éé«˜æˆ–éä½
    "forward_progress": 0.2,  # é¼“å‹µå‰é€²
}


class TrainingConfig:
    """è¨“ç·´é…ç½®ç®¡ç†"""

    def __init__(self, use_gpu: bool = True):
        self.config = RTX_3060TI_CONFIG if use_gpu else CPU_CONFIG
        self.reward_config = REWARD_SHAPING_CONFIG

    def get_ppo_kwargs(self) -> Dict[str, Any]:
        """ç²å– PPO è¨“ç·´å™¨åƒæ•¸"""
        return {
            "device": self.config["device"],
            "batch_size": self.config["batch_size"],
            "ppo_epochs": self.config["ppo_epochs"],
            "lr": self.config["lr"],
            "gamma": self.config["gamma"],
            "lam": self.config["lam"],
            "clip_eps": self.config["clip_eps"],
            "vf_coef": self.config["vf_coef"],
            "ent_coef": self.config["ent_coef"],
        }

    def get_training_params(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´åƒæ•¸"""
        return {
            "horizon": self.config["horizon"],
            "max_grad_norm": self.config["max_grad_norm"],
        }

    def should_use_vectorized_env(self) -> bool:
        """æ˜¯å¦æ‡‰è©²ä½¿ç”¨å‘é‡åŒ–ç’°å¢ƒ"""
        return self.config["device"] == "cuda"

    def get_recommended_n_envs(self) -> int:
        """æ¨è–¦çš„ä¸¦è¡Œç’°å¢ƒæ•¸é‡"""
        if self.config["device"] == "cuda":
            return 8  # GPU å¯ä»¥è™•ç†æ›´å¤š
        return 4  # CPU è¼ƒå°‘


def list_available_checkpoints(algorithm: str = "ppo") -> list:
    """åˆ—å‡ºå¯ç”¨çš„ checkpoint æª”æ¡ˆ"""
    if algorithm.lower() == "ppo":
        checkpoint_dir = "checkpoints"
    else:
        checkpoint_dir = f"checkpoints_{algorithm.lower()}"

    if not os.path.exists(checkpoint_dir):
        return []

    checkpoints = []
    for file in os.listdir(checkpoint_dir):
        if file.startswith("checkpoint_") and file.endswith(".pt"):
            try:
                iteration = int(file.replace("checkpoint_", "").replace(".pt", ""))
                full_path = os.path.join(checkpoint_dir, file)
                size_mb = os.path.getsize(full_path) / (1024 * 1024)
                checkpoints.append(
                    {
                        "file": file,
                        "path": full_path,
                        "iteration": iteration,
                        "size_mb": size_mb,
                    }
                )
            except ValueError:
                continue

    # æŒ‰è¿­ä»£æ¬¡æ•¸æ’åº
    checkpoints.sort(key=lambda x: x["iteration"], reverse=True)
    return checkpoints


def get_latest_checkpoint(algorithm: str = "ppo") -> Optional[str]:
    """ç²å–æœ€æ–°çš„ checkpoint"""
    checkpoints = list_available_checkpoints(algorithm)
    if checkpoints:
        return checkpoints[0]["path"]
    return None


def print_training_summary(config: TrainingConfig):
    """æ‰“å°è¨“ç·´é…ç½®æ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸš€ PPO è¨“ç·´é…ç½®")
    print("=" * 60)
    print(f"è¨­å‚™: {config.config['device'].upper()}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.config['batch_size']}")
    print(f"PPO æ›´æ–°æ¬¡æ•¸: {config.config['ppo_epochs']}")
    print(f"å­¸ç¿’ç‡: {config.config['lr']}")
    print(f"Horizon: {config.config['horizon']}")
    print(f"æ¨è–¦ä¸¦è¡Œç’°å¢ƒæ•¸: {config.get_recommended_n_envs()}")
    print("=" * 60)
    print("çå‹µå¡‘é€ :")
    for key, value in config.reward_config.items():
        print(f"  {key}: {value}")
    print("=" * 60 + "\n")
