# ğŸ® Train Game - æ·±åº¦å¼·åŒ–å­¸ç¿’è¨“ç·´å¹³å°

<div align="center">

**åŸºæ–¼ Pygame + PyTorch çš„ Flappy-like éŠæˆ²ç’°å¢ƒï¼Œæ¡ç”¨ PPO (Proximal Policy Optimization) æ¼”ç®—æ³•**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.0-EE4C2C.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[åŠŸèƒ½ç‰¹è‰²](#-åŠŸèƒ½ç‰¹è‰²) â€¢ [å¿«é€Ÿé–‹å§‹](#-å¿«é€Ÿé–‹å§‹) â€¢ [æ¶æ§‹è¨­è¨ˆ](#-ç³»çµ±æ¶æ§‹) â€¢ [æ·±åº¦å­¸ç¿’åŸç†](#-æ·±åº¦å­¸ç¿’åŸç†) â€¢ [ä½¿ç”¨èªªæ˜](#-ä½¿ç”¨èªªæ˜)

</div>

---

## ğŸ“‹ ç›®éŒ„

- [å°ˆæ¡ˆç°¡ä»‹](#-å°ˆæ¡ˆç°¡ä»‹)
- [åŠŸèƒ½ç‰¹è‰²](#-åŠŸèƒ½ç‰¹è‰²)
- [ç³»çµ±æ¶æ§‹](#-ç³»çµ±æ¶æ§‹)
- [ç’°å¢ƒé…ç½®](#-ç’°å¢ƒé…ç½®)
- [å¿«é€Ÿé–‹å§‹](#-å¿«é€Ÿé–‹å§‹)
- [éŠæˆ²åŠŸèƒ½èªªæ˜](#-éŠæˆ²åŠŸèƒ½èªªæ˜)
- [æ·±åº¦å­¸ç¿’åŸç†](#-æ·±åº¦å­¸ç¿’åŸç†)
- [æå¤±å‡½æ•¸è©³è§£](#-æå¤±å‡½æ•¸è©³è§£)
- [è‡ªé©æ‡‰èª¿åº¦å™¨](#-è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿åº¦å™¨)
- [Actor-Critic æ¶æ§‹åœ–](#-actor-critic-æ¶æ§‹åœ–aov-system)
- [åƒæ•¸èª¿æ•´æŒ‡å—](#-åƒæ•¸èª¿æ•´æŒ‡å—)
- [æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)

---

## ğŸ¯ å°ˆæ¡ˆç°¡ä»‹

**Train Game** æ˜¯ä¸€å€‹å®Œæ•´çš„æ·±åº¦å¼·åŒ–å­¸ç¿’è¨“ç·´å¹³å°ï¼Œçµåˆäº†ï¼š
- **éŠæˆ²ç’°å¢ƒ**ï¼šé¡ Flappy Bird çš„ 2D ç‰©ç†å¼•æ“
- **AI æ¼”ç®—æ³•**ï¼šPPO (Proximal Policy Optimization) æ·±åº¦å¼·åŒ–å­¸ç¿’
- **è¦–è¦ºåŒ–ä»‹é¢**ï¼šå¯¦æ™‚è§€å¯Ÿè¨“ç·´éç¨‹èˆ‡ç¥ç¶“ç¶²è·¯ç‹€æ…‹
- **è‡ªé©æ‡‰ç³»çµ±**ï¼šå­¸ç¿’ç‡è‡ªå‹•èª¿æ•´ï¼Œç„¡éœ€æ‰‹å‹•èª¿åƒ

æœ¬å°ˆæ¡ˆé©åˆï¼š
- ğŸ“ å­¸ç¿’æ·±åº¦å¼·åŒ–å­¸ç¿’çš„å­¸ç”Ÿ/ç ”ç©¶è€…
- ğŸ”¬ ç ”ç©¶ PPO æ¼”ç®—æ³•çš„å¯¦éš›æ‡‰ç”¨
- ğŸ® å° AI éŠæˆ²è¨“ç·´æ„Ÿèˆˆè¶£çš„é–‹ç™¼è€…
- ğŸ¤– æƒ³è¦ç†è§£ç¥ç¶“ç¶²è·¯è¨“ç·´éç¨‹çš„åˆå­¸è€…

---

## âœ¨ åŠŸèƒ½ç‰¹è‰²

### ğŸš€ æ ¸å¿ƒåŠŸèƒ½
1. **å®Œæ•´ PPO å¯¦ç¾**
   - Actor-Critic ç¥ç¶“ç¶²è·¯æ¶æ§‹
   - Generalized Advantage Estimation (GAE)
   - å¤šç’°å¢ƒä¸¦è¡Œè¨“ç·´ (SubprocVecEnv)
   - GPU/CPU è‡ªå‹•é©é…

2. **æ™ºèƒ½è‡ªé©æ‡‰ç³»çµ±**
   - 6 ç¨®å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆadaptive, step, exponential, cosine, reduce_on_plateau, noneï¼‰
   - å‹•æ…‹é…ç½®ç†±é‡è¼‰ï¼ˆæ¯ 10 æ¬¡è¿­ä»£è‡ªå‹•è®€å–é…ç½®æ–‡ä»¶ï¼‰
   - æ€§èƒ½è¿½è¹¤èˆ‡è‡ªå‹•èª¿æ•´

3. **è±å¯Œçš„è¦–è¦ºåŒ–**
   - å¯¦æ™‚éŠæˆ²ç•«é¢ï¼ˆAI/äººé¡é›™æ¨¡å¼ï¼‰
   - ç¥ç¶“ç¶²è·¯æ¬Šé‡ç†±åœ–
   - è¨“ç·´æŒ‡æ¨™æ›²ç·šï¼ˆLoss, Reward, Entropyï¼‰
   - TensorBoard æ·±åº¦åˆ†æ

4. **å®Œæ•´çš„è¨“ç·´ç”Ÿæ…‹**
   - è‡ªå‹•æª¢æŸ¥é»ä¿å­˜
   - è¨“ç·´æ¢å¾©æ©Ÿåˆ¶
   - æ’è¡Œæ¦œç³»çµ±
   - å®Œå–„çš„æ¸¬è©¦å¥—ä»¶

### ğŸ¨ ä½¿ç”¨è€…ä»‹é¢
- **é›™æ¨¡å¼åˆ‡æ›**ï¼šç©å®¶æ‰‹å‹•æ§åˆ¶ â†” AI è‡ªå‹•è¨“ç·´
- **å³æ™‚åé¥‹**ï¼šé¡¯ç¤ºç•¶å‰åˆ†æ•¸ã€è¨“ç·´è¿­ä»£æ•¸ã€å­¸ç¿’ç‡ç­‰
- **ç¥ç¶“ç¶²è·¯å¯è¦–åŒ–**ï¼šè§€å¯Ÿ 5000+ å€‹æ¬Šé‡çš„å¯¦æ™‚è®ŠåŒ–
- **è©³ç´°çµ±è¨ˆ**ï¼šæ¯æ¬¡è¿­ä»£çš„æå¤±å€¼ã€çå‹µã€æ¢¯åº¦ç¯„æ•¸ç­‰

---

## ğŸ— ç³»çµ±æ¶æ§‹

### ğŸ“ å°ˆæ¡ˆçµæ§‹

```
traingame/
â”œâ”€â”€ ğŸ® game/                      # éŠæˆ²ç’°å¢ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ environment.py            # æ ¸å¿ƒéŠæˆ²é‚è¼¯ï¼ˆç‰©ç†å¼•æ“ã€ç¢°æ’æª¢æ¸¬ï¼‰
â”‚   â”œâ”€â”€ ui.py                     # Pygame ä½¿ç”¨è€…ä»‹é¢
â”‚   â”œâ”€â”€ training_window.py        # è¨“ç·´è¦–è¦ºåŒ–è¦–çª—
â”‚   â””â”€â”€ vec_env.py                # å¤šç’°å¢ƒä¸¦è¡ŒåŒ…è£å™¨
â”‚
â”œâ”€â”€ ğŸ¤– agents/                    # AI æ¼”ç®—æ³•æ¨¡çµ„
â”‚   â”œâ”€â”€ networks.py               # Actor-Critic ç¥ç¶“ç¶²è·¯å®šç¾©
â”‚   â”œâ”€â”€ ppo_agent.py              # PPO Agent æ¨ç†ä»‹é¢
â”‚   â”œâ”€â”€ pytorch_trainer.py        # PPO è¨“ç·´å™¨ï¼ˆæ ¸å¿ƒè¨“ç·´é‚è¼¯ï¼‰
â”‚   â””â”€â”€ trainer.py                # è¨“ç·´å™¨åŸºé¡
â”‚
â”œâ”€â”€ ğŸ’¾ checkpoints/               # è¨“ç·´æª¢æŸ¥é»
â”‚   â”œâ”€â”€ checkpoint_XXXX.pt        # æ¨¡å‹æ¬Šé‡å¿«ç…§
â”‚   â”œâ”€â”€ training_meta.json        # è¨“ç·´å…ƒæ•¸æ“š
â”‚   â”œâ”€â”€ scores.json               # æ’è¡Œæ¦œæ•¸æ“š
â”‚   â””â”€â”€ tb/                       # TensorBoard æ—¥èªŒ
â”‚
â”œâ”€â”€ ğŸ§ª tests/                     # æ¸¬è©¦å¥—ä»¶
â”‚   â”œâ”€â”€ test_environment.py       # ç’°å¢ƒå–®å…ƒæ¸¬è©¦
â”‚   â”œâ”€â”€ test_training.py          # è¨“ç·´æµç¨‹æ¸¬è©¦
â”‚   â””â”€â”€ test_ui.py                # UI smoke æ¸¬è©¦
â”‚
â”œâ”€â”€ ğŸ“„ é…ç½®èˆ‡æ–‡æª”
â”‚   â”œâ”€â”€ training_config.json      # ğŸ”¥ è¨“ç·´è¶…åƒæ•¸é…ç½®ï¼ˆå¯ç†±é‡è¼‰ï¼‰
â”‚   â”œâ”€â”€ requirements.txt          # Python ä¾è³´
â”‚   â”œâ”€â”€ LR_SCHEDULER_GUIDE.md     # å­¸ç¿’ç‡èª¿åº¦å™¨è©³ç´°æŒ‡å—
â”‚   â”œâ”€â”€ PARAMETER_TUNING_GUIDE.md # åƒæ•¸èª¿æ•´æŒ‡å—
â”‚   â””â”€â”€ TRAINING_GUIDE.md         # è¨“ç·´æ•…éšœæ’é™¤æŒ‡å—
â”‚
â””â”€â”€ ğŸš€ å•Ÿå‹•è…³æœ¬
    â”œâ”€â”€ run_game.py               # å–®è¦–çª—éŠæˆ²/è¨“ç·´å…¥å£
    â””â”€â”€ run_multi_train.py        # å¤šè¦–çª—ä¸¦è¡Œè¨“ç·´ï¼ˆå¯é¸ï¼‰
```

### ğŸ”„ ç³»çµ±æµç¨‹åœ–

```mermaid
graph TD
    A[å•Ÿå‹• run_game.py] --> B{é¸æ“‡æ¨¡å¼}
    B -->|ç©å®¶æ¨¡å¼| C[æ‰‹å‹•æ§åˆ¶éŠæˆ²]
    B -->|AIæ¨¡å¼| D[è¼‰å…¥/åˆå§‹åŒ– PPO Agent]
    
    D --> E[SubprocVecEnv<br/>16å€‹ä¸¦è¡Œç’°å¢ƒ]
    E --> F[æ”¶é›†ç¶“é©—<br/>32768 steps]
    F --> G[è¨ˆç®— GAE Advantage]
    G --> H[PPO æ›´æ–°<br/>10 epochs]
    
    H --> I[åå‘å‚³æ’­<br/>æ›´æ–°5000+æ¬Šé‡]
    I --> J[å„²å­˜æª¢æŸ¥é»]
    J --> K{æª¢æŸ¥é…ç½®}
    K -->|æ¯10æ¬¡è¿­ä»£| L[é‡æ–°è¼‰å…¥ training_config.json]
    K -->|æ¯æ¬¡è¿­ä»£| M[å­¸ç¿’ç‡èª¿åº¦å™¨æ›´æ–°]
    
    L --> E
    M --> E
    
    C --> N[é¡¯ç¤ºåˆ†æ•¸/æ’è¡Œæ¦œ]
    J --> O[TensorBoard è¨˜éŒ„]
    O --> P[è¦–è¦ºåŒ–è¦–çª—æ›´æ–°]
```

### ğŸ§  AI è¨“ç·´å¾ªç’°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬ N æ¬¡è¨“ç·´è¿­ä»£                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  1ï¸âƒ£ æ•¸æ“šæ”¶é›†éšæ®µ (Data Collection)                      â”‚
â”‚     â”œâ”€ 16 å€‹ç’°å¢ƒä¸¦è¡Œé‹è¡Œ                                 â”‚
â”‚     â”œâ”€ æ¯å€‹ç’°å¢ƒé‹è¡Œ 2048 æ­¥                              â”‚
â”‚     â”œâ”€ ç¸½è¨ˆ: 32768 æ­¥ç¶“é©—                                â”‚
â”‚     â””â”€ è¨˜éŒ„: (state, action, reward, next_state, done)  â”‚
â”‚                                                         â”‚
â”‚  2ï¸âƒ£ å„ªå‹¢è¨ˆç®—éšæ®µ (Advantage Estimation)                 â”‚
â”‚     â”œâ”€ è¨ˆç®— TD èª¤å·®: Î´â‚œ = râ‚œ + Î³V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)        â”‚
â”‚     â”œâ”€ GAE å„ªå‹¢: Aâ‚œ = Î£(Î³Î»)â±Î´â‚œâ‚Šáµ¢                       â”‚
â”‚     â””â”€ å›å ±ä¼°è¨ˆ: Râ‚œ = Aâ‚œ + V(sâ‚œ)                        â”‚
â”‚                                                         â”‚
â”‚  3ï¸âƒ£ ç­–ç•¥æ›´æ–°éšæ®µ (Policy Update)                        â”‚
â”‚     â”œâ”€ é‡è¤‡ 10 è¼ª (ppo_epochs)                          â”‚
â”‚     â”œâ”€ æ¯è¼ªåˆ† 128 æ‰¹ (batch_size=256)                   â”‚
â”‚     â”œâ”€ è¨ˆç®— PPO Loss (è¦‹æå¤±å‡½æ•¸ç« ç¯€)                    â”‚
â”‚     â”œâ”€ åå‘å‚³æ’­: loss.backward()                        â”‚
â”‚     â”œâ”€ æ¢¯åº¦è£å‰ª: clip_grad_norm_(0.5)                   â”‚
â”‚     â””â”€ æ¬Šé‡æ›´æ–°: optimizer.step()                       â”‚
â”‚                                                         â”‚
â”‚  4ï¸âƒ£ æª¢æŸ¥é»ä¿å­˜ (æ¯10æ¬¡è¿­ä»£)                             â”‚
â”‚     â””â”€ ä¿å­˜: model_state_dict + optimizer_state_dict    â”‚
â”‚                                                         â”‚
â”‚  5ï¸âƒ£ é…ç½®é‡è¼‰ (æ¯10æ¬¡è¿­ä»£)                               â”‚
â”‚     â””â”€ è®€å– training_config.json æ›´æ–°è¶…åƒæ•¸              â”‚
â”‚                                                         â”‚
â”‚  6ï¸âƒ£ å­¸ç¿’ç‡èª¿æ•´ (æ¯æ¬¡è¿­ä»£)                               â”‚
â”‚     â””â”€ æ ¹æ“šèª¿åº¦å™¨é¡å‹è‡ªå‹•èª¿æ•´å­¸ç¿’ç‡                       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
         ä¸‹ä¸€æ¬¡è¿­ä»£ (N+1)
```

---

## ğŸ›  ç’°å¢ƒé…ç½®

### ç³»çµ±éœ€æ±‚

| çµ„ä»¶ | æœ€ä½é…ç½® | æ¨è–¦é…ç½® |
|------|---------|---------|
| **ä½œæ¥­ç³»çµ±** | Windows 10/11, Linux, macOS | Windows 11 |
| **Python** | 3.8+ | 3.10+ |
| **è¨˜æ†¶é«”** | 4 GB | 8 GB+ |
| **é¡¯å¡** | ç„¡ï¼ˆCPUè¨“ç·´ï¼‰ | NVIDIA GPU (CUDA 11.8+) |
| **ç¡¬ç¢Ÿ** | 500 MB | 2 GB (å«æª¢æŸ¥é») |

### æ ¸å¿ƒä¾è³´

```txt
numpy==1.26.4          # æ•¸å€¼è¨ˆç®—
pygame==2.6.1          # éŠæˆ²å¼•æ“
torch==2.2.0           # æ·±åº¦å­¸ç¿’æ¡†æ¶
tensorboard==2.16.0    # è¨“ç·´è¦–è¦ºåŒ–

# é–‹ç™¼å·¥å…·ï¼ˆå¯é¸ï¼‰
pytest==7.4.2          # æ¸¬è©¦æ¡†æ¶
black==24.1.0          # ç¨‹å¼ç¢¼æ ¼å¼åŒ–
ruff==0.13.1           # Linter
```

### GPU æ”¯æ´ (å¯é¸ä½†æ¨è–¦)

æœ¬å°ˆæ¡ˆæ”¯æ´ NVIDIA CUDA åŠ é€Ÿè¨“ç·´ã€‚å¦‚æœä½ æœ‰ NVIDIA é¡¯å¡ï¼š

**Windows (PowerShell):**
```powershell
# å¸è¼‰ CPU ç‰ˆæœ¬
pip uninstall torch torchvision torchaudio -y

# å®‰è£ CUDA ç‰ˆæœ¬ (ä¾ä½ çš„ CUDA ç‰ˆæœ¬é¸æ“‡)
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**é©—è­‰ GPU å¯ç”¨æ€§:**
```powershell
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1ï¸âƒ£ å…‹éš†å°ˆæ¡ˆ

```powershell
git clone https://github.com/SmailDot/train_game.git
cd traingame
```

### 2ï¸âƒ£ å»ºç«‹è™›æ“¬ç’°å¢ƒ

```powershell
# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv .venv

# å•Ÿç”¨è™›æ“¬ç’°å¢ƒ
.\.venv\Scripts\Activate.ps1  # Windows PowerShell
# æˆ–
source .venv/bin/activate     # Linux/macOS
```

### 3ï¸âƒ£ å®‰è£ä¾è³´

```powershell
# å‡ç´š pip
python -m pip install --upgrade pip

# åŸºæœ¬å®‰è£ï¼ˆCPU ç‰ˆæœ¬ï¼‰
pip install -r requirements.txt

# GPU ç‰ˆæœ¬ï¼ˆæ¨è–¦ï¼Œéœ€ NVIDIA é¡¯å¡ï¼‰
pip uninstall torch torchvision torchaudio -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### 4ï¸âƒ£ é©—è­‰å®‰è£

```powershell
# é‹è¡Œæ¸¬è©¦å¥—ä»¶
python -m pytest -q

# è¼¸å‡ºæ‡‰è©²é¡ä¼¼:
# .......................                                    [100%]
# 23 passed in 2.50s
```

### 5ï¸âƒ£ é–‹å§‹è¨“ç·´

```powershell
# å•Ÿå‹•éŠæˆ²/è¨“ç·´ä»‹é¢
python run_game.py
```

**ç¬¬ä¸€æ¬¡å•Ÿå‹•æµç¨‹ï¼š**
1. è¦–çª—æ‰“é–‹å¾Œï¼ŒæŒ‰ `A` éµåˆ‡æ›åˆ° AI æ¨¡å¼
2. AI æœƒè‡ªå‹•é–‹å§‹è¨“ç·´ï¼ˆ16 å€‹ä¸¦è¡Œç’°å¢ƒï¼‰
3. è§€å¯Ÿè¦–çª—å³å´çš„è¨“ç·´æŒ‡æ¨™
4. æª¢æŸ¥é»æ¯ 10 æ¬¡è¿­ä»£è‡ªå‹•ä¿å­˜åˆ° `checkpoints/`

### 6ï¸âƒ£ ç›£æ§è¨“ç·´é€²åº¦ï¼ˆå¯é¸ï¼‰

```powershell
# åœ¨æ–°çµ‚ç«¯å•Ÿå‹• TensorBoard
tensorboard --logdir=checkpoints/tb --port=6006

# æ‰“é–‹ç€è¦½å™¨è¨ªå•: http://localhost:6006
```

---

## ğŸ® éŠæˆ²åŠŸèƒ½èªªæ˜

### ğŸ•¹ï¸ æ§åˆ¶æ–¹å¼

| æ¨¡å¼ | æŒ‰éµ | åŠŸèƒ½ |
|------|------|------|
| **ç©å®¶æ¨¡å¼** | `SPACE` / `â†‘` | è·³èº |
| **æ¨¡å¼åˆ‡æ›** | `A` éµ | AI æ¨¡å¼ â†” ç©å®¶æ¨¡å¼ |
| **è¨“ç·´æ§åˆ¶** | `T` éµ | å•Ÿå‹•/åœæ­¢è¨“ç·´ |
| **è¦–çª—ç®¡ç†** | `W` éµ | é–‹å•Ÿ/é—œé–‰è¨“ç·´è¦–è¦ºåŒ–è¦–çª— |
| **ç³»çµ±** | `ESC` / `Q` | é€€å‡ºéŠæˆ² |

### ğŸ¯ éŠæˆ²è¦å‰‡

#### ç‰©ç†åƒæ•¸
- **ç•«é¢å°ºå¯¸**: 600px é«˜åº¦
- **çƒåŠå¾‘**: 12 åƒç´ 
- **é‡åŠ›åŠ é€Ÿåº¦**: 0.6 px/stepÂ²
- **è·³èºåŠ›**: -7.0 px/step
- **æœ€å¤§é€Ÿåº¦**: Â±20 px/step
- **æ»¾å‹•é€Ÿåº¦**: 3.0 px/step (åŸºç¤)
- **é€Ÿåº¦å¢é•·**: é€šéæ¯å€‹éšœç¤™ç‰©å¾Œ +2%

#### éšœç¤™ç‰©ç”Ÿæˆ
- **åˆå§‹è·é›¢**: 150 åƒç´ 
- **éšœç¤™ç‰©é–“è·**: 250 åƒç´ 
- **é–“éš™å¤§å°**: 160-200 åƒç´ ï¼ˆéš¨æ©Ÿï¼‰
- **é–“éš™ä½ç½®**: è·é›¢é ‚éƒ¨/åº•éƒ¨è‡³å°‘ 150 åƒç´ 

#### çå‹µæ©Ÿåˆ¶

| äº‹ä»¶ | çå‹µå€¼ | èªªæ˜ |
|------|--------|------|
| **å­˜æ´»** | +0.1 / step | æ¯ä¸€æ­¥éƒ½å­˜æ´»çš„åŸºç¤çå‹µ |
| **é€šééšœç¤™ç‰©** | +5.0 | æˆåŠŸé€šéé–“éš™ |
| **ç¢°æ’éšœç¤™ç‰©** | -5.0 | æ’åˆ°éšœç¤™ç‰©é ‚éƒ¨/åº•éƒ¨ |
| **ç¢°æ’é‚Šç•Œ** | -5.0 | æ’åˆ°å¤©èŠ±æ¿/åœ°æ¿ |

**ç¯„ä¾‹è¨ˆç®—ï¼š**
```
ä¸€å±€éŠæˆ²æŒçºŒ 50 æ­¥ï¼Œé€šé 2 å€‹éšœç¤™ç‰©ï¼š
ç¸½çå‹µ = (0.1 Ã— 50) + (5.0 Ã— 2) = 5 + 10 = 15 åˆ†
```

### ğŸ“Š ä»‹é¢å…ƒç´ 

#### ä¸»éŠæˆ²è¦–çª— (1280Ã—720)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ® Train Game - PPO Training                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      â”‚  ğŸ“ˆ è¨“ç·´ç‹€æ…‹                      â”‚
â”‚                      â”‚  â”œâ”€ æ¨¡å¼: AI / ç©å®¶               â”‚
â”‚    éŠæˆ²ç•«é¢          â”‚  â”œâ”€ è¨“ç·´ä¸­: âœ“ / âœ—                 â”‚
â”‚    (600Ã—600)         â”‚  â”œâ”€ ç•¶å‰åˆ†æ•¸: XXX                 â”‚
â”‚                      â”‚  â”œâ”€ è¨“ç·´è¿­ä»£: XXXX                â”‚
â”‚    ğŸ€ (çƒé«”)         â”‚  â”œâ”€ å­¸ç¿’ç‡: 0.000XXX              â”‚
â”‚    â•‘ â•‘ (éšœç¤™ç‰©)     â”‚  â””â”€ å¹³å‡çå‹µ: XX.XX               â”‚
â”‚    â•‘ â•‘              â”‚                                   â”‚
â”‚                      â”‚  ğŸ† æ’è¡Œæ¦œ Top 5                   â”‚
â”‚                      â”‚  1. AI-PPO: 1250                  â”‚
â”‚                      â”‚  2. Player: 980                   â”‚
â”‚                      â”‚  ...                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### è¨“ç·´è¦–è¦ºåŒ–è¦–çª— (æŒ‰ `W` é–‹å•Ÿ)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š Neural Network Visualization                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸ§  æ¬Šé‡ç†±åœ– (5Ã—64 çŸ©é™£)                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ ğŸŸ¥ğŸŸ§ğŸŸ¨ğŸŸ©ğŸŸ¦ğŸŸª ... (64å€‹ç¥ç¶“å…ƒ)            â”‚             â”‚
â”‚  â”‚ ğŸŸ¥ğŸŸ§ğŸŸ¨ğŸŸ©ğŸŸ¦ğŸŸª ...                        â”‚             â”‚
â”‚  â”‚ ... (5å€‹è¼¸å…¥ç¶­åº¦)                      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                         â”‚
â”‚  ğŸ“‰ æå¤±æ›²ç·š                                             â”‚
â”‚  â”œâ”€ Policy Loss: 0.XXX                                 â”‚
â”‚  â”œâ”€ Value Loss: 0.XXX                                  â”‚
â”‚  â””â”€ Entropy: 0.XXX                                     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ² ç‹€æ…‹ç©ºé–“ (State Space)

AI è§€å¯Ÿçš„ 5 ç¶­ç‹€æ…‹å‘é‡ï¼ˆå·²æ­¸ä¸€åŒ–åˆ° [0, 1]ï¼‰ï¼š

```python
state = [
    y / ScreenHeight,           # çƒçš„å‚ç›´ä½ç½® [0-1]
    vy / MaxAbsVel,             # çƒçš„å‚ç›´é€Ÿåº¦ [-1 ~ 1]
    x_obs / MaxDist,            # æœ€è¿‘éšœç¤™ç‰©çš„æ°´å¹³è·é›¢ [0-1]
    gap_top / ScreenHeight,     # é–“éš™é ‚éƒ¨ä½ç½® [0-1]
    gap_bottom / ScreenHeight   # é–“éš™åº•éƒ¨ä½ç½® [0-1]
]
```

**ç›´è§€ç†è§£ï¼š**
- `y = 0.5` â†’ çƒåœ¨ç•«é¢æ­£ä¸­å¤®
- `vy = -0.35` â†’ çƒæ­£åœ¨ä¸Šå‡ï¼ˆé€Ÿåº¦ç‚º -7ï¼‰
- `x_obs = 0.2` â†’ éšœç¤™ç‰©è·é›¢çƒ 30 åƒç´ 
- `gap_top = 0.4, gap_bottom = 0.6` â†’ é–“éš™åœ¨ä¸­é–“ï¼Œé«˜åº¦ 120 åƒç´ 

### ğŸ¯ å‹•ä½œç©ºé–“ (Action Space)

é›¢æ•£å‹•ä½œç©ºé–“ï¼ˆBernoulli åˆ†ä½ˆï¼‰ï¼š

```python
action = 0  # ä¸è·³ï¼ˆè®“é‡åŠ›ä½œç”¨ï¼‰
action = 1  # è·³èºï¼ˆæ–½åŠ  -7.0 çš„å‘ä¸Šè¡é‡ï¼‰
```

---

## ğŸ§  æ·±åº¦å­¸ç¿’åŸç†

### ğŸ“ PPO æ¼”ç®—æ³•æ¦‚è¿°

**Proximal Policy Optimization (PPO)** æ˜¯ä¸€ç¨® on-policy å¼·åŒ–å­¸ç¿’æ¼”ç®—æ³•ï¼Œçµåˆäº†ï¼š
- **ç­–ç•¥æ¢¯åº¦** (Policy Gradient)ï¼šç›´æ¥å„ªåŒ–ç­–ç•¥å‡½æ•¸
- **ä¿¡è³´åŸŸæ–¹æ³•** (Trust Region)ï¼šé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œé¿å…ç ´å£æ€§æ›´æ–°
- **Actor-Critic** æ¶æ§‹ï¼šåŒæ™‚å­¸ç¿’ç­–ç•¥ï¼ˆActorï¼‰å’Œåƒ¹å€¼å‡½æ•¸ï¼ˆCriticï¼‰

### ğŸ—ï¸ Actor-Critic ç¥ç¶“ç¶²è·¯æ¶æ§‹

```python
class ActorCritic(nn.Module):
    def __init__(self, input_dim=5, hidden=64):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden)   # 5 â†’ 64
        self.fc2 = nn.Linear(hidden, hidden)      # 64 â†’ 64
        self.actor = nn.Linear(hidden, 1)         # 64 â†’ 1 (å‹•ä½œlogit)
        self.critic = nn.Linear(hidden, 1)        # 64 â†’ 1 (ç‹€æ…‹åƒ¹å€¼)
```

**ç¶²è·¯çµæ§‹è¦–è¦ºåŒ–ï¼š**

```
è¼¸å…¥å±¤          éš±è—å±¤ 1         éš±è—å±¤ 2         è¼¸å‡ºå±¤
(5)             (64)            (64)            (2)

  y  â”€â”€â”€â”€â”€â”
  vy â”€â”€â”€â”€â”€â”¤
  x  â”€â”€â”€â”€â”€â”¼â”€â”€â”€â–º [ReLU] â”€â”€â”€â–º [ReLU] â”€â”€â”€â”¬â”€â”€â”€â–º Actor  (å‹•ä½œæ¦‚ç‡)
  gt â”€â”€â”€â”€â”€â”¤      64å€‹         64å€‹    â”‚
  gb â”€â”€â”€â”€â”€â”˜     ç¥ç¶“å…ƒ       ç¥ç¶“å…ƒ    â””â”€â”€â”€â–º Critic (ç‹€æ…‹åƒ¹å€¼)

ç¸½åƒæ•¸é‡: 
  fc1: 5Ã—64 + 64 = 384
  fc2: 64Ã—64 + 64 = 4,160
  actor: 64Ã—1 + 1 = 65
  critic: 64Ã—1 + 1 = 65
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ç¸½è¨ˆ: ~4,674 å€‹åƒæ•¸
```

**å‰å‘å‚³æ’­æµç¨‹ï¼š**

```python
def forward(self, x):
    # x shape: (batch_size, 5)
    x = F.relu(self.fc1(x))      # â†’ (batch_size, 64)
    x = F.relu(self.fc2(x))      # â†’ (batch_size, 64)
    
    logits = self.actor(x)       # â†’ (batch_size, 1)  å‹•ä½œ logit
    value = self.critic(x)       # â†’ (batch_size, 1)  ç‹€æ…‹åƒ¹å€¼
    
    return logits, value
```

### ğŸ² ç­–ç•¥è¼¸å‡º (Policy Output)

å¾ logit åˆ°å‹•ä½œæ¦‚ç‡ï¼š

```python
# 1. Sigmoid è½‰æ›ç‚ºæ¦‚ç‡
prob = torch.sigmoid(logits)  # logit âˆˆ â„ â†’ prob âˆˆ [0, 1]

# 2. Bernoulli åˆ†ä½ˆæ¡æ¨£
distribution = Bernoulli(probs=prob)
action = distribution.sample()  # 0 æˆ– 1

# 3. è¨ˆç®— log æ¦‚ç‡ï¼ˆç”¨æ–¼ PPO lossï¼‰
log_prob = distribution.log_prob(action)
```

**ç¯„ä¾‹ï¼š**
```
logits = 0.8  â†’  prob = sigmoid(0.8) = 0.69  â†’  69% æ©Ÿç‡è·³èº
logits = -1.2 â†’  prob = sigmoid(-1.2) = 0.23 â†’  23% æ©Ÿç‡è·³èº
```

### ğŸ“Š Generalized Advantage Estimation (GAE)

**ç›®æ¨™ï¼š** ä¼°è¨ˆæ¯å€‹å‹•ä½œçš„"å„ªå‹¢"ï¼ˆç›¸æ¯”å¹³å‡è¡¨ç¾æœ‰å¤šå¥½ï¼‰

#### æ­¥é©Ÿ 1ï¼šè¨ˆç®— TD èª¤å·®

```
Î´â‚œ = râ‚œ + Î³Â·V(sâ‚œâ‚Šâ‚) - V(sâ‚œ)
```

å…¶ä¸­ï¼š
- `râ‚œ`: å³æ™‚çå‹µ
- `Î³`: æŠ˜æ‰£å› å­ (0.99)
- `V(sâ‚œ)`: Critic é æ¸¬çš„ç‹€æ…‹åƒ¹å€¼

#### æ­¥é©Ÿ 2ï¼šè¨ˆç®— GAE å„ªå‹¢

```
Aâ‚œ = Î£áµ¢â‚Œâ‚€^âˆ (Î³Î»)â± Â· Î´â‚œâ‚Šáµ¢
```

å…¶ä¸­ï¼š
- `Î»`: GAE åƒæ•¸ (0.95)ï¼Œå¹³è¡¡åå·®èˆ‡æ–¹å·®

**Python å¯¦ç¾ï¼š**

```python
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    advantages = []
    gae = 0
    
    # å¾å¾Œå¾€å‰è¨ˆç®—
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]
        
        # TD èª¤å·®
        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        
        # GAE ç´¯ç©
        gae = delta + gamma * lam * (1 - dones[t]) * gae
        advantages.insert(0, gae)
    
    return advantages
```

#### æ­¥é©Ÿ 3ï¼šè¨ˆç®—å›å ± (Returns)

```
Râ‚œ = Aâ‚œ + V(sâ‚œ)
```

é€™å€‹å›å ±ç”¨æ–¼è¨“ç·´ Critic ç¶²è·¯ã€‚

---

## ğŸ“‰ æå¤±å‡½æ•¸è©³è§£

### ğŸ¯ PPO ç¸½æå¤±å‡½æ•¸

```python
L_total = L_policy + câ‚Â·L_value - câ‚‚Â·H(Ï€)
```

å…¶ä¸­ï¼š
- `L_policy`: ç­–ç•¥æå¤±ï¼ˆä¸»è¦å„ªåŒ–ç›®æ¨™ï¼‰
- `L_value`: åƒ¹å€¼å‡½æ•¸æå¤±
- `H(Ï€)`: ç­–ç•¥ç†µï¼ˆé¼“å‹µæ¢ç´¢ï¼‰
- `câ‚ = 0.5`: åƒ¹å€¼æå¤±ä¿‚æ•¸
- `câ‚‚ = 0.01`: ç†µä¿‚æ•¸

---

### 1ï¸âƒ£ ç­–ç•¥æå¤± (Policy Loss)

**PPO Clipped Objectiveï¼š**

```
L_policy = -E[ min(râ‚œ(Î¸)Â·Aâ‚œ, clip(râ‚œ(Î¸), 1-Îµ, 1+Îµ)Â·Aâ‚œ) ]
```

**è©³ç´°æ‹†è§£ï¼š**

#### æ¦‚ç‡æ¯”ç‡ (Probability Ratio)

```python
# èˆŠç­–ç•¥çš„ log æ¦‚ç‡ï¼ˆæ”¶é›†æ•¸æ“šæ™‚ï¼‰
old_log_prob = log Ï€_old(aâ‚œ | sâ‚œ)

# æ–°ç­–ç•¥çš„ log æ¦‚ç‡ï¼ˆç•¶å‰ç¶²è·¯ï¼‰
new_log_prob = log Ï€_Î¸(aâ‚œ | sâ‚œ)

# æ¦‚ç‡æ¯”ç‡
ratio = exp(new_log_prob - old_log_prob) = Ï€_Î¸(aâ‚œ | sâ‚œ) / Ï€_old(aâ‚œ | sâ‚œ)
```

**ç›´è§€ç†è§£ï¼š**
- `ratio > 1`: æ–°ç­–ç•¥æ›´å‚¾å‘é¸æ“‡é€™å€‹å‹•ä½œ
- `ratio < 1`: æ–°ç­–ç•¥æ›´ä¸å‚¾å‘é¸æ“‡é€™å€‹å‹•ä½œ
- `ratio = 1`: ç­–ç•¥æ²’æœ‰æ”¹è®Š

#### è£å‰ªæ©Ÿåˆ¶ (Clipping)

```python
# è¨­å®šè£å‰ªç¯„åœ Îµ = 0.2
clip_range = 0.2

# åŸå§‹ç›®æ¨™
surr1 = ratio * advantage

# è£å‰ªå¾Œçš„ç›®æ¨™
surr2 = clip(ratio, 1-Îµ, 1+Îµ) * advantage
      = clip(ratio, 0.8, 1.2) * advantage

# å–è¼ƒå°å€¼ï¼ˆä¿å®ˆæ›´æ–°ï¼‰
policy_loss = -min(surr1, surr2).mean()
```

**è£å‰ªç¯„åœè¦–è¦ºåŒ–ï¼š**

```
Advantage > 0 (å¥½çš„å‹•ä½œ):
    ratio âˆˆ [0, 0.8]  â†’ ä¸é¼“å‹µé™ä½æ¦‚ç‡ï¼ˆé™åˆ¶åœ¨0.8ï¼‰
    ratio âˆˆ [0.8, 1.2] â†’ æ­£å¸¸æ›´æ–°
    ratio âˆˆ [1.2, âˆ]  â†’ ä¸éåº¦æé«˜æ¦‚ç‡ï¼ˆé™åˆ¶åœ¨1.2ï¼‰

Advantage < 0 (å£çš„å‹•ä½œ):
    ratio âˆˆ [0, 0.8]  â†’ æ­£å¸¸é™ä½æ¦‚ç‡
    ratio âˆˆ [0.8, 1.2] â†’ æ­£å¸¸æ›´æ–°
    ratio âˆˆ [1.2, âˆ]  â†’ ä¸é¼“å‹µæé«˜æ¦‚ç‡ï¼ˆé™åˆ¶åœ¨1.2ï¼‰
```

**å®Œæ•´ Python å¯¦ç¾ï¼š**

```python
def ppo_policy_loss(states, actions, old_log_probs, advantages, clip_range=0.2):
    # å‰å‘å‚³æ’­
    logits, _ = self.net(states)
    prob = torch.sigmoid(logits)
    dist = torch.distributions.Bernoulli(probs=prob)
    
    # æ–°ç­–ç•¥çš„ log æ¦‚ç‡
    new_log_probs = dist.log_prob(actions)
    
    # æ¦‚ç‡æ¯”ç‡
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # å…©å€‹ç›®æ¨™
    surr1 = ratio * advantages
    surr2 = torch.clamp(ratio, 1.0 - clip_range, 1.0 + clip_range) * advantages
    
    # å–æœ€å°å€¼ä¸¦åŠ è² è™Ÿï¼ˆæ¢¯åº¦ä¸Šå‡ â†’ æ¢¯åº¦ä¸‹é™ï¼‰
    policy_loss = -torch.min(surr1, surr2).mean()
    
    return policy_loss
```

---

### 2ï¸âƒ£ åƒ¹å€¼å‡½æ•¸æå¤± (Value Loss)

**å‡æ–¹èª¤å·® (MSE)ï¼š**

```
L_value = MSE(V(sâ‚œ), Râ‚œ) = 1/N Î£ (V(sâ‚œ) - Râ‚œ)Â²
```

å…¶ä¸­ï¼š
- `V(sâ‚œ)`: Critic é æ¸¬çš„ç‹€æ…‹åƒ¹å€¼
- `Râ‚œ`: GAE è¨ˆç®—çš„å›å ± (Aâ‚œ + V(sâ‚œ))

**Python å¯¦ç¾ï¼š**

```python
def value_loss(states, returns):
    _, value = self.net(states)
    loss = F.mse_loss(value, returns)
    return loss
```

**ä½œç”¨ï¼š** è¨“ç·´ Critic æ›´æº–ç¢ºåœ°é æ¸¬æœªä¾†ç´¯ç©çå‹µã€‚

---

### 3ï¸âƒ£ ç†µæå¤± (Entropy Loss)

**Bernoulli åˆ†ä½ˆç†µï¼š**

```
H(Ï€) = -Î£ [pÂ·log(p) + (1-p)Â·log(1-p)]
```

å…¶ä¸­ `p` æ˜¯è·³èºæ¦‚ç‡ã€‚

**Python å¯¦ç¾ï¼š**

```python
def entropy_loss(states):
    logits, _ = self.net(states)
    prob = torch.sigmoid(logits)
    dist = torch.distributions.Bernoulli(probs=prob)
    entropy = dist.entropy().mean()
    return entropy
```

**ä½œç”¨ï¼š** 
- é«˜ç†µ = ç­–ç•¥æ›´éš¨æ©Ÿ = æ›´å¤šæ¢ç´¢
- ä½ç†µ = ç­–ç•¥æ›´ç¢ºå®š = æ›´å¤šåˆ©ç”¨
- ä¿‚æ•¸ `câ‚‚ = 0.01` å¾®å¼±åœ°é¼“å‹µæ¢ç´¢

---

### ğŸ”„ åå‘å‚³æ’­èˆ‡æ¬Šé‡æ›´æ–°

```python
# å®Œæ•´è¨“ç·´æ­¥é©Ÿ
for epoch in range(ppo_epochs):  # é‡è¤‡ 10 æ¬¡
    for batch in data_loader:    # æ¯æ‰¹ 256 å€‹æ¨£æœ¬
        
        # 1. è¨ˆç®—ä¸‰å€‹æå¤±
        policy_loss = ppo_policy_loss(...)
        value_loss = mse_loss(...)
        entropy = entropy_bonus(...)
        
        # 2. çµ„åˆç¸½æå¤±
        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
        
        # 3. æ¸…ç©ºæ¢¯åº¦
        optimizer.zero_grad()
        
        # 4. åå‘å‚³æ’­ï¼ˆè‡ªå‹•å¾®åˆ†ï¼‰
        loss.backward()
        # æ­¤æ­¥é©Ÿè‡ªå‹•è¨ˆç®—æ‰€æœ‰æ¬Šé‡çš„æ¢¯åº¦ï¼š
        # âˆ‚loss/âˆ‚wâ‚, âˆ‚loss/âˆ‚wâ‚‚, ..., âˆ‚loss/âˆ‚wâ‚„â‚†â‚‡â‚„
        
        # 5. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=0.5)
        
        # 6. æ›´æ–°æ¬Šé‡
        optimizer.step()
        # Adam å„ªåŒ–å™¨æ ¹æ“šæ¢¯åº¦æ›´æ–°æ¬Šé‡ï¼š
        # wáµ¢ â† wáµ¢ - learning_rate Ã— gradient
```

**æ¯æ¬¡è¿­ä»£çš„æ¬Šé‡æ›´æ–°é‡ï¼š**

```
ç¸½æ›´æ–°æ¬¡æ•¸ = ppo_epochs Ã— (horizon / batch_size)
           = 10 Ã— (32768 / 256)
           = 10 Ã— 128
           = 1,280 æ¬¡æ¬Šé‡æ›´æ–°

æ¯å€‹æ¬Šé‡è¢«æ›´æ–° 1,280 æ¬¡ Ã— æ¢¯åº¦å¤§å° (~0.0001-0.001)
```

---

## ğŸ› è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿åº¦å™¨

### ğŸ”§ é…ç½®ç³»çµ±

æœ¬å°ˆæ¡ˆå¯¦ç¾äº† 6 ç¨®å­¸ç¿’ç‡èª¿åº¦å™¨ï¼Œé€šé `training_config.json` é…ç½®ï¼š

```json
{
    "lr_scheduler": {
        "type": "adaptive",              // èª¿åº¦å™¨é¡å‹
        "patience": 30,                  // ç„¡æ”¹å–„çš„å®¹å¿æ¬¡æ•¸
        "factor": 0.5,                   // è¡°æ¸›å› å­
        "min_lr": 1e-6,                  // æœ€å°å­¸ç¿’ç‡
        "improvement_threshold": 0.01    // æ”¹å–„é–¾å€¼ï¼ˆ1%ï¼‰
    }
}
```

---

### ğŸ“Š èª¿åº¦å™¨é¡å‹è©³è§£

#### 1ï¸âƒ£ Adaptive (è‡ªå®šç¾©è‡ªé©æ‡‰) â­ æ¨è–¦

**é‹ä½œåŸç†ï¼š**
```python
if current_reward > best_reward Ã— (1 + threshold):
    best_reward = current_reward
    patience_counter = 0
    print("æ–°æœ€ä½³çå‹µï¼")
else:
    patience_counter += 1
    
if patience_counter >= patience:
    lr = max(lr Ã— factor, min_lr)
    print(f"é™ä½å­¸ç¿’ç‡: {old_lr} â†’ {lr}")
    patience_counter = 0
```

**é©ç”¨å ´æ™¯ï¼š**
- âœ… çå‹µæ›²ç·šä¸è¦å‰‡ï¼ˆå¿½é«˜å¿½ä½ï¼‰
- âœ… ä¸çŸ¥é“è¨“ç·´æœƒè·‘å¤šä¹…
- âœ… æƒ³è¦è‡ªå‹•åŒ–èª¿åƒ

**é…ç½®ç¯„ä¾‹ï¼š**
```json
{
    "type": "adaptive",
    "patience": 30,              // 30æ¬¡ç„¡æ”¹å–„â†’é™ä½LR
    "factor": 0.5,               // æ¯æ¬¡æ¸›åŠ
    "min_lr": 1e-6,              // æœ€ä½ 0.000001
    "improvement_threshold": 0.01 // æ”¹å–„éœ€>1%æ‰ç®—
}
```

---

#### 2ï¸âƒ£ Step (éšæ¢¯å¼è¡°æ¸›)

**é‹ä½œåŸç†ï¼š**
```python
if iteration % step_size == 0:
    lr = lr Ã— gamma
```

**æ™‚é–“ç·šï¼š**
```
Iteration:  0   100  200  300  400  500
LR:       0.001 0.0009 0.00081 0.000729 ...
          â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜
          æ¯ 100 æ¬¡è¡°æ¸› Ã—0.9
```

**é©ç”¨å ´æ™¯ï¼š**
- âœ… è¨“ç·´æ™‚é•·å·²çŸ¥ï¼ˆå¦‚ 1000 æ¬¡è¿­ä»£ï¼‰
- âœ… çå‹µæ›²ç·šå¹³ç©©
- âŒ ä¸é©åˆæå‰åœæ­¢çš„è¨“ç·´

**é…ç½®ç¯„ä¾‹ï¼š**
```json
{
    "type": "step",
    "step_size": 100,   // æ¯ 100 æ¬¡è¿­ä»£
    "gamma": 0.9        // å­¸ç¿’ç‡ Ã—0.9
}
```

---

#### 3ï¸âƒ£ Exponential (æŒ‡æ•¸è¡°æ¸›)

**é‹ä½œåŸç†ï¼š**
```python
lr = initial_lr Ã— (gamma ** iteration)
```

**æ›²ç·šï¼š**
```
0.001 â”¤â—
      â”‚ â—
0.0008â”¤  â—
      â”‚   â—
0.0006â”¤    â—â—
      â”‚      â—â—
0.0004â”¤        â—â—â—
      â”‚           â—â—â—â—
0.0002â”¤               â—â—â—â—â—â—â—â—â—â—â—
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   200  400  600  800  1000 iterations
```

**é©ç”¨å ´æ™¯ï¼š**
- âœ… é•·æœŸè¨“ç·´ï¼ˆ>5000 æ¬¡è¿­ä»£ï¼‰
- âœ… å¸Œæœ›å¹³æ»‘è¡°æ¸›
- âŒ çŸ­æœŸè¨“ç·´æœƒè¡°æ¸›å¤ªæ…¢

**é…ç½®ç¯„ä¾‹ï¼š**
```json
{
    "type": "exponential",
    "gamma": 0.999      // æ¯æ¬¡è¿­ä»£ Ã—0.999
}
```

---

#### 4ï¸âƒ£ Cosine (é¤˜å¼¦é€€ç«)

**é‹ä½œåŸç†ï¼š**
```python
lr = eta_min + (initial_lr - eta_min) Ã— (1 + cos(Ï€ Ã— iteration / T_max)) / 2
```

**æ›²ç·šï¼ˆT_max=500ï¼‰ï¼š**
```
0.001 â”¤â—
      â”‚ â—â—
0.0008â”¤   â—â—
      â”‚     â—â—
0.0006â”¤       â—â—
      â”‚         â—â—
0.0004â”¤           â—â—
      â”‚             â—â—
0.0002â”¤               â—â—
      â”‚                 â—â—
0.0000â”¤                   â—
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       0   100  200  300  400  500
```

**é©ç”¨å ´æ™¯ï¼š**
- âœ… è¨“ç·´è¿­ä»£æ•¸å›ºå®š
- âœ… å¸Œæœ›å¹³æ»‘é™åˆ°æœ€å°å€¼
- âœ… æƒ³è¦åœ¨æœ«æœŸåšç²¾ç´°èª¿æ•´

**é…ç½®ç¯„ä¾‹ï¼š**
```json
{
    "type": "cosine",
    "T_max": 500,       // 500 æ¬¡è¿­ä»£é™åˆ°æœ€å°
    "eta_min": 1e-6     // æœ€å°å­¸ç¿’ç‡
}
```

---

#### 5ï¸âƒ£ Reduce on Plateau (æ€§èƒ½åœæ»¯é™ä½)

**é‹ä½œåŸç†ï¼š**
```python
if no_improvement_for(patience) iterations:
    lr = lr Ã— factor
```

**é©ç”¨å ´æ™¯ï¼š**
- âœ… çå‹µæ›²ç·šæ³¢å‹•å¤§
- âœ… éœ€è¦ç­‰å¾…æ€§èƒ½çœŸæ­£åœæ»¯æ‰é™ä½ LR
- âŒ å¯èƒ½åœ¨éæ“¬åˆå¾Œæ‰åæ‡‰

**é…ç½®ç¯„ä¾‹ï¼š**
```json
{
    "type": "reduce_on_plateau",
    "patience": 20,     // 20 æ¬¡ç„¡æå‡
    "factor": 0.5       // æ¸›åŠ
}
```

---

#### 6ï¸âƒ£ None (ä¸ä½¿ç”¨èª¿åº¦å™¨)

å›ºå®šå­¸ç¿’ç‡ï¼Œé©åˆï¼š
- çŸ­æœŸå¯¦é©—
- èª¿è©¦éšæ®µ
- æ‰‹å‹•æ§åˆ¶å­¸ç¿’ç‡

```json
{
    "type": "none"
}
```

---

### ğŸ” å¦‚ä½•é¸æ“‡èª¿åº¦å™¨ï¼Ÿ

| æƒ…æ³ | æ¨è–¦èª¿åº¦å™¨ | åŸå›  |
|------|-----------|------|
| **ä¸ç¢ºå®šè¨“ç·´å¤šä¹…** | `adaptive` | è‡ªå‹•é©æ‡‰ä»»æ„é•·åº¦ |
| **çå‹µæ³¢å‹•å¤§** | `adaptive` / `reduce_on_plateau` | å®¹å¿çŸ­æœŸæ³¢å‹• |
| **è¨“ç·´æ™‚é•·å›ºå®š** | `cosine` / `step` | æŒ‰è¨ˆåŠƒè¡°æ¸› |
| **é•·æœŸè¨“ç·´ (>5000)** | `exponential` / `cosine` | å¹³æ»‘é€£çºŒè¡°æ¸› |
| **é¦–æ¬¡å˜—è©¦** | `adaptive` (patience=30) | æœ€ç©©å¦¥ |
| **èª¿è©¦éšæ®µ** | `none` | ä¿æŒç°¡å–® |

---

### ğŸ“ˆ å¯¦éš›è¨“ç·´ç¯„ä¾‹

**åˆå§‹é…ç½®ï¼ˆadaptiveï¼‰ï¼š**
```json
{
    "type": "adaptive",
    "patience": 30,
    "factor": 0.5,
    "min_lr": 1e-6,
    "improvement_threshold": 0.01
}
```

**è¨“ç·´éç¨‹ï¼š**
```
Iteration    Avg Reward    Learning Rate    äº‹ä»¶
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0           -5.0         0.00025       åˆå§‹
  100            2.0         0.00025       ç·©æ…¢æ”¹å–„
  200            4.5         0.00025       
  250            4.2         0.00025       é–‹å§‹åœæ»¯
  280            4.3         0.00025       patience=30
  310            4.1         0.000125      âš ï¸ é™ä½LR (Ã—0.5)
  400            8.5         0.000125      ğŸ“ˆ æ–°æœ€ä½³ï¼
  500           12.0         0.000125      æŒçºŒé€²æ­¥
  600           11.8         0.000125      
  630           12.1         0.0000625     âš ï¸ å†æ¬¡é™ä½
  800           15.0         0.0000625     ğŸ“ˆ æ–°æœ€ä½³ï¼
 1000           14.9         0.0000625     æ¥è¿‘æ”¶æ–‚
```

**å­¸ç¿’ç‡æ›²ç·šï¼š**
```
LR
0.00025 â”¤â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
        â”‚                                 â†“
0.000125â”¤                             â—â—â—â—â—â—â—â—â—â—â—â—â—
        â”‚                                       â†“
0.000063â”¤                                   â—â—â—â—â—â—â—â—â—
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         0   200   400   600   800  1000  iterations
```

---

### âš™ï¸ å‹•æ…‹é…ç½®ç†±é‡è¼‰

**é‡è¦ç‰¹æ€§ï¼š** ç„¡éœ€é‡å•Ÿè¨“ç·´ï¼Œé…ç½®è‡ªå‹•ç”Ÿæ•ˆï¼

```python
# pytorch_trainer.py ä¸­çš„è‡ªå‹•é‡è¼‰é‚è¼¯
def _load_dynamic_config(self, iteration):
    if iteration % 10 != 0:  # æ¯ 10 æ¬¡è¿­ä»£æª¢æŸ¥ä¸€æ¬¡
        return
    
    # è®€å– training_config.json
    with open("training_config.json") as f:
        config = json.load(f)
    
    # æ›´æ–°è¶…åƒæ•¸
    new_lr = config["gpu_training"]["learning_rate"]
    if new_lr != self.lr:
        for param_group in self.opt.param_groups:
            param_group['lr'] = new_lr
        print(f"âš™ï¸ å­¸ç¿’ç‡å·²æ›´æ–°: {self.lr} â†’ {new_lr}")
```

**ä½¿ç”¨æµç¨‹ï¼š**
1. è¨“ç·´é‹è¡Œä¸­
2. ç·¨è¼¯ `training_config.json`
3. å„²å­˜æ–‡ä»¶
4. ç­‰å¾…ä¸‹ä¸€å€‹ 10 çš„å€æ•¸è¿­ä»£ï¼ˆå¦‚ç¬¬ 1230 â†’ 1240ï¼‰
5. è‡ªå‹•ç”Ÿæ•ˆï¼âœ¨

**å¯å‹•æ…‹èª¿æ•´çš„åƒæ•¸ï¼š**
- âœ… `learning_rate` - å­¸ç¿’ç‡
- âœ… `batch_size` - æ‰¹é‡å¤§å°
- âœ… `ppo_epochs` - PPO è¨“ç·´è¼ªæ•¸
- âœ… `ent_coef` - ç†µä¿‚æ•¸
- âœ… `vf_coef` - åƒ¹å€¼å‡½æ•¸ä¿‚æ•¸
- âœ… `clip_range` - PPO è£å‰ªç¯„åœ
- âœ… `gamma` - æŠ˜æ‰£å› å­
- âœ… `gae_lambda` - GAE Î» åƒæ•¸

---

## ğŸ¨ Actor-Critic æ¶æ§‹åœ–(AOV System)

### ğŸ§  å®Œæ•´ç³»çµ±æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¨“ç·´ç’°å¢ƒ (Training Environment)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  ğŸ® Game Environment (Ã—16 ä¸¦è¡Œ)                                     â”‚
â”‚  â”œâ”€ ç‹€æ…‹: [y, vy, x_obs, gap_top, gap_bottom]                      â”‚
â”‚  â”œâ”€ å‹•ä½œ: jump (0/1)                                               â”‚
â”‚  â””â”€ çå‹µ: +0.1 (å­˜æ´») / +5.0 (é€šé) / -5.0 (ç¢°æ’)                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ state (5D vector)
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Actor-Critic ç¥ç¶“ç¶²è·¯                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  ğŸ“¥ è¼¸å…¥å±¤ (5 neurons)                                              â”‚
â”‚     [y_norm, vy_norm, x_norm, gap_top_norm, gap_bottom_norm]        â”‚
â”‚                          â”‚                                          â”‚
â”‚                          â”‚ fc1.weight (5Ã—64)                        â”‚
â”‚                          â†“                                          â”‚
â”‚  ğŸ§  éš±è—å±¤ 1 (64 neurons) + ReLU                                    â”‚
â”‚     [h1â‚, h1â‚‚, h1â‚ƒ, ..., h1â‚†â‚„]                                     â”‚
â”‚                          â”‚                                          â”‚
â”‚                          â”‚ fc2.weight (64Ã—64)                       â”‚
â”‚                          â†“                                          â”‚
â”‚  ğŸ§  éš±è—å±¤ 2 (64 neurons) + ReLU                                    â”‚
â”‚     [h2â‚, h2â‚‚, h2â‚ƒ, ..., h2â‚†â‚„]                                     â”‚
â”‚                          â”‚                                          â”‚
â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚                 â”‚                 â”‚                                 â”‚
â”‚         actor.weight (64Ã—1)   critic.weight (64Ã—1)                 â”‚
â”‚                 â†“                 â†“                                 â”‚
â”‚  ğŸ­ Actor è¼¸å‡º      ğŸ¯ Critic è¼¸å‡º                                   â”‚
â”‚     logit â†’ prob      value                                        â”‚
â”‚     (å‹•ä½œæ¦‚ç‡)        (ç‹€æ…‹åƒ¹å€¼)                                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ action               â”‚ value

```powershell
python -c "from agents.pytorch_trainer import PPOTrainer; t=PPOTrainer(); t.train(total_timesteps=2000)"
```

5. å•Ÿå‹• pre-commit hooksï¼ˆç¢ºä¿æ ¼å¼ä¸€è‡´ï¼‰

```powershell
pip install pre-commit
pre-commit install
pre-commit run --all-files
```

## è¨“ç·´è®Šæ•¸èˆ‡çå‹µ

- **ç‹€æ…‹** $s_t = [y, v_y, x_{obs}, y_{gap\_top}, y_{gap\_bottom}]$ï¼ˆå‡å·²æ­£è¦åŒ–ï¼‰ã€‚
- **å‹•ä½œ** $a_t \in \{0,1\}$ï¼š0 = ä¸è·³ã€1 = è·³ã€‚
- **çå‹µ**ï¼š
   - é€šééšœç¤™ï¼š$r_{pass} = +5$
   - ç¢°æ’æˆ–é£›å‡ºä¸Šä¸‹ç•Œï¼š$r_{collision} = -5$
   - å…¶ä»–æ™‚é–“æ­¥ï¼š0ï¼ˆå·²ç§»é™¤æ™‚é–“æ‡²ç½°ï¼‰ã€‚

## ğŸ§¾ è¨“ç·´å…¬å¼ (Training formulas)

ä¸‹é¢ä»¥æ¨™æº– LaTeX å½¢å¼åˆ—å‡ºå¸¸ç”¨çš„è¨“ç·´å…¬å¼ï¼ŒåŒ…å« PPOï¼ˆå« GAEï¼‰ã€DQN / Double DQNã€SACï¼ˆé›¢æ•£ç‰ˆï¼‰èˆ‡ TD3ï¼ˆé€£çºŒç‰ˆï¼Œä¾›æ¯”è¼ƒï¼‰ã€‚è«‹å°‡æ­¤å°ç¯€æ”¾åœ¨ã€Œæ·±åº¦å­¸ç¿’åŸç†ã€èˆ‡ã€Œæå¤±å‡½æ•¸è©³è§£ã€é™„è¿‘ä»¥ä¾¿å¿«é€Ÿåƒè€ƒã€‚

### PPOï¼ˆå« GAEï¼‰

æŠ˜æ‰£å›å ±ï¼ˆDiscounted returnï¼‰ï¼š

$$
G_t = \sum_{k=0}^{\infty} \gamma^{k} r_{t+k}
$$

å„ªå‹¢ä¼°è¨ˆï¼ˆGAEï¼‰ï¼š

$$
\begin{aligned}
\delta_t &= r_t + \gamma V(s_{t+1}) - V(s_t),\\
\hat{A}_t &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \; \delta_{t+l}.
\end{aligned}
$$

è£å‰ªå¾Œçš„ PPO ç›®æ¨™ï¼ˆClipped objectiveï¼‰ï¼š

$$
L^{\mathrm{CLIP}}(\theta) = -\mathbb{E}_t\left[ \min\left( r_t(\theta) \hat{A}_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\, \hat{A}_t \right) \right]
$$

å…¶ä¸­

$$
r_t(\theta) = \frac{\pi_{\theta}(a_t\mid s_t)}{\pi_{\theta_{\mathrm{old}}}(a_t\mid s_t)}.
$$

å€¼å‡½æ•¸èˆ‡ç†µé …ï¼š

$$
L^{\mathrm{VF}} = \mathbb{E}_t\big[ (V_{\theta}(s_t) - G_t)^2 \big],\qquad
S[\pi_{\theta}](s_t) = -\sum_a \pi_{\theta}(a\mid s_t) \log \pi_{\theta}(a\mid s_t).
$$

ç¸½æå¤±ï¼ˆpolicy + value + entropyï¼‰ï¼š

$$
L = L^{\mathrm{CLIP}} + c_{vf} L^{\mathrm{VF}} - c_{ent} \; S[\pi_{\theta}]
$$

---

### DQN / Double DQNï¼ˆQ-Learning Trainerï¼‰

ç¶“é©—å›æ”¾æ¨£æœ¬çš„ç›®æ¨™å€¼ï¼š

$$
\begin{aligned}
y_t^{\mathrm{DQN}} &= r_t + \gamma \max_{a'} Q_{\theta^-}(s_{t+1}, a'),\\
y_t^{\mathrm{DDQN}} &= r_t + \gamma Q_{\theta^-}\bigl(s_{t+1}, \arg\max_{a'} Q_{\theta}(s_{t+1}, a')\bigr).
\end{aligned}
$$

å¹³æ–¹æå¤±ï¼ˆMSEï¼‰ï¼š

$$
L(\theta) = \mathbb{E}_t\big[ (y_t - Q_{\theta}(s_t, a_t))^2 \big].
$$

---

### SACï¼ˆé›¢æ•£ç‰ˆï¼‰

Critic ç›®æ¨™ï¼ˆQ-net lossï¼‰ï¼š

$$
J_Q = \mathbb{E}\big[ (Q_{\phi}(s_t,a_t) - y_t)^2 \big],
$$

å…¶ä¸­ç›®æ¨™å€¼ç‚ºï¼š

$$
y_t = r_t + \gamma\; \mathbb{E}_{a_{t+1}\sim\pi}\Big[ \min\big( Q_{\bar{\phi}_1}(s_{t+1},a_{t+1}),\; Q_{\bar{\phi}_2}(s_{t+1},a_{t+1}) \big) - \alpha\,\log\pi(a_{t+1}\mid s_{t+1}) \Big].
$$

Actor ç›®æ¨™ï¼ˆpolicy lossï¼‰ï¼š

$$
J_{\pi} = \mathbb{E}_{s_t\sim D}\Big[ \mathbb{E}_{a_t\sim\pi}\big[ \alpha \log \pi(a_t\mid s_t) - Q_{\phi}(s_t,a_t) \big] \Big].
$$

é›™ç¶²è·¯è»Ÿæ›´æ–°ï¼ˆtarget networks soft updateï¼‰ï¼š

$$
\bar{\phi} \leftarrow \tau \phi + (1-\tau) \bar{\phi}.
$$

---

### TD3ï¼ˆé€£çºŒç‰ˆï¼Œä¾›æ¯”è¼ƒï¼‰

TD3 çš„é‡é»ï¼šä½¿ç”¨å…©å€‹ critic å–æœ€å°å€¼ä»¥é˜²æ­¢ Q-value é«˜ä¼°ï¼›å»¶é²æ›´æ–° actor èˆ‡ target policy smoothingã€‚

å¹³æ»‘ç›®æ¨™å‹•ä½œï¼ˆtarget policy smoothingï¼‰ï¼š

$$
\tilde{a} = \text{clip}\big(\pi_{\theta^-}(s_{t+1}) + \epsilon,\; a_{\mathrm{low}},\; a_{\mathrm{high}}\big)
$$

å°æ‡‰ç›®æ¨™å€¼ï¼š

$$
y_t = r_t + \gamma \min_{i=1,2} Q_{\phi_i^-}(s_{t+1}, \tilde{a}).
$$

---

Notes / implementation hints:
- ä½¿ç”¨ display math ($$ ... $$) å¯åœ¨æ”¯æ´ MathJax çš„å¹³å°ä¸Šæ­£ç¢ºæ¸²æŸ“ï¼Œä¸”åœ¨ç´” Markdown ä¸­ä»å¯è®€ã€‚
- å»ºè­°ä½¿ç”¨ \text{clip} æˆ– \mathrm{clip}ï¼ˆä¸è¦ç”¨ \operatorname{clip}ï¼Œä»¥é¿å… KaTeX/GitHub æ‹’çµ• macroï¼‰ã€‚
- ä¿ç•™å¸¸ç”¨è®Šæ•¸åç¨±ï¼ˆG_t, A_t, \delta_t, \gamma, \lambda, \epsilon, \alpha, \tauï¼‰ä»¥ä¾¿èˆ‡å¯¦ä½œç¨‹å¼ç¢¼å°æ‡‰ã€‚

## ğŸ“Š åƒæ•¸èª¿æ•´æŒ‡å—

è©³ç´°çš„åƒæ•¸èª¿æ•´å»ºè­°è«‹åƒé–± `PARAMETER_TUNING_GUIDE.md`ã€‚

**å¿«é€Ÿåƒè€ƒï¼š**
- ğŸš€ **è¨“ç·´å¤ªæ…¢**ï¼šå¢åŠ  `learning_rate`, `n_envs`
- ğŸ”¥ **è¨“ç·´ä¸ç©©å®š**ï¼šé™ä½ `learning_rate`, å¢åŠ  `batch_size`
- ğŸ¯ **å¡åœ¨å±€éƒ¨æœ€å„ª**ï¼šå¢åŠ  `ent_coef`, é™ä½ `learning_rate`
- ğŸ’¾ **GPU è¨˜æ†¶é«”ä¸è¶³**ï¼šé™ä½ `batch_size`, `n_envs`, `horizon`

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. **CUDA out of memory**
**è§£æ±ºï¼š** é™ä½ `batch_size` æˆ– `n_envs`

#### 2. **è¨“ç·´æ²’æœ‰é€²æ­¥**
**è§£æ±ºï¼š** æª¢æŸ¥ç’°å¢ƒé›£åº¦ï¼Œå¢åŠ å­˜æ´»çå‹µï¼Œé™ä½å­¸ç¿’ç‡

#### 3. **è¨“ç·´è¿­ä»£ä¸å¢åŠ **
**èªªæ˜ï¼š** æ­£å¸¸ï¼1 æ¬¡è¿­ä»£ = 32768 æ­¥ï¼Œå¯èƒ½éœ€è¦æ•¸ç™¾å±€éŠæˆ²

è©³ç´°æ•…éšœæ’é™¤è«‹åƒé–± `TRAINING_GUIDE.md`ã€‚

---

## ğŸš€ é€²éšä½¿ç”¨

### è·¨é›»è…¦è¨“ç·´é·ç§»

```powershell
# é›»è…¦ A: æ¨é€æª¢æŸ¥é»
git add checkpoints/checkpoint_5000.pt
git commit -m "è¨“ç·´åˆ° 5000 æ¬¡"
git push

# é›»è…¦ B: ä¸‹è¼‰ä¸¦ç¹¼çºŒ
git pull
python run_game.py  # è‡ªå‹•å¾æœ€æ–°æª¢æŸ¥é»æ¢å¾©
```

### TensorBoard ç›£æ§

```powershell
tensorboard --logdir=checkpoints/tb --port=6006
# è¨ªå• http://localhost:6006
```

### ç´”è¨“ç·´ï¼ˆç„¡ UIï¼‰

```python
from agents.pytorch_trainer import PPOTrainer

trainer = PPOTrainer()
trainer.train(total_timesteps=100000)
```

---

## ğŸ“š ç›¸é—œæ–‡æª”

- ğŸ“– [å­¸ç¿’ç‡èª¿åº¦å™¨å®Œæ•´æŒ‡å—](LR_SCHEDULER_GUIDE.md) - 6ç¨®èª¿åº¦å™¨è©³è§£èˆ‡é¸æ“‡å»ºè­°
- âš™ï¸ [åƒæ•¸èª¿æ•´æŒ‡å—](PARAMETER_TUNING_GUIDE.md) - è¶…åƒæ•¸èª¿æ•´æœ€ä½³å¯¦è¸
- ğŸ”§ [è¨“ç·´æ•…éšœæ’é™¤](TRAINING_GUIDE.md) - å¸¸è¦‹å•é¡Œè¨ºæ–·èˆ‡è§£æ±º
- ğŸ–¥ï¸ [GPU è¨­ç½®æŒ‡å—](GPU_SETUP.md) - CUDA å®‰è£èˆ‡é…ç½®
- ğŸ“Š [å°ˆæ¡ˆç‹€æ…‹å ±å‘Š](PROJECT_STATUS_REVIEW.md) - é–‹ç™¼é€²åº¦è¿½è¹¤

---

## ğŸ¤ è²¢ç»

æ­¡è¿è²¢ç»ï¼è«‹éµå¾ªä»¥ä¸‹æµç¨‹ï¼š

1. Fork æœ¬å°ˆæ¡ˆ
2. å‰µå»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. é–‹å•Ÿ Pull Request

### é–‹ç™¼ç’°å¢ƒè¨­ç½®

```powershell
# å®‰è£é–‹ç™¼ä¾è³´
pip install -r requirements.txt
pip install pre-commit

# è¨­ç½® pre-commit hooks
pre-commit install

# é‹è¡Œæ‰€æœ‰æ¸¬è©¦
python -m pytest -v

# é‹è¡Œä»£ç¢¼æª¢æŸ¥
pre-commit run --all-files
```

### ä»£ç¢¼è¦ç¯„

- ä½¿ç”¨ **Black** æ ¼å¼åŒ– Python ä»£ç¢¼
- ä½¿ç”¨ **Ruff** é€²è¡Œ Linting
- ä½¿ç”¨ **isort** æ’åº imports
- æ‰€æœ‰æ–°åŠŸèƒ½å¿…é ˆæœ‰å–®å…ƒæ¸¬è©¦
- æäº¤å‰é‹è¡Œ `pytest` ç¢ºä¿æ¸¬è©¦é€šé

---

## ğŸ“„ æˆæ¬Š

æœ¬å°ˆæ¡ˆæ¡ç”¨ **MIT License** æˆæ¬Š - è©³è¦‹ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è¬

### æŠ€è¡“æ¡†æ¶
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¸ç¿’æ¡†æ¶
- [Pygame](https://www.pygame.org/) - éŠæˆ²å¼•æ“
- [TensorBoard](https://www.tensorflow.org/tensorboard) - è¨“ç·´è¦–è¦ºåŒ–

### ç†è«–åŸºç¤
- **PPO è«–æ–‡**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) by Schulman et al. (2017)
- **GAE è«–æ–‡**: [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438) by Schulman et al. (2016)
- **Actor-Critic**: [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html) by Sutton et al. (2000)

### éˆæ„Ÿä¾†æº
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) - é«˜è³ªé‡ RL åº«
- [CleanRL](https://github.com/vwxyzjn/cleanrl) - ç°¡æ½”çš„ RL å¯¦ç¾
- [OpenAI Spinning Up](https://spinningup.openai.com/) - RL æ•™è‚²è³‡æº

---

## ğŸ“ è¯ç¹«æ–¹å¼

- **ä½œè€…**: SmailDot
- **GitHub**: [@SmailDot](https://github.com/SmailDot)
- **å°ˆæ¡ˆé€£çµ**: [https://github.com/SmailDot/train_game](https://github.com/SmailDot/train_game)

---

## ğŸ“ˆ å°ˆæ¡ˆçµ±è¨ˆ

![GitHub stars](https://img.shields.io/github/stars/SmailDot/train_game?style=social)
![GitHub forks](https://img.shields.io/github/forks/SmailDot/train_game?style=social)
![GitHub issues](https://img.shields.io/github/issues/SmailDot/train_game)
![GitHub pull requests](https://img.shields.io/github/issues-pr/SmailDot/train_game)

---

## ğŸ—ºï¸ è·¯ç·šåœ–

### âœ… å·²å®Œæˆ
- [x] PPO æ¼”ç®—æ³•å¯¦ç¾
- [x] å¤šç’°å¢ƒä¸¦è¡Œè¨“ç·´
- [x] 6 ç¨®å­¸ç¿’ç‡èª¿åº¦å™¨
- [x] å‹•æ…‹é…ç½®ç†±é‡è¼‰
- [x] TensorBoard æ•´åˆ
- [x] å®Œæ•´æ¸¬è©¦å¥—ä»¶
- [x] GPU/CPU è‡ªå‹•é©é…

### ğŸš§ é€²è¡Œä¸­
- [ ] æ·»åŠ æ›´å¤š RL æ¼”ç®—æ³• (SAC, TD3, DDPG)
- [ ] æ”¹é€²ç¥ç¶“ç¶²è·¯å¯è¦–åŒ–
- [ ] å¢å¼·è¨“ç·´åˆ†æå·¥å…·

### ğŸ“… è¨ˆåŠƒä¸­
- [ ] æ”¯æŒæ›´å¤šéŠæˆ²ç’°å¢ƒ
- [ ] åˆ†ä½ˆå¼è¨“ç·´æ”¯æŒ
- [ ] Web ä»‹é¢å„€è¡¨æ¿
- [ ] æ¨¡å‹å£“ç¸®èˆ‡éƒ¨ç½²
- [ ] å¤šèªè¨€æ–‡æª” (English, æ—¥æœ¬èª)

---

<div align="center">

**â­ å¦‚æœé€™å€‹å°ˆæ¡ˆå°ä½ æœ‰å¹«åŠ©ï¼Œè«‹çµ¦ä¸€å€‹ Starï¼â­**

Made with â¤ï¸ by SmailDot

</div>

````

