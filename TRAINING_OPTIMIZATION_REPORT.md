# 訓練優化報告 - 2025/11/15

## 🎯 問題診斷

### 觀察到的現象
- **1000+ 分數變少**，從 31.6% 下降
- **500-999 分數增加**，佔比 68.4%
- **頻繁觸發性能崩潰回檔**（5480, 5500, 5520, 5530...連續多次）
- **權重幾乎不變**：`權重平均值: 0.444940`（多次迭代保持不變）

### 根本原因分析

#### ❌ 不是網路容量問題
- 網路架構 `5 → 64 → 64 → 1` 已經證明足夠
- 歷史最高分 **1192 分**證明模型有足夠表達能力

#### ✅ 真正的問題

1. **學習率過低導致的惡性循環**
   ```
   學習率 0.000125 (已減半)
        ↓
   權重更新緩慢 (幾乎不變)
        ↓
   性能無法改善
        ↓
   觸發崩潰檢測 (40%閾值)
        ↓
   回檔 + 學習率再減半 (0.0000625)
        ↓
   完全無法學習 → 再次崩潰
   ```

2. **回檔閾值過於敏感**
   - 40% 下降就觸發回檔
   - PPO 本身有探索性波動
   - 正常探索被誤判為崩潰

3. **探索不足**
   - 熵係數 `ent_coef=0.05` 太低
   - 策略過於確定，缺乏多樣性
   - 陷入局部最優無法跳出

---

## 🔧 實施的解決方案

### 方案 1: 提高熵係數
```python
# 修改前
ent_coef=0.05

# 修改後
ent_coef=0.15  # 提高 3 倍
```

**預期效果**：
- ✅ 增加策略隨機性和探索
- ✅ 鼓勵嘗試新策略
- ✅ 避免過早收斂到局部最優
- ✅ 提高策略多樣性

**理論基礎**：
- 熵值 = 策略的不確定性
- 高熵 = 更多探索 = 發現更好策略的機會
- PPO 常用範圍：0.01~0.2

---

### 方案 2: 放寬回檔閾值
```python
# 修改前
degradation_threshold = 0.40  # 下降 40% 就回檔

# 修改後
degradation_threshold = 0.70  # 下降 70% 才回檔
```

**預期效果**：
- ✅ 給予更多探索空間
- ✅ 減少頻繁回檔（避免打斷學習）
- ✅ 允許正常的性能波動
- ✅ 只在真正崩潰時才回檔

**對比**：
| 指標 | 修改前 (40%) | 修改後 (70%) |
|------|-------------|-------------|
| 平均分 500 vs 300 | 回檔 (40%↓) | 繼續 (40%↓ < 70%) |
| 平均分 500 vs 100 | 回檔 (80%↓) | 回檔 (80%↓ > 70%) |

---

### 方案 3: 回檔時保持原始學習率
```python
# 修改前
rollback_lr = self.initial_lr * 0.5  # 減半 (3e-4 → 1.5e-4)

# 修改後
rollback_lr = self.initial_lr * 1.0  # 保持原值 (3e-4)
```

**修改了 3 個位置**：
1. checkpoint_best.pt 回檔時
2. scores.json 歷史最佳回檔時
3. 最近 5 個檢查點回檔時

**預期效果**：
- ✅ 避免學習率過度降低
- ✅ 保持模型的學習能力
- ✅ 打破「回檔→降LR→更慢→再回檔」惡性循環
- ✅ 權重能夠有效更新

**學習率演變對比**：

修改前：
```
初始: 3e-4
  ↓ 第一次回檔
1.5e-4 (減半)
  ↓ 第二次回檔
0.75e-4 (再減半)
  ↓ 第三次回檔
0.375e-4 (幾乎無法學習)
```

修改後：
```
初始: 3e-4
  ↓ 第一次回檔
3e-4 (保持)
  ↓ 第二次回檔
3e-4 (保持)
  ↓ 持續保持學習能力
```

---

## 📊 預期成果

### 短期效果（10-50 次迭代）
- ✅ 減少頻繁回檔次數
- ✅ 權重開始有效更新（不再停滯）
- ✅ 熵值提高（策略多樣性增加）

### 中期效果（50-200 次迭代）
- ✅ 1000+ 分數出現頻率回升
- ✅ 平均分穩定提升
- ✅ 訓練曲線更平滑

### 長期效果（200+ 次迭代）
- ✅ 突破 1192 分記錄
- ✅ 穩定達到 1000+ 分
- ✅ 探索到更優策略

---

## 🔍 監控指標

### 需要觀察的關鍵指標

1. **熵值（Entropy）**
   - 之前：0.09~0.15（太低）
   - 目標：0.15~0.25（健康範圍）
   - 含義：策略的不確定性/探索性

2. **權重變化**
   - 之前：`權重平均值: 0.444940`（多次不變）
   - 目標：每次迭代都有變化
   - 含義：模型是否在學習

3. **回檔頻率**
   - 之前：每 10-20 次迭代就回檔一次
   - 目標：100+ 次迭代才回檔一次
   - 含義：訓練穩定性

4. **學習率**
   - 之前：0.000125（已減半）
   - 目標：保持在 0.0003
   - 含義：學習能力

5. **分數分布**
   ```
   之前（5400+）:
     ≥1000分: 31.6%
     500-999分: 68.4%
     <500分: 0%
   
   目標:
     ≥1000分: 50%+
     500-999分: 40%
     <500分: <10%
   ```

---

## 📝 使用說明

### 如何驗證優化效果

1. **重新開始訓練**
   ```bash
   python run_game.py
   ```

2. **觀察訓練日誌**
   - 查看熵值是否提高到 0.15+
   - 確認權重平均值每次迭代都在變化
   - 回檔訊息出現頻率是否降低

3. **檢查分數趨勢**（訓練 50-100 次後）
   ```bash
   python -c "import json; data=json.load(open('checkpoints/scores.json', encoding='utf-8')); recent = [d for d in data if d['iteration'] >= 最新迭代-100]; scores = [d['score'] for d in recent]; print(f'≥1000分: {len([s for s in scores if s>=1000])/len(scores)*100:.1f}%')"
   ```

### 如果效果不佳

如果訓練 100 次後仍然：
- ❌ 1000+ 分數 < 30%
- ❌ 頻繁回檔（每 20 次迭代）
- ❌ 權重仍然不變

則考慮：
1. **進一步提高熵係數** → 0.2 或 0.25
2. **完全禁用回檔機制**（測試用）
3. **增加網路容量**（hidden: 64 → 128）
4. **增加網路深度**（2 層 → 3 層）

---

## 🔬 技術細節

### 熵係數的作用機制

```python
# PPO 損失函數
total_loss = policy_loss + vf_coef * value_loss - ent_coef * entropy

# 當 ent_coef 提高時：
# - entropy 項權重增加
# - 模型被鼓勵保持高熵（不確定性）
# - 避免策略過於確定（貪婪）
```

### 回檔閾值的數學含義

```python
# 平均分下降百分比
mean_drop = (best_mean - current_mean) / best_mean

# 例子：
# best_mean = 5.0, current_mean = 3.0
# mean_drop = (5.0 - 3.0) / 5.0 = 0.4 (40%)

# 40% 閾值：下降 40% 就回檔（敏感）
# 70% 閾值：下降 70% 才回檔（寬鬆）
```

---

## 📚 參考資料

### PPO 超參數建議範圍

| 參數 | 典型範圍 | 本專案 | 說明 |
|------|---------|--------|------|
| `lr` | 1e-4 ~ 5e-3 | **3e-4** | 學習率 |
| `gamma` | 0.95 ~ 0.999 | **0.99** | 折扣因子 |
| `clip_eps` | 0.1 ~ 0.3 | **0.2** | PPO裁剪 |
| `ent_coef` | 0.01 ~ 0.2 | **0.15** ⬆️ | 熵係數（已提高） |
| `vf_coef` | 0.5 ~ 1.0 | **0.5** | 價值損失 |

### 相關論文

1. **Proximal Policy Optimization** (Schulman et al., 2017)
   - 熵正則化的重要性
   - 探索-利用平衡

2. **OpenAI Five** (2018)
   - 大規模 PPO 訓練經驗
   - 學習率調度策略

---

## 📅 更新記錄

- **2025-11-15**: 初始優化實施
  - 提高熵係數 (0.05 → 0.15)
  - 放寬回檔閾值 (40% → 70%)
  - 保持原始學習率 (0.5x → 1.0x)

---

## 💬 預期問答

**Q: 為什麼不增加網路容量？**
A: 因為模型已經學會過 1192 分，證明容量足夠。問題在於訓練策略，不是模型容量。

**Q: 熵係數會不會太高？**
A: 0.15 在 PPO 的正常範圍內（0.01~0.2）。如果效果不佳可以調整到 0.1 或 0.2。

**Q: 70% 閾值會不會太寬鬆？**
A: 70% 意味著平均分從 500 降到 150 才回檔，這是真正的崩潰。正常波動不會這麼大。

**Q: 回檔時為何不降低學習率？**
A: 因為回檔本身就是在嘗試恢復，如果再降低學習率，模型會學不動，陷入惡性循環。

---

## ✅ 檢查清單

訓練前確認：
- [x] 熵係數已提高到 0.15
- [x] 回檔閾值已放寬到 70%
- [x] 學習率重置為原始值（3處都已修改）
- [x] 程式碼語法檢查通過
- [x] 提交到 Git 並推送

訓練後驗證（100 次迭代後）：
- [ ] 熵值保持在 0.15~0.25
- [ ] 權重每次迭代都有變化
- [ ] 回檔次數 < 5 次
- [ ] 1000+ 分數佔比 > 40%
- [ ] 最高分超過 1192

---

**祝訓練順利！期待看到新紀錄！** 🚀
