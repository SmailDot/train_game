"""
åˆ†æç¡è¦ºæœŸé–“ï¼ˆ#5940 åˆ° #14460ï¼‰çš„æª¢æŸ¥é»åƒæ•¸è®ŠåŒ–
æ‰¾å‡ºå°è‡´å´©æ½°çš„åƒæ•¸èª¿æ•´æ¨¡å¼
"""

import json
import os
from collections import defaultdict
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import torch

print("=" * 80)
print("ğŸ”¬ æ·±åº¦åˆ†ææª¢æŸ¥é»åƒæ•¸è®ŠåŒ–ï¼ˆ#5940 â†’ #14460ï¼‰")
print("=" * 80)

# === é…ç½® ===
START_ITER = 5940  # ç¡è¦ºå¾Œé–‹å§‹
END_ITER = 14460  # é†’ä¾†æ™‚
SAMPLE_INTERVAL = 100  # æ¯ 100 æ¬¡è¿­ä»£æ¡æ¨£ä¸€æ¬¡
CHECKPOINT_DIR = "checkpoints"

# === æ”¶é›†è¦åˆ†æçš„æª¢æŸ¥é» ===
print("\nç¬¬ä¸€æ­¥ï¼šæ”¶é›†æª¢æŸ¥é»æª”æ¡ˆ")
print("-" * 80)

checkpoints_to_analyze = []
for iter_num in range(START_ITER, END_ITER + 1, SAMPLE_INTERVAL):
    # æ‰¾æœ€æ¥è¿‘çš„æª¢æŸ¥é»ï¼ˆæ¯ 10 æ¬¡ä¿å­˜ä¸€æ¬¡ï¼‰
    checkpoint_iter = (iter_num // 10) * 10
    checkpoint_file = os.path.join(CHECKPOINT_DIR, f"checkpoint_{checkpoint_iter}.pt")
    if os.path.exists(checkpoint_file):
        checkpoints_to_analyze.append((checkpoint_iter, checkpoint_file))

print(f"âœ… æ‰¾åˆ° {len(checkpoints_to_analyze)} å€‹æª¢æŸ¥é»ç”¨æ–¼åˆ†æ")
print(f"   ç¯„åœ: #{checkpoints_to_analyze[0][0]} â†’ #{checkpoints_to_analyze[-1][0]}")

# === åˆ†æåƒæ•¸è®ŠåŒ– ===
print("\nç¬¬äºŒæ­¥ï¼šåˆ†ææ¨¡å‹åƒæ•¸çµ±è¨ˆ")
print("-" * 80)

param_stats = defaultdict(
    list
)  # {param_name: [(iter, mean, std, min, max, norm), ...]}
optimizer_stats = defaultdict(list)  # {key: [(iter, value), ...]}

for iter_num, checkpoint_file in checkpoints_to_analyze:
    try:
        # è¼‰å…¥æª¢æŸ¥é»ï¼ˆåƒ… CPUï¼Œä¸éœ€ GPUï¼‰
        checkpoint = torch.load(checkpoint_file, map_location="cpu")

        # åˆ†ææ¨¡å‹åƒæ•¸ (ä½¿ç”¨ model_state è€Œé model_state_dict)
        if "model_state" in checkpoint:
            model_state = checkpoint["model_state"]
        elif "model_state_dict" in checkpoint:
            model_state = checkpoint["model_state_dict"]
        else:
            model_state = None

        if model_state is not None:

            for param_name, param_tensor in model_state.items():
                if param_tensor.dtype in [torch.float32, torch.float16]:
                    param_np = param_tensor.cpu().numpy().flatten()

                    stats = {
                        "iter": iter_num,
                        "mean": float(np.mean(param_np)),
                        "std": float(np.std(param_np)),
                        "min": float(np.min(param_np)),
                        "max": float(np.max(param_np)),
                        "norm": float(np.linalg.norm(param_np)),
                        "abs_mean": float(np.mean(np.abs(param_np))),
                        "zeros_pct": float(np.sum(param_np == 0) / len(param_np) * 100),
                    }
                    param_stats[param_name].append(stats)

        # åˆ†æå„ªåŒ–å™¨ç‹€æ…‹ (ä½¿ç”¨ optimizer_state è€Œé optimizer_state_dict)
        if "optimizer_state" in checkpoint:
            opt_state = checkpoint["optimizer_state"]
        elif "optimizer_state_dict" in checkpoint:
            opt_state = checkpoint["optimizer_state_dict"]
        else:
            opt_state = None

        if opt_state is not None:
            if "param_groups" in opt_state and len(opt_state["param_groups"]) > 0:
                pg = opt_state["param_groups"][0]
                optimizer_stats["learning_rate"].append((iter_num, pg.get("lr", 0)))
                optimizer_stats["eps"].append((iter_num, pg.get("eps", 0)))
                optimizer_stats["weight_decay"].append(
                    (iter_num, pg.get("weight_decay", 0))
                )

        # åˆ†æå…¶ä»–å…ƒæ•¸æ“š
        if "iteration" in checkpoint:
            optimizer_stats["checkpoint_iteration"].append(
                (iter_num, checkpoint["iteration"])
            )

        print(f"   âœ“ åˆ†æ #{iter_num}", end="\r")

    except Exception as e:
        print(f"   âœ— ç„¡æ³•è¼‰å…¥ #{iter_num}: {e}")

print(f"\nâœ… å®Œæˆ {len(checkpoints_to_analyze)} å€‹æª¢æŸ¥é»åˆ†æ")

# === æª¢æ¸¬ç•°å¸¸è®ŠåŒ– ===
print("\nç¬¬ä¸‰æ­¥ï¼šæª¢æ¸¬ç•°å¸¸åƒæ•¸è®ŠåŒ–")
print("-" * 80)

anomalies = []

for param_name, stats_list in param_stats.items():
    if len(stats_list) < 5:
        continue

    # æå–æ™‚é–“åºåˆ—
    iters = [s["iter"] for s in stats_list]
    norms = [s["norm"] for s in stats_list]
    means = [s["mean"] for s in stats_list]
    stds = [s["std"] for s in stats_list]
    zeros_pcts = [s["zeros_pct"] for s in stats_list]

    # æª¢æ¸¬ 1: åƒæ•¸ç¯„æ•¸çˆ†ç‚¸ï¼ˆnorm çªç„¶å¢åŠ  > 50%ï¼‰
    for i in range(1, len(norms)):
        if norms[i - 1] > 0:
            change_pct = (norms[i] - norms[i - 1]) / norms[i - 1] * 100
            if abs(change_pct) > 50:
                anomalies.append(
                    {
                        "type": "norm_explosion" if change_pct > 0 else "norm_collapse",
                        "param": param_name,
                        "iter": iters[i],
                        "change_pct": change_pct,
                        "norm_before": norms[i - 1],
                        "norm_after": norms[i],
                    }
                )

    # æª¢æ¸¬ 2: åƒæ•¸è®Šæˆå…¨é›¶ï¼ˆdead neuronsï¼‰
    for i in range(len(zeros_pcts)):
        if zeros_pcts[i] > 90:  # è¶…é 90% æ˜¯é›¶
            anomalies.append(
                {
                    "type": "dead_parameters",
                    "param": param_name,
                    "iter": iters[i],
                    "zeros_pct": zeros_pcts[i],
                }
            )

    # æª¢æ¸¬ 3: æ¨™æº–å·®å´©æ½°ï¼ˆåƒæ•¸ä¸å†æ›´æ–°ï¼‰
    for i in range(1, len(stds)):
        if stds[i] < stds[0] * 0.1:  # æ¨™æº–å·®é™åˆ°åˆå§‹çš„ 10% ä»¥ä¸‹
            anomalies.append(
                {
                    "type": "std_collapse",
                    "param": param_name,
                    "iter": iters[i],
                    "std_before": stds[0],
                    "std_after": stds[i],
                }
            )

# æŒ‰è¿­ä»£æ¬¡æ•¸æ’åº
anomalies.sort(key=lambda x: x["iter"])

# è¼¸å‡ºç•°å¸¸
print(f"\nğŸš¨ æª¢æ¸¬åˆ° {len(anomalies)} å€‹ç•°å¸¸")

if anomalies:
    print("\nå‰ 20 å€‹ç•°å¸¸:")
    for i, anomaly in enumerate(anomalies[:20]):
        print(f"\n[{i+1}] é¡å‹: {anomaly['type']}")
        print(f"    åƒæ•¸: {anomaly['param']}")
        print(f"    è¿­ä»£: #{anomaly['iter']}")
        for key, value in anomaly.items():
            if key not in ["type", "param", "iter"]:
                if isinstance(value, float):
                    print(f"    {key}: {value:.6f}")
                else:
                    print(f"    {key}: {value}")

# === åˆ†æå­¸ç¿’ç‡è®ŠåŒ– ===
print("\nç¬¬å››æ­¥ï¼šåˆ†æå„ªåŒ–å™¨åƒæ•¸è®ŠåŒ–")
print("-" * 80)

if "learning_rate" in optimizer_stats:
    lr_data = optimizer_stats["learning_rate"]
    print(f"\nğŸ“Š å­¸ç¿’ç‡è®ŠåŒ–:")
    print(f"   åˆå§‹ LR (#{lr_data[0][0]}): {lr_data[0][1]:.8f}")
    print(f"   æœ€çµ‚ LR (#{lr_data[-1][0]}): {lr_data[-1][1]:.8f}")

    # æª¢æŸ¥å­¸ç¿’ç‡æ˜¯å¦æœ‰ç•°å¸¸è·³è®Š
    for i in range(1, len(lr_data)):
        if lr_data[i][1] != lr_data[i - 1][1]:
            change_pct = (lr_data[i][1] - lr_data[i - 1][1]) / lr_data[i - 1][1] * 100
            if abs(change_pct) > 10:
                print(
                    f"   âš ï¸ å­¸ç¿’ç‡è®ŠåŒ– #{lr_data[i][0]}: {lr_data[i-1][1]:.8f} â†’ {lr_data[i][1]:.8f} ({change_pct:+.1f}%)"
                )

# === æ‰¾å‡ºæœ€å¯ç–‘çš„å±¤ ===
print("\nç¬¬äº”æ­¥ï¼šè­˜åˆ¥æœ€å¯ç–‘çš„å±¤")
print("-" * 80)

# çµ±è¨ˆæ¯å€‹åƒæ•¸çš„ç•°å¸¸æ¬¡æ•¸
param_anomaly_count = defaultdict(int)
for anomaly in anomalies:
    param_anomaly_count[anomaly["param"]] += 1

# æ’åº
sorted_params = sorted(param_anomaly_count.items(), key=lambda x: x[1], reverse=True)

print(f"\nğŸ¯ ç•°å¸¸æ¬¡æ•¸æœ€å¤šçš„åƒæ•¸ (Top 10):")
for i, (param_name, count) in enumerate(sorted_params[:10]):
    print(f"   {i+1}. {param_name}: {count} æ¬¡ç•°å¸¸")

# === ç”Ÿæˆæ™‚é–“ç·šåˆ†æ ===
print("\nç¬¬å…­æ­¥ï¼šç”Ÿæˆåƒæ•¸è®ŠåŒ–æ™‚é–“ç·š")
print("-" * 80)

# æŒ‰è¿­ä»£åˆ†çµ„ç•°å¸¸
iter_anomaly_count = defaultdict(int)
for anomaly in anomalies:
    # ä»¥ 100 ç‚ºå–®ä½åˆ†çµ„
    iter_group = (anomaly["iter"] // 100) * 100
    iter_anomaly_count[iter_group] += 1

print(f"\nğŸ“ˆ ç•°å¸¸å¯†åº¦åˆ†å¸ƒ:")
for iter_group in sorted(iter_anomaly_count.keys())[:20]:
    count = iter_anomaly_count[iter_group]
    bar = "â–ˆ" * min(count, 50)
    print(f"   #{iter_group:5d}-{iter_group+99:5d}: {bar} ({count})")

# === ä¿å­˜è©³ç´°å ±å‘Š ===
print("\nç¬¬ä¸ƒæ­¥ï¼šä¿å­˜åˆ†æå ±å‘Š")
print("-" * 80)

report = {
    "analysis_time": datetime.now().isoformat(),
    "iteration_range": [START_ITER, END_ITER],
    "checkpoints_analyzed": len(checkpoints_to_analyze),
    "total_anomalies": len(anomalies),
    "anomaly_breakdown": {
        "norm_explosion": len([a for a in anomalies if a["type"] == "norm_explosion"]),
        "norm_collapse": len([a for a in anomalies if a["type"] == "norm_collapse"]),
        "dead_parameters": len(
            [a for a in anomalies if a["type"] == "dead_parameters"]
        ),
        "std_collapse": len([a for a in anomalies if a["type"] == "std_collapse"]),
    },
    "top_problematic_params": [
        {"param": param, "anomaly_count": count} for param, count in sorted_params[:20]
    ],
    "anomaly_timeline": [
        {"iter_range": f"{iter_group}-{iter_group+99}", "count": count}
        for iter_group, count in sorted(iter_anomaly_count.items())
    ],
    "detailed_anomalies": anomalies[:100],  # ä¿å­˜å‰ 100 å€‹
}

with open("checkpoints/parameter_analysis_report.json", "w", encoding="utf-8") as f:
    json.dump(report, f, ensure_ascii=False, indent=2)

print("âœ… å ±å‘Šå·²ä¿å­˜åˆ°: checkpoints/parameter_analysis_report.json")

# === ç¸½çµå’Œå»ºè­° ===
print("\n" + "=" * 80)
print("ğŸ“‹ åˆ†æç¸½çµ")
print("=" * 80)

print(f"\nçµ±è¨ˆ:")
print(f"   åˆ†æçš„æª¢æŸ¥é»: {len(checkpoints_to_analyze)} å€‹")
print(f"   åˆ†æçš„åƒæ•¸: {len(param_stats)} å€‹")
print(f"   æª¢æ¸¬åˆ°çš„ç•°å¸¸: {len(anomalies)} å€‹")

print(f"\nç•°å¸¸é¡å‹åˆ†å¸ƒ:")
for anomaly_type, count in report["anomaly_breakdown"].items():
    print(f"   {anomaly_type}: {count} æ¬¡")

if sorted_params:
    print(f"\næœ€å¯ç–‘çš„å±¤:")
    for i, (param_name, count) in enumerate(sorted_params[:3]):
        print(f"   {i+1}. {param_name} ({count} æ¬¡ç•°å¸¸)")

print(f"\nğŸ¯ å»ºè­°:")
if report["anomaly_breakdown"]["norm_explosion"] > 10:
    print(f"   âš ï¸ æª¢æ¸¬åˆ° {report['anomaly_breakdown']['norm_explosion']} æ¬¡åƒæ•¸ç¯„æ•¸çˆ†ç‚¸")
    print(f"      å»ºè­°: é™ä½å­¸ç¿’ç‡æˆ–å¢åŠ æ¢¯åº¦è£å‰ª")

if report["anomaly_breakdown"]["dead_parameters"] > 10:
    print(f"   âš ï¸ æª¢æ¸¬åˆ° {report['anomaly_breakdown']['dead_parameters']} æ¬¡æ­»äº¡åƒæ•¸")
    print(f"      å»ºè­°: èª¿æ•´åˆå§‹åŒ–æ–¹æ³•æˆ–ä½¿ç”¨ Leaky ReLU")

if report["anomaly_breakdown"]["std_collapse"] > 10:
    print(f"   âš ï¸ æª¢æ¸¬åˆ° {report['anomaly_breakdown']['std_collapse']} æ¬¡æ¨™æº–å·®å´©æ½°")
    print(f"      å»ºè­°: å¢åŠ æ¢ç´¢å™ªéŸ³æˆ–èª¿æ•´å­¸ç¿’ç‡è¡°æ¸›")

print(f"\n" + "=" * 80)
