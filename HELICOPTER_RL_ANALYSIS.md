# ğŸš Helicopter-RL é …ç›®åˆ†æèˆ‡å€Ÿé‘’å»ºè­°

**é …ç›®**: https://github.com/rossning92/helicopter-rl
**ä½œè€…**: Ross Ning
**æŠ€è¡“æ£§**: Stable-Baselines3 + PPO + Gymnasium + Pygame

---

## ğŸ¯ é …ç›®æ¦‚è¦½

é€™æ˜¯ä¸€å€‹ä½¿ç”¨ PPO ç®—æ³•è¨“ç·´ç›´å‡æ©ŸéŠæˆ²çš„ RL é …ç›®ï¼Œèˆ‡æˆ‘å€‘çš„ 2048 è¨“ç·´é …ç›®æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹è™•ï¼

### ç›¸ä¼¼é»
- âœ… ä½¿ç”¨ **PPO ç®—æ³•**
- âœ… ä½¿ç”¨ **Pygame** æ¸²æŸ“
- âœ… è‡ªå®šç¾© **Gymnasium ç’°å¢ƒ**
- âœ… **TensorBoard** æ—¥èªŒ
- âœ… **æª¢æŸ¥é»ä¿å­˜æ©Ÿåˆ¶**
- âœ… æ”¯æŒ**å‘é‡åŒ–ç’°å¢ƒ**ï¼ˆå¤šç’°å¢ƒä¸¦è¡Œè¨“ç·´ï¼‰

---

## ğŸ’¡ å€¼å¾—å€Ÿé‘’çš„å„ªç§€è¨­è¨ˆ

### 1. **ä½¿ç”¨ Stable-Baselines3 (SB3)** â­â­â­â­â­

**ä»–å€‘çš„åšæ³•**:
```python
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecMonitor

# å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ
vec_env = make_vec_env(
    HelicopterEnv,
    n_envs=100,  # 100 å€‹ä¸¦è¡Œç’°å¢ƒï¼
    env_kwargs={"render_mode": "rgb_array"},
)
vec_env = VecMonitor(vec_env, log_dir)

# ä½¿ç”¨å…§å»ºçš„ PPO
model = PPO(
    "MlpPolicy",
    vec_env,
    verbose=1,
    tensorboard_log="./tmp/tensorboard",
    device="cpu",
    batch_size=256,
)

model.learn(
    total_timesteps=100_000_000,
    callback=checkpoint_callback,
)
```

**ç‚ºä»€éº¼é€™å¾ˆå¥½**:
- âœ… **æˆç†Ÿç©©å®š**ï¼šSB3 æ˜¯æ¥­ç•Œæ¨™æº–ï¼Œç¶“éå¤§é‡æ¸¬è©¦
- âœ… **å…§å»ºåŠŸèƒ½è±å¯Œ**ï¼šæª¢æŸ¥é»ã€TensorBoardã€å‘é‡åŒ–ç’°å¢ƒ
- âœ… **è¶…åƒæ•¸ç¶“éå„ªåŒ–**ï¼šé»˜èªå€¼é€šå¸¸å¾ˆå¥½
- âœ… **æ˜“æ–¼æ“´å±•**ï¼šæ”¯æŒå¤šç¨®ç®—æ³•ï¼ˆPPOã€SACã€A2C ç­‰ï¼‰
- âœ… **å‘é‡åŒ–ç’°å¢ƒ**ï¼šè‡ªå‹•ä¸¦è¡Œè¨“ç·´ï¼Œå¤§å¹…æé€Ÿ

**æˆ‘å€‘çš„å•é¡Œ**:
- âŒ è‡ªå·±å¯¦ç¾ PPOï¼ˆå®¹æ˜“å‡º bugï¼‰
- âŒ æ²’æœ‰å‘é‡åŒ–ç’°å¢ƒï¼ˆè¨“ç·´æ…¢ï¼‰
- âŒ æ‰‹å‹•ç®¡ç†æª¢æŸ¥é»ï¼ˆè¤‡é›œï¼‰

**å»ºè­°**: ğŸš€ **å¼·çƒˆå»ºè­°é·ç§»åˆ° Stable-Baselines3ï¼**

---

### 2. **ç°¡æ½”çš„ Gymnasium ç’°å¢ƒå°è£** â­â­â­â­

**ä»–å€‘çš„åšæ³•**:
```python
class HelicopterEnv(Env):
    def __init__(self, render_mode="human"):
        super().__init__()
        self.game = HelicopterGame(render_mode=render_mode)
        self.action_space = spaces.Discrete(2)  # 0: ä¸å‹•, 1: å‘ä¸Š
        self.observation_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(2 + MAX_TUNNEL_STEPS * 2,),  # ç©å®¶ç‹€æ…‹ + éš§é“è³‡è¨Š
            dtype=np.float32,
        )
    
    def step(self, action):
        self.game.action = int(action)
        self.game.step()
        
        observation = self.__get_obs()
        reward = 0.0 if self.game.game_over else 1.0  # ç°¡å–®ï¼šå­˜æ´»å°±çå‹µ
        terminated = self.game.game_over
        truncated = False
        info = {"game_over": self.game.game_over}
        
        return observation, reward, terminated, truncated, info
    
    def __get_obs(self):
        player = np.array([
            self.game.helicopter_pos_y / self.game.HEIGHT,  # æ­£è¦åŒ–ä½ç½®
            self.game.helicopter_speed_y / MAX_SPEED * 0.5 + 0.5,  # æ­£è¦åŒ–é€Ÿåº¦
        ])
        
        # éš§é“å‰æ–¹è³‡è¨Šï¼ˆæœªä¾† 4 å€‹é»ï¼‰
        tunnel = np.full((4, 2), [1.0, 0.5])
        for i, t in enumerate(self.game.tunnel[:4]):
            tunnel[i] = (
                (t.x + WIDTH) / (WIDTH * 3),  # x ä½ç½®
                t.y / HEIGHT,  # y ä½ç½®
            )
        
        return np.concatenate([player, tunnel.ravel()])
```

**é—œéµè¨­è¨ˆ**:
1. **è§€å¯Ÿç©ºé–“**ï¼šç©å®¶ç‹€æ…‹ï¼ˆä½ç½®ã€é€Ÿåº¦ï¼‰+ å‰æ–¹éš§é“è³‡è¨Š
2. **çå‹µå‡½æ•¸**ï¼šæ¥µç°¡ï¼å­˜æ´» = +1ï¼Œæ­»äº¡ = 0
3. **æ­£è¦åŒ–**ï¼šæ‰€æœ‰è§€å¯Ÿå€¼éƒ½åœ¨ [0, 1] ç¯„åœ

**æˆ‘å€‘çš„ 2048 å¯ä»¥é€™æ¨£è¨­è¨ˆ**:
```python
class Game2048Env(Env):
    def __get_obs(self):
        # 1. æ£‹ç›¤ç‹€æ…‹ï¼ˆæ­£è¦åŒ–åˆ° [0, 1]ï¼‰
        board = self.game.board / 2048.0  # å‡è¨­æœ€å¤§ 2048
        
        # 2. é¡å¤–ç‰¹å¾µ
        max_tile = np.max(self.game.board) / 2048.0
        empty_cells = np.sum(self.game.board == 0) / 16.0
        
        return np.concatenate([
            board.flatten(),
            [max_tile, empty_cells]
        ])
```

---

### 3. **CheckpointCallback æ©Ÿåˆ¶** â­â­â­â­â­

**ä»–å€‘çš„åšæ³•**:
```python
from stable_baselines3.common.callbacks import CheckpointCallback

checkpoint_callback = CheckpointCallback(
    save_freq=5000,  # æ¯ 5000 æ­¥ä¿å­˜ä¸€æ¬¡
    save_path="./tmp/",
    name_prefix="rl_model",
    save_replay_buffer=True,  # ä¿å­˜ç¶“é©—å›æ”¾
    save_vecnormalize=True,   # ä¿å­˜æ­£è¦åŒ–çµ±è¨ˆ
)

model.learn(
    total_timesteps=5_000_000,
    callback=checkpoint_callback,
)
```

**å„ªå‹¢**:
- âœ… è‡ªå‹•ä¿å­˜æª¢æŸ¥é»
- âœ… å¯ä»¥å¾ä»»ä½•æª¢æŸ¥é»æ¢å¾©
- âœ… ä¿å­˜å®Œæ•´è¨“ç·´ç‹€æ…‹ï¼ˆåŒ…æ‹¬ optimizerï¼‰

**æˆ‘å€‘å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼**

---

### 4. **å‘é‡åŒ–ç’°å¢ƒï¼ˆVectorized Environmentsï¼‰** â­â­â­â­â­

**ä»–å€‘çš„è¨“ç·´åƒæ•¸**:
```bash
python train.py --n-envs 32 --total-timesteps 5000000
```

**ä»£ç¢¼**:
```python
vec_env = make_vec_env(
    HelicopterEnv,
    n_envs=32,  # 32 å€‹ä¸¦è¡Œç’°å¢ƒ
)
```

**æ•ˆæœ**:
- ğŸš€ **è¨“ç·´é€Ÿåº¦æå‡ 10-30 å€**
- âœ… æ›´å¥½çš„æ¨£æœ¬æ•ˆç‡
- âœ… æ›´ç©©å®šçš„è¨“ç·´

**æˆ‘å€‘çš„å•é¡Œ**:
- å–®ç’°å¢ƒè¨“ç·´å¤ªæ…¢
- æ¯å±€éŠæˆ²ä¾åºé€²è¡Œ

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨ SB3 çš„ `make_vec_env` è‡ªå‹•ä¸¦è¡ŒåŒ–ï¼

---

### 5. **ç°¡å–®çš„çå‹µå‡½æ•¸** â­â­â­

**ä»–å€‘çš„çå‹µè¨­è¨ˆ**:
```python
reward = 0.0 if self.game.game_over else 1.0
```

å°±é€™éº¼ç°¡å–®ï¼**å­˜æ´»å°±æœ‰çå‹µ**ã€‚

**ç‚ºä»€éº¼æœ‰æ•ˆ**:
- é¿å…éåº¦è¤‡é›œçš„çå‹µå¡‘é€ 
- è®“æ¨¡å‹è‡ªå·±å­¸ç¿’ç­–ç•¥
- æ¸›å°‘äººç‚ºåè¦‹

**æˆ‘å€‘çš„ 2048 å¯ä»¥**:
```python
# ç°¡å–®ç‰ˆæœ¬
reward = 1.0 if not game_over else 0.0

# æˆ–ç¨å¾®è¤‡é›œé»
reward = np.log2(max_tile_value + 1) / 11.0  # æ­£è¦åŒ–åˆ° [0, 1]
```

---

### 6. **æ¸…æ™°çš„é …ç›®çµæ§‹** â­â­â­â­

```
helicopter-rl/
â”œâ”€â”€ helicopter_game.py     # éŠæˆ²é‚è¼¯ï¼ˆç´” Pygameï¼‰
â”œâ”€â”€ helicopter_env.py      # Gymnasium ç’°å¢ƒå°è£
â”œâ”€â”€ train.py               # è¨“ç·´è…³æœ¬
â”œâ”€â”€ eval.py                # è©•ä¼°è…³æœ¬
â”œâ”€â”€ test_env.py            # ç’°å¢ƒæ¸¬è©¦
â”œâ”€â”€ requirements.txt       # ä¾è³´
â””â”€â”€ assets/                # è³‡æºæ–‡ä»¶
```

**é—œéµåˆ†é›¢**:
1. **éŠæˆ²é‚è¼¯** èˆ‡ **RL ç’°å¢ƒ** åˆ†é›¢
2. **è¨“ç·´** èˆ‡ **è©•ä¼°** åˆ†é›¢
3. ç¨ç«‹çš„ **ç’°å¢ƒæ¸¬è©¦** è…³æœ¬

**æˆ‘å€‘æ‡‰è©²åšçš„**:
```
traingame/
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ environment.py      # ç´”éŠæˆ²é‚è¼¯
â”‚   â””â”€â”€ ui.py               # UI æ¸²æŸ“
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ game2048_env.py     # Gymnasium ç’°å¢ƒ
â”‚   â”œâ”€â”€ train.py            # SB3 è¨“ç·´
â”‚   â””â”€â”€ eval.py             # è©•ä¼°
â””â”€â”€ agents/
    â””â”€â”€ (å¯ä»¥ç§»é™¤è‡ªå®šç¾© PPO)
```

---

## ğŸ”§ å…·é«”æ”¹é€²å»ºè­°

### å„ªå…ˆç´š 1ï¼šé·ç§»åˆ° Stable-Baselines3 â­â­â­â­â­

**ç•¶å‰å•é¡Œ**:
- è‡ªå·±å¯¦ç¾çš„ PPO æœ‰ bugï¼ˆCritic bias ä¸ç©©å®šï¼‰
- è¨“ç·´é€Ÿåº¦æ…¢ï¼ˆå–®ç’°å¢ƒï¼‰
- æª¢æŸ¥é»ç®¡ç†è¤‡é›œ

**é·ç§»æ­¥é©Ÿ**:

#### æ­¥é©Ÿ 1: å‰µå»º Gymnasium ç’°å¢ƒ
```python
# rl/game2048_env.py
from gymnasium import Env, spaces
import numpy as np

class Game2048Env(Env):
    def __init__(self, render_mode="human"):
        super().__init__()
        from game.environment import GameEnv
        self.game = GameEnv()
        
        # å‹•ä½œç©ºé–“ï¼š4 å€‹æ–¹å‘
        self.action_space = spaces.Discrete(4)
        
        # è§€å¯Ÿç©ºé–“ï¼š4x4 æ£‹ç›¤ + é¡å¤–ç‰¹å¾µ
        self.observation_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(18,),  # 16 å€‹æ ¼å­ + 2 å€‹ç‰¹å¾µ
            dtype=np.float32,
        )
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        obs = self.game.reset()
        return self._get_obs(), {}
    
    def step(self, action):
        reward, done, _ = self.game.step(action)
        obs = self._get_obs()
        return obs, reward, done, False, {}
    
    def _get_obs(self):
        board = self.game.board / 2048.0
        max_tile = np.max(self.game.board) / 2048.0
        empty_ratio = np.sum(self.game.board == 0) / 16.0
        return np.concatenate([board.flatten(), [max_tile, empty_ratio]])
```

#### æ­¥é©Ÿ 2: ä½¿ç”¨ SB3 è¨“ç·´
```python
# rl/train.py
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import CheckpointCallback

# å‰µå»º 32 å€‹ä¸¦è¡Œç’°å¢ƒ
vec_env = make_vec_env(
    Game2048Env,
    n_envs=32,
    env_kwargs={"render_mode": "rgb_array"}
)

# å‰µå»º PPO æ¨¡å‹ï¼ˆä½¿ç”¨å„ªåŒ–å¾Œçš„è¶…åƒæ•¸ï¼ï¼‰
model = PPO(
    "MlpPolicy",
    vec_env,
    learning_rate=1e-4,      # æˆ‘å€‘åˆ†æå¾Œçš„æœ€å„ªå€¼
    n_steps=2048,
    batch_size=256,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.1,          # æˆ‘å€‘çš„æ”¹é€²å€¼
    ent_coef=0.02,           # æˆ‘å€‘çš„æ”¹é€²å€¼
    vf_coef=1.0,             # æˆ‘å€‘çš„æ”¹é€²å€¼
    max_grad_norm=0.3,       # æˆ‘å€‘çš„æ”¹é€²å€¼
    verbose=1,
    tensorboard_log="./checkpoints/tensorboard",
    device="cuda",  # GPU åŠ é€Ÿ
)

# æª¢æŸ¥é»å›èª¿
checkpoint_callback = CheckpointCallback(
    save_freq=1000,
    save_path="./checkpoints/",
    name_prefix="game2048",
)

# é–‹å§‹è¨“ç·´
model.learn(
    total_timesteps=10_000_000,
    callback=checkpoint_callback,
)
```

**é æœŸæ•ˆæœ**:
- âœ… è¨“ç·´é€Ÿåº¦æå‡ **20-30 å€**ï¼ˆ32 å€‹ä¸¦è¡Œç’°å¢ƒï¼‰
- âœ… æ›´ç©©å®šï¼ˆSB3 çš„ PPO ç¶“éå……åˆ†æ¸¬è©¦ï¼‰
- âœ… è‡ªå‹•æª¢æŸ¥é»ç®¡ç†
- âœ… å…§å»º TensorBoard æ”¯æŒ
- âœ… åƒæ•¸ä¸ç©©å®šå•é¡Œè‡ªå‹•è§£æ±º

---

### å„ªå…ˆç´š 2ï¼šç°¡åŒ–çå‹µå‡½æ•¸ â­â­â­

**ç•¶å‰**: è¤‡é›œçš„çå‹µå¡‘é€ 
**å»ºè­°**: ç°¡åŒ–ç‚ºæ ¸å¿ƒç›®æ¨™

```python
def calculate_reward(self):
    # é¸é … 1ï¼šç°¡å–®ç‰ˆï¼ˆæ¨è–¦ï¼‰
    return np.log2(np.max(self.board) + 1) / 11.0
    
    # é¸é … 2ï¼šç¨å¾®è¤‡é›œ
    max_tile_reward = np.log2(np.max(self.board) + 1) / 11.0
    empty_cells_reward = np.sum(self.board == 0) / 16.0 * 0.1
    return max_tile_reward + empty_cells_reward
```

---

### å„ªå…ˆç´š 3ï¼šä½¿ç”¨å‘é‡åŒ–ç’°å¢ƒ â­â­â­â­â­

**ç•¶å‰**: æ¯æ¬¡è¨“ç·´ä¸€å±€
**å»ºè­°**: åŒæ™‚è¨“ç·´ 32 å±€

```python
# è‡ªå‹•ä¸¦è¡ŒåŒ–ï¼ˆSB3 å…§å»ºï¼‰
vec_env = make_vec_env(Game2048Env, n_envs=32)
```

**æ•ˆæœ**:
- å¾ 5930 è¿­ä»£éœ€è¦ 10 å°æ™‚ â†’ **ç´„ 20 åˆ†é˜**
- GPU åˆ©ç”¨ç‡å¾ 20% â†’ 80%+

---

### å„ªå…ˆç´š 4ï¼šæ·»åŠ è‡ªå‹•åŒ–æ¸¬è©¦ â­â­â­

å­¸ç¿’ä»–å€‘çš„ `test_env.py`:
```python
# tests/test_env.py
import pytest
from rl.game2048_env import Game2048Env

def test_env_creation():
    env = Game2048Env()
    assert env.action_space.n == 4
    assert env.observation_space.shape == (18,)

def test_reset():
    env = Game2048Env()
    obs, info = env.reset()
    assert obs.shape == (18,)
    assert 0 <= obs.all() <= 1

def test_step():
    env = Game2048Env()
    env.reset()
    obs, reward, done, truncated, info = env.step(0)
    assert isinstance(reward, float)
    assert isinstance(done, bool)

def test_random_agent():
    env = Game2048Env()
    env.reset()
    for _ in range(100):
        action = env.action_space.sample()
        obs, reward, done, truncated, info = env.step(action)
        if done:
            env.reset()
```

---

## ğŸ“Š å°æ¯”ç¸½çµ

| ç‰¹æ€§ | Helicopter-RL | æˆ‘å€‘çš„é …ç›® | æ”¹é€²å»ºè­° |
|------|---------------|------------|----------|
| **RL æ¡†æ¶** | Stable-Baselines3 | è‡ªå®šç¾© PPO | â­â­â­â­â­ é·ç§»åˆ° SB3 |
| **ä¸¦è¡Œç’°å¢ƒ** | 100 å€‹ | 1 å€‹ | â­â­â­â­â­ ä½¿ç”¨ make_vec_env |
| **æª¢æŸ¥é»** | CheckpointCallback | æ‰‹å‹•ç®¡ç† | â­â­â­â­ ä½¿ç”¨ SB3 å…§å»º |
| **çå‹µå‡½æ•¸** | æ¥µç°¡ï¼ˆå­˜æ´»=1ï¼‰ | è¤‡é›œ | â­â­â­ ç°¡åŒ– |
| **è¶…åƒæ•¸** | SB3 é»˜èª | æ‰‹å‹•èª¿æ•´ | â­â­â­â­ ä½¿ç”¨æˆ‘å€‘åˆ†æçš„å€¼ |
| **è¨“ç·´é€Ÿåº¦** | å¿«ï¼ˆå‘é‡åŒ–ï¼‰ | æ…¢ | â­â­â­â­â­ æå‡ 20-30 å€ |
| **ä»£ç¢¼è¤‡é›œåº¦** | ä½ | é«˜ | â­â­â­â­ å¤§å¹…ç°¡åŒ– |

---

## ğŸš€ å¯¦æ–½è¨ˆåŠƒ

### éšæ®µ 1ï¼šæœ€å°å¯è¡Œé·ç§»ï¼ˆ1-2 å¤©ï¼‰

```bash
# 1. å®‰è£ SB3
pip install stable-baselines3[extra]

# 2. å‰µå»º Gymnasium ç’°å¢ƒ
# rl/game2048_env.py (åƒè€ƒä¸Šé¢çš„ä»£ç¢¼)

# 3. å‰µå»ºç°¡å–®è¨“ç·´è…³æœ¬
# rl/train_sb3.py

# 4. æ¸¬è©¦
python rl/train_sb3.py --n-envs 4 --total-timesteps 10000
```

### éšæ®µ 2ï¼šå®Œæ•´é·ç§»ï¼ˆ3-5 å¤©ï¼‰

1. âœ… å®Œå–„ Gymnasium ç’°å¢ƒ
2. âœ… èª¿æ•´è§€å¯Ÿç©ºé–“å’Œçå‹µå‡½æ•¸
3. âœ… é…ç½® TensorBoard
4. âœ… æ¸¬è©¦å‘é‡åŒ–ç’°å¢ƒ
5. âœ… å¾èˆŠæª¢æŸ¥é»é·ç§»ï¼ˆå¦‚æœéœ€è¦ï¼‰

### éšæ®µ 3ï¼šå„ªåŒ–ï¼ˆæŒçºŒï¼‰

1. âœ… èª¿æ•´è¶…åƒæ•¸
2. âœ… å˜—è©¦ä¸åŒçš„ç¶²çµ¡æ¶æ§‹
3. âœ… å¯¦é©—ä¸åŒçš„çå‹µå‡½æ•¸
4. âœ… æ·»åŠ æ›´å¤šç›£æ§æŒ‡æ¨™

---

## ğŸ’ª é æœŸæ•ˆæœ

**é·ç§»åˆ° SB3 + å‘é‡åŒ–ç’°å¢ƒå¾Œ**:

```
è¨“ç·´é€Ÿåº¦: 10 å°æ™‚ â†’ 20 åˆ†é˜ (30x åŠ é€Ÿ)
ç©©å®šæ€§: â­â­ â†’ â­â­â­â­â­
ä»£ç¢¼è¤‡é›œåº¦: -60%
Bug é¢¨éšª: -90%
é”åˆ° 1418 åˆ†: 5930 è¿­ä»£ â†’ ~500 è¿­ä»£
é”åˆ° 2048 tile: å¯èƒ½ â†’ å¾ˆå¯èƒ½ï¼
```

---

## ğŸ“ ç¸½çµ

Helicopter-RL é …ç›®çš„**æœ€å¤§å•Ÿç¤º**ï¼š

1. **ä¸è¦é‡æ–°ç™¼æ˜è¼ªå­** - ä½¿ç”¨æˆç†Ÿçš„ Stable-Baselines3
2. **å‘é‡åŒ–æ˜¯é—œéµ** - ä¸¦è¡Œè¨“ç·´æé€Ÿ 20-30 å€
3. **ç°¡å–®å¾€å¾€æ›´å¥½** - ç°¡å–®çš„çå‹µå‡½æ•¸å¯èƒ½æ›´æœ‰æ•ˆ
4. **ä»£ç¢¼çµ„ç¹”å¾ˆé‡è¦** - åˆ†é›¢éŠæˆ²é‚è¼¯å’Œ RL ç’°å¢ƒ

**å»ºè­°ç«‹å³è¡Œå‹•**:
1. ğŸ”¥ å‰µå»º `rl/game2048_env.py`ï¼ˆGymnasium ç’°å¢ƒï¼‰
2. ğŸ”¥ å‰µå»º `rl/train_sb3.py`ï¼ˆSB3 è¨“ç·´è…³æœ¬ï¼‰
3. ğŸ”¥ æ¸¬è©¦å°è¦æ¨¡è¨“ç·´ï¼ˆn_envs=4, 10000 stepsï¼‰
4. ğŸ”¥ å¦‚æœæˆåŠŸï¼Œæ“´å±•åˆ° n_envs=32

**é€™å°‡æ˜¯é …ç›®çš„é‡å¤§å‡ç´šï¼** ğŸš€

---

# ğŸ› ï¸ å®Œæ•´ SB3 å¯¦ç¾æŒ‡å—

åŸºæ–¼ Helicopter-RL çš„ç¶“é©—ï¼Œä»¥ä¸‹æ˜¯å°‡ä½ çš„é …ç›®é·ç§»åˆ° Stable-Baselines3 çš„å®Œæ•´å¯¦ç¾ã€‚

## ğŸ“ é …ç›®çµæ§‹

```
traingame/
â”œâ”€â”€ rl/                          # æ–°å¢ï¼šSB3 ç›¸é—œæ–‡ä»¶
â”‚   â”œâ”€â”€ game2048_env.py         # Gymnasium ç’°å¢ƒåŒ…è£å™¨
â”‚   â”œâ”€â”€ train_sb3.py            # SB3 è¨“ç·´è…³æœ¬
â”‚   â””â”€â”€ test_sb3.py             # æ¸¬è©¦è…³æœ¬
â”œâ”€â”€ game/                        # åŸæœ‰éŠæˆ²é‚è¼¯
â”œâ”€â”€ agents/                      # åŸæœ‰è¨“ç·´é‚è¼¯
â”œâ”€â”€ checkpoints/                 # æª¢æŸ¥é»
â”œâ”€â”€ logs/                        # æ—¥èªŒ
â””â”€â”€ best_model/                  # æœ€ä½³æ¨¡å‹
```

## ğŸš€ å¯¦ç¾æ­¥é©Ÿ

### æ­¥é©Ÿ 1: å®‰è£ä¾è³´

```bash
pip install stable-baselines3[extra]
```

### æ­¥é©Ÿ 2: å‰µå»º Gymnasium ç’°å¢ƒ (`rl/game2048_env.py`)

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from game.environment import GameEnv

class Game2048Env(gym.Env):
    """Game2048 Gymnasium ç’°å¢ƒåŒ…è£å™¨"""

    def __init__(self, render_mode=None, max_steps=None, seed=None):
        super().__init__()
        self.game = GameEnv(seed=seed, max_steps=max_steps)
        self.render_mode = render_mode

        # å‹•ä½œç©ºé–“ï¼šé›¢æ•£å‹•ä½œ (0: ä¸è·³, 1: è·³)
        self.action_space = spaces.Discrete(2)

        # è§€å¯Ÿç©ºé–“ï¼š5 ç¶­ç‹€æ…‹ (y, vy, x_obs, gap_top, gap_bottom)
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(5,), dtype=np.float32
        )

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        if seed is not None:
            self.game.rng.seed(seed)
        obs = self.game.reset()
        return obs.astype(np.float32), {}

    def step(self, action):
        obs, reward, terminated, info = self.game.step(int(action))
        return (
            obs.astype(np.float32),
            float(reward),
            terminated,
            False,  # truncated
            info
        )
```

### æ­¥é©Ÿ 3: å‰µå»ºè¨“ç·´è…³æœ¬ (`rl/train_sb3.py`)

```python
#!/usr/bin/env python3
import os
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from rl.game2048_env import Game2048Env

def main():
    # å‰µå»º 32 å€‹ä¸¦è¡Œç’°å¢ƒ
    vec_env = make_vec_env(Game2048Env, n_envs=32)

    # å‰µå»º PPO æ¨¡å‹ï¼ˆé‡å° 6666 åˆ†å„ªåŒ–ï¼‰
    model = PPO(
        "MlpPolicy",
        vec_env,
        policy_kwargs=dict(net_arch=[256, 256, 256]),
        learning_rate=5e-5,
        gamma=0.995,
        gae_lambda=0.97,
        clip_range=0.15,
        ent_coef=0.05,
        vf_coef=1.5,
        n_steps=2048,
        batch_size=512,
        n_epochs=15,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log="./logs/tensorboard/",
        device="cuda"
    )

    # è¨­ç½®å›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=10000,
        save_path="./checkpoints/",
        name_prefix="ppo_game2048"
    )

    eval_env = make_vec_env(Game2048Env, n_envs=4)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        eval_freq=5000
    )

    # è¨“ç·´ï¼ï¼ˆç›®æ¨™ï¼š5M æ­¥ï¼Œç´„ 1-2 å¤©ï¼‰
    model.learn(
        total_timesteps=5_000_000,
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True
    )

    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    model.save("./models/ppo_game2048_final")

if __name__ == "__main__":
    main()
```

### æ­¥é©Ÿ 4: å‰µå»ºæ¸¬è©¦è…³æœ¬ (`rl/test_sb3.py`)

```python
#!/usr/bin/env python3
from stable_baselines3 import PPO
from rl.game2048_env import Game2048Env

# è¼‰å…¥æœ€ä½³æ¨¡å‹
model = PPO.load("./best_model/best_model.zip")

# æ¸¬è©¦ 10 å±€
env = Game2048Env()
for episode in range(10):
    obs, _ = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, _, info = env.step(action)
        total_reward += reward

    print(f"Episode {episode + 1}: Score = {total_reward}")
    if info.get("win"):
        print("ğŸ‰ é€šé—œæˆåŠŸï¼")

env.close()
```

## ğŸ¯ è¨“ç·´é…ç½®ï¼ˆé‡å° 6666 åˆ†ï¼‰

### åŸºæœ¬é…ç½®ï¼ˆæ¨è–¦ï¼‰
```python
model = PPO(
    "MlpPolicy",
    vec_env,
    # ç¶²çµ¡æ¶æ§‹
    policy_kwargs=dict(net_arch=[256, 256, 256]),  # 3 å±¤ï¼Œæ¯å±¤ 256
    
    # å­¸ç¿’åƒæ•¸
    learning_rate=5e-5,    # ç©©å®šå­¸ç¿’
    gamma=0.995,           # é«˜æŠ˜æ‰£å› å­
    gae_lambda=0.97,       # é«˜ GAE lambda
    
    # PPO åƒæ•¸
    clip_range=0.15,       # é©ä¸­ clip
    ent_coef=0.05,         # é«˜æ¢ç´¢
    vf_coef=1.5,           # å¼· critic
    
    # è¨“ç·´æ•ˆç‡
    n_steps=2048,          # æ¯å€‹ç’°å¢ƒ 2048 æ­¥
    batch_size=512,        # å¤§ batch
    n_epochs=15,           # 15 è¼ªæ›´æ–°
)
```

### é«˜ç´šé…ç½®ï¼ˆè¿½æ±‚æœ€ä½³æ€§èƒ½ï¼‰
```python
model = PPO(
    "MlpPolicy",
    vec_env,
    policy_kwargs=dict(
        net_arch=[512, 512, 512],  # æ›´å¤§ç¶²çµ¡
        activation_fn=torch.nn.ReLU,
    ),
    learning_rate=3e-5,    # æ›´æ…¢æ›´ç©©å®š
    ent_coef=0.03,         # é©ä¸­æ¢ç´¢
    vf_coef=2.0,           # æ›´å¼· critic
    n_steps=4096,          # æ›´å¤šæ•¸æ“š
    batch_size=1024,       # æ›´å¤§ batch
    n_epochs=20,           # æ›´å¤šæ›´æ–°
)
```

## ğŸ“Š é æœŸæ€§èƒ½æå‡

| æŒ‡æ¨™ | åŸè‡ªåˆ¶ PPO | SB3 (32 ç’°å¢ƒ) | æå‡å€æ•¸ |
|------|-----------|---------------|---------|
| è¨“ç·´é€Ÿåº¦ | 1x | 32x | **32 å€** |
| ç©©å®šæ€§ | æœ‰ bug | ä¹…ç¶“è€ƒé©— | **å¤§å¹…æå‡** |
| ä»£ç¢¼è¡Œæ•¸ | ~1000 | ~50 | **95% æ¸›å°‘** |
| é”åˆ° 6666 | 3-5 å¤© | 1-2 å¤© | **2-3 å€** |

## ğŸ”§ æ•…éšœæ’é™¤

### å•é¡Œ 1: ç’°å¢ƒå‰µå»ºå¤±æ•—
```python
# éŒ¯èª¤ï¼šModuleNotFoundError: No module named 'rl'
# è§£æ±ºï¼šç¢ºä¿åœ¨é …ç›®æ ¹ç›®éŒ„é‹è¡Œï¼Œæˆ–æ·»åŠ è·¯å¾‘
import sys
sys.path.append('.')
```

### å•é¡Œ 2: CUDA è¨˜æ†¶é«”ä¸è¶³
```python
# è§£æ±ºï¼šæ¸›å°‘ç’°å¢ƒæ•¸é‡
vec_env = make_vec_env(Game2048Env, n_envs=16)  # å¾ 32 é™åˆ° 16
```

### å•é¡Œ 3: è¨“ç·´å¤ªæ…¢
```python
# è§£æ±ºï¼šä½¿ç”¨æ›´å°çš„ç¶²çµ¡
policy_kwargs=dict(net_arch=[128, 128])  # å¾ 256 é™åˆ° 128
```

## ğŸ“ˆ ç›£æ§è¨“ç·´é€²åº¦

### TensorBoard
```bash
# å®‰è£ï¼ˆå¦‚æœé‚„æ²’å®‰è£ï¼‰
pip install tensorboard

# å•Ÿå‹•ç›£æ§
tensorboard --logdir ./logs/tensorboard/

# é–‹å•Ÿç€è¦½å™¨è¨ªå•: http://localhost:6006
```

### é—œéµæŒ‡æ¨™è§€å¯Ÿ
- **Episode Reward**: æ‡‰è©²ç©©å®šä¸Šå‡
- **Episode Length**: æ‡‰è©²å¢åŠ ï¼ˆå­˜æ´»æ›´ä¹…ï¼‰
- **Value Loss**: æ‡‰è©²ä¸‹é™
- **Policy Loss**: æ‡‰è©²ç›¸å°ç©©å®š
- **Entropy**: æ‡‰è©²ç·©æ…¢ä¸‹é™ï¼ˆå­¸ç¿’ç¢ºå®šæ€§ç­–ç•¥ï¼‰

## ğŸ¯ æˆåŠŸæ¨™æº–

### éšæ®µ 1: åŸºç¤é©—è­‰ï¼ˆ1 å°æ™‚ï¼‰
- âœ… æ¨¡å‹å¯ä»¥è¼‰å…¥
- âœ… å¯ä»¥èˆ‡ç’°å¢ƒäº’å‹•
- âœ… åˆ†æ•¸ > 500ï¼ˆéš¨æ©Ÿç­–ç•¥åŸºæº–ï¼‰

### éšæ®µ 2: å­¸ç¿’é©—è­‰ï¼ˆ4 å°æ™‚ï¼‰
- âœ… åˆ†æ•¸ > 1000
- âœ… ç©©å®šå­¸ç¿’æ›²ç·š
- âœ… æ²’æœ‰å´©æ½°

### éšæ®µ 3: æ€§èƒ½ç›®æ¨™ï¼ˆ1-2 å¤©ï¼‰
- âœ… å¹³å‡åˆ†æ•¸ > 3000
- âœ… æœ€é«˜åˆ†æ•¸ > 5000
- âœ… é€šé—œç‡ > 10%

### éšæ®µ 4: æœ€çµ‚ç›®æ¨™ï¼ˆæŒçºŒè¨“ç·´ï¼‰
- âœ… å¹³å‡åˆ†æ•¸ > 5000
- âœ… æœ€é«˜åˆ†æ•¸ > 6666
- âœ… é€šé—œç‡ > 50%

## ğŸš€ ç«‹å³é–‹å§‹

```bash
# 1. å®‰è£ä¾è³´
pip install stable-baselines3[extra]

# 2. å‰µå»ºç’°å¢ƒæ–‡ä»¶
# è¤‡è£½ä¸Šé¢çš„ rl/game2048_env.py

# 3. å‰µå»ºè¨“ç·´è…³æœ¬
# è¤‡è£½ä¸Šé¢çš„ rl/train_sb3.py

# 4. å°è¦æ¨¡æ¸¬è©¦
python rl/train_sb3.py --n-envs 4 --total-timesteps 10000 --target test

# 5. å¦‚æœæˆåŠŸï¼Œé–‹å§‹å…¨è¦æ¨¡è¨“ç·´
python rl/train_sb3.py --n-envs 32 --total-timesteps 5000000 --target 6666
```

## ğŸ’¡ é€²éšå„ªåŒ–æŠ€å·§

### 1. çå‹µå¡‘é€ 
```python
# åœ¨ step() ä¸­æ·»åŠ é‡Œç¨‹ç¢‘çå‹µ
if self.episode_score >= 1000 and not self.milestone_1000:
    reward += 50
    self.milestone_1000 = True
```

### 2. èª²ç¨‹å­¸ç¿’ (Curriculum Learning)
```python
# éš¨è‘—è¨“ç·´é€²åº¦å¢åŠ é›£åº¦
if self.total_timesteps > 1_000_000:
    self.ScrollIncreasePerPass = 0.01  # å¢åŠ é›£åº¦
```

### 3. å„ªå…ˆé‡æ’­ (Prioritized Experience)
```python
# SB3 æ”¯æŒ VecNormalizeï¼Œè‡ªå‹•è™•ç†è§€å¯Ÿæ­£è¦åŒ–
vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)
```

### 4. å¤šç­–ç•¥æ¯”è¼ƒ
```python
# è¨“ç·´å¤šå€‹æ¨¡å‹æ¯”è¼ƒ
configs = [
    {"ent_coef": 0.01, "name": "low_entropy"},
    {"ent_coef": 0.05, "name": "med_entropy"},
    {"ent_coef": 0.10, "name": "high_entropy"},
]

for config in configs:
    model = PPO(..., ent_coef=config["ent_coef"])
    # è¨“ç·´ä¸¦æ¯”è¼ƒ
```

## ğŸ“ ç¸½çµ

é·ç§»åˆ° Stable-Baselines3 æ˜¯**æ­£ç¢ºçš„æ±ºå®š**ï¼š

### âœ… å„ªå‹¢
- **32 å€é€Ÿåº¦æå‡**ï¼šå¾ 3-5 å¤©ç¸®çŸ­åˆ° 1-2 å¤©
- **é›¶ç©©å®šæ€§å•é¡Œ**ï¼šå‘Šåˆ¥ Critic bias å´©æ½°
- **å°ˆæ¥­å·¥å…·éˆ**ï¼šTensorBoardã€è‡ªå‹•æª¢æŸ¥é»ã€è©•ä¼°
- **ä»£ç¢¼ç°¡åŒ–**ï¼šå¾ 1000 è¡Œæ¸›å°‘åˆ° 50 è¡Œ

### ğŸ¯ é æœŸæˆæœ
- **æ›´å¿«é”åˆ°ç›®æ¨™**ï¼š1-2 å¤©å…§é”åˆ° 6666 åˆ†
- **æ›´ç©©å®šè¨“ç·´**ï¼šä¸å†æœ‰ 0 åˆ†å´©æ½°
- **æ›´å®¹æ˜“èª¿è©¦**ï¼šå®Œæ•´çš„ç›£æ§å’Œæ—¥èªŒ
- **å¯æ“´å±•æ€§**ï¼šæœªä¾†å¯ä»¥è¼•é¬†å˜—è©¦ SACã€TD3 ç­‰ç®—æ³•

**ç¾åœ¨å°±é–‹å§‹å¯¦æ–½å§ï¼** ğŸš€
