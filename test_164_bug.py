"""æ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹èƒ½å¦çªç ´ 164 åˆ†"""

import torch

from agents.networks import ActorCritic
from game.environment import GameEnv


def test_trained_model():
    """ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹æ¸¬è©¦"""

    # æª¢æŸ¥æª¢æŸ¥é»
    import os

    checkpoint_dir = "checkpoints"
    checkpoints = [
        f
        for f in os.listdir(checkpoint_dir)
        if f.startswith("checkpoint_") and f.endswith(".pt")
    ]

    if not checkpoints:
        print("âŒ æ²’æœ‰æ‰¾åˆ°è¨“ç·´æª¢æŸ¥é»")
        return

    # ä½¿ç”¨æœ€æ–°çš„æª¢æŸ¥é»
    latest = max(checkpoints, key=lambda x: int(x.split("_")[1].split(".")[0]))
    checkpoint_path = os.path.join(checkpoint_dir, latest)

    print(f"è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")

    # è¼‰å…¥æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net = ActorCritic().to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # å˜—è©¦ä¸åŒçš„éµå
    if "model_state" in checkpoint:
        net.load_state_dict(checkpoint["model_state"])
    elif "model_state_dict" in checkpoint:
        net.load_state_dict(checkpoint["model_state_dict"])
    elif "net" in checkpoint:
        net.load_state_dict(checkpoint["net"])
    else:
        # ç›´æ¥è¼‰å…¥ï¼ˆå¯èƒ½æ•´å€‹æ–‡ä»¶å°±æ˜¯ state_dictï¼‰
        net.load_state_dict(checkpoint)

    net.eval()

    # å‰µå»ºç’°å¢ƒ
    env = GameEnv()
    print(f"ç’°å¢ƒ max_steps: {env.max_steps}")

    # é‹è¡Œ 5 å€‹å›åˆ
    print("\n" + "=" * 60)
    print("é–‹å§‹æ¸¬è©¦ 5 å€‹å›åˆ...")
    print("=" * 60)

    scores = []
    steps_list = []

    for episode in range(5):
        state = env.reset()
        done = False
        steps = 0
        episode_reward = 0

        while not done:
            # ä½¿ç”¨æ¨¡å‹é¸æ“‡å‹•ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                logits, _ = net(state_tensor)
                prob = torch.sigmoid(logits).item()
                action = 1 if prob > 0.5 else 0

            state, reward, done, info = env.step(action)
            episode_reward += reward
            steps += 1

            # é˜²æ­¢ç„¡é™å¾ªç’°
            if steps > 5000:
                print(f"  è­¦å‘Šï¼šç¬¬ {episode+1} å›åˆè¶…é 5000 æ­¥ï¼Œå¼·åˆ¶çµæŸ")
                break

        score = info.get("episode_score", episode_reward)
        win = info.get("win", False)
        scores.append(score)
        steps_list.append(steps)

        status = "ğŸ† å‹åˆ©" if win else "ğŸ’¥ ç¢°æ’"
        print(f"å›åˆ {episode+1}: {status} | åˆ†æ•¸: {score:.0f} | æ­¥æ•¸: {steps}")

    print("\n" + "=" * 60)
    print("æ¸¬è©¦çµæœçµ±è¨ˆ:")
    print("=" * 60)
    print(f"å¹³å‡åˆ†æ•¸: {sum(scores)/len(scores):.1f}")
    print(f"æœ€é«˜åˆ†æ•¸: {max(scores):.0f}")
    print(f"æœ€ä½åˆ†æ•¸: {min(scores):.0f}")
    print(f"å¹³å‡æ­¥æ•¸: {sum(steps_list)/len(steps_list):.0f}")

    # æª¢æŸ¥æ˜¯å¦æœ‰åˆ†æ•¸å¡åœ¨ 164
    scores_at_164 = [s for s in scores if 163 <= s <= 165]
    if len(scores_at_164) >= 3:
        print(f"\nâš ï¸ è­¦å‘Šï¼šæœ‰ {len(scores_at_164)} å€‹å›åˆçš„åˆ†æ•¸åœ¨ 164 å·¦å³")
        print("   é€™å¯èƒ½è¡¨ç¤ºä»ç„¶å­˜åœ¨é™åˆ¶")
    elif max(scores) > 200:
        print("\nâœ… æˆåŠŸï¼šAI èƒ½å¤ çªç ´ 164 åˆ†ï¼")
    else:
        print("\nâœ… æ¸¬è©¦å®Œæˆï¼Œæ²’æœ‰ç™¼ç¾æ˜é¡¯çš„ 164 åˆ†é™åˆ¶")


if __name__ == "__main__":
    test_trained_model()
