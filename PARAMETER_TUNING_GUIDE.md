# 訓練參數調整指南

## 🎯 快速開始

訓練過程中可以**實時調整參數**而無需重新啟動！只需編輯 `training_config.json` 文件，參數會在每10個迭代時自動重新讀取。

## 📝 配置文件位置

```
traingame/training_config.json
```

## ⚙️ 可調整參數

### 1. `batch_size` - 批量大小
- **當前值**: GPU=256, CPU=64
- **影響**: 訓練穩定性和速度
- **調整建議**:
  - GPU訓練效果差 → 降低到 **128** 或 **64**
  - 訓練太慢 → 提高（但可能不穩定）
  - 記憶體不足 → 降低

### 2. `ppo_epochs` - PPO訓練輪數
- **當前值**: GPU=10, CPU=4
- **影響**: 樣本利用效率 vs 過擬合
- **調整建議**:
  - 損失震盪 → 降低到 **4-6**
  - 學習太慢 → 提高到 **8-12**
  - 過擬合（訓練好但測試差）→ 降低

### 3. `learning_rate` - 學習率
- **當前值**: GPU=0.00025, CPU=0.0003
- **影響**: 學習速度和穩定性
- **調整建議**:
  - 損失不下降 → 提高到 **0.0005**
  - 訓練不穩定/爆炸 → 降低到 **0.0001**
  - GPU效果差 → 試試改成 **0.0003** (與CPU一致)

### 4. `ent_coef` - 熵係數
- **當前值**: 0.01
- **影響**: 探索 vs 收斂
- **調整建議**:
  - 策略過早收斂（熵<0.05）→ 提高到 **0.02-0.05**
  - 探索太隨機 → 降低到 **0.005**

### 5. `n_envs` - 並行環境數量
- **當前值**: GPU=16, CPU=4
- **影響**: 樣本多樣性和訓練速度
- **調整建議**:
  - GPU訓練效果差 → 降低到 **8** 或 **4**
  - CPU訓練太慢 → 試試 **8**（如果CPU夠強）

### 6. `horizon` - 軌跡長度
- **當前值**: GPU=4096, CPU=2048
- **影響**: 更新頻率和記憶體使用
- **調整建議**:
  - 記憶體不足 → 降低
  - 訓練不穩定 → 降低到 **2048** (GPU)

### 7. `clip_range` - PPO裁剪範圍
- **當前值**: 0.2
- **影響**: 策略更新幅度
- **調整建議**:
  - 訓練太激進 → 降低到 **0.1-0.15**
  - 學習太保守 → 提高到 **0.3**

### 8. `gamma` - 折扣因子
- **當前值**: 0.99
- **影響**: 長期 vs 短期獎勵權衡
- **調整建議**:
  - 只關注短期生存 → 降低到 **0.95**
  - 需要更長遠規劃 → 保持 **0.99**

## 🔧 針對當前問題的建議修改

### 問題1: GPU訓練效果(5分)遠不如CPU(70分)

**推薦調整** (修改 `training_config.json` 的 `gpu_training` 部分):
```json
{
  "gpu_training": {
    "batch_size": 64,        // 從256降到64（與CPU一致）
    "ppo_epochs": 4,         // 從10降到4（與CPU一致）
    "learning_rate": 0.0003, // 從0.00025改到0.0003（與CPU一致）
    "n_envs": 8              // 從16降到8
  }
}
```

**解釋**: 大的batch_size和過多的ppo_epochs可能導致訓練不穩定或過擬合小批次數據。

### 問題2: 沒有完成任何回合（mean_reward = N/A）

這不是參數問題，是環境或獎勵函數問題。需要檢查：

1. **遊戲是否太難**：智能體立即死亡
2. **done標誌**：檢查 `game/environment.py` 的 `step()` 方法
3. **獎勵函數**：是否有正向激勵

**臨時緩解**：試試單環境訓練
```json
{
  "gpu_training": {
    "n_envs": 1  // 單環境更容易調試
  }
}
```

## 📊 如何驗證參數是否生效

訓練時觀察控制台輸出，每10個迭代會顯示：

```
⚙️ 參數已從配置文件更新:
   • 學習率: 0.0003
   • batch_size: 64
   • ppo_epochs: 4
```

## 🎓 參數調整流程

1. **只改一個參數**：每次只修改一個，觀察效果
2. **等待10個迭代**：參數會自動重新讀取
3. **觀察指標變化**：查看損失、獎勵、熵值的變化趨勢
4. **記錄結果**：好的調整保留，壞的改回來

## ⚠️ 注意事項

- **不要同時改太多參數**：難以判斷哪個參數有效
- **給足夠時間觀察**：至少30-50個迭代才能看出趨勢
- **備份好的配置**：記錄效果好的參數組合
- **JSON格式要正確**：語法錯誤會導致參數無法加載

## 🚀 進階技巧

### 自適應調整策略

1. **早期訓練** (0-500 iterations):
   - 較高學習率 (0.0003-0.0005)
   - 較高熵係數 (0.02-0.05) 
   - 較大batch_size

2. **中期訓練** (500-2000 iterations):
   - 中等學習率 (0.0002-0.0003)
   - 中等熵係數 (0.01-0.02)

3. **後期訓練** (2000+ iterations):
   - 較低學習率 (0.0001-0.0002)
   - 較低熵係數 (0.005-0.01)
   - 策略收斂

## 📈 成功訓練的指標

- ✅ 平均獎勵持續上升
- ✅ 完成回合數增加
- ✅ 總損失穩定下降或平穩
- ✅ 熵值緩慢下降（0.1-0.5範圍）
- ✅ 策略損失和價值損失都在下降

## 🆘 緊急重置

如果訓練完全崩潰，可以使用這組保守參數：

```json
{
  "gpu_training": {
    "batch_size": 32,
    "ppo_epochs": 3,
    "learning_rate": 0.0001,
    "n_envs": 4,
    "ent_coef": 0.05,
    "clip_range": 0.1
  }
}
```

---

**提示**: 修改配置文件後，不需要重啟訓練！系統會每10個迭代自動檢測並應用新參數。
