"""
æ›´æ·±å…¥åˆ†æï¼šç¹ªè£½æ‰€æœ‰åƒæ•¸çš„è®ŠåŒ–æ›²ç·š
æ‰¾å‡ºå´©æ½°å‰å¾Œçš„é—œéµè®ŠåŒ–é»
"""

import json
import os
from collections import defaultdict

import matplotlib.pyplot as plt
import numpy as np
import torch

print("=" * 80)
print("ğŸ“Š ç¹ªè£½åƒæ•¸è®ŠåŒ–è¶¨å‹¢åœ–ï¼ˆ#5940 â†’ #14460ï¼‰")
print("=" * 80)

# === é…ç½® ===
START_ITER = 5940
END_ITER = 14460
SAMPLE_INTERVAL = 50  # æ›´å¯†é›†æ¡æ¨£
CHECKPOINT_DIR = "checkpoints"
CRASH_ITER = 7436  # å·²çŸ¥çš„å´©æ½°é»

# === æ”¶é›†æª¢æŸ¥é» ===
print("\næ”¶é›†æª¢æŸ¥é»...")
checkpoints_to_analyze = []
for iter_num in range(START_ITER, END_ITER + 1, SAMPLE_INTERVAL):
    checkpoint_iter = (iter_num // 10) * 10
    checkpoint_file = os.path.join(CHECKPOINT_DIR, f"checkpoint_{checkpoint_iter}.pt")
    if os.path.exists(checkpoint_file):
        checkpoints_to_analyze.append((checkpoint_iter, checkpoint_file))

print(f"âœ… æ‰¾åˆ° {len(checkpoints_to_analyze)} å€‹æª¢æŸ¥é»")

# === æ”¶é›†åƒæ•¸çµ±è¨ˆ ===
print("\nåˆ†æåƒæ•¸...")
param_history = defaultdict(
    lambda: {
        "iters": [],
        "mean": [],
        "std": [],
        "norm": [],
        "abs_mean": [],
        "min": [],
        "max": [],
    }
)

for iter_num, checkpoint_file in checkpoints_to_analyze:
    try:
        checkpoint = torch.load(checkpoint_file, map_location="cpu")
        model_state = checkpoint.get(
            "model_state", checkpoint.get("model_state_dict", {})
        )

        for param_name, param_tensor in model_state.items():
            if param_tensor.dtype in [torch.float32, torch.float16]:
                param_np = param_tensor.cpu().numpy().flatten()

                param_history[param_name]["iters"].append(iter_num)
                param_history[param_name]["mean"].append(float(np.mean(param_np)))
                param_history[param_name]["std"].append(float(np.std(param_np)))
                param_history[param_name]["norm"].append(
                    float(np.linalg.norm(param_np))
                )
                param_history[param_name]["abs_mean"].append(
                    float(np.mean(np.abs(param_np)))
                )
                param_history[param_name]["min"].append(float(np.min(param_np)))
                param_history[param_name]["max"].append(float(np.max(param_np)))

        print(f"   âœ“ #{iter_num}", end="\r")
    except Exception as e:
        print(f"   âœ— #{iter_num}: {e}")

print(f"\nâœ… å®Œæˆï¼Œå…± {len(param_history)} å€‹åƒæ•¸")

# === ç¹ªè£½é—œéµåƒæ•¸çš„è¶¨å‹¢åœ– ===
print("\nç”Ÿæˆè¶¨å‹¢åœ–...")

# å‰µå»ºåœ–è¡¨
fig, axes = plt.subplots(4, 2, figsize=(15, 12))
fig.suptitle("æ¨¡å‹åƒæ•¸è®ŠåŒ–è¶¨å‹¢ (#5940 â†’ #14460)", fontsize=16)

plot_idx = 0
param_names = list(param_history.keys())

for param_name in param_names[:8]:  # ç¹ªè£½å‰ 8 å€‹åƒæ•¸
    row = plot_idx // 2
    col = plot_idx % 2
    ax = axes[row, col]

    data = param_history[param_name]
    iters = data["iters"]

    # ç¹ªè£½ norm å’Œ abs_mean
    ax.plot(iters, data["norm"], "b-", label="L2 Norm", linewidth=1)
    ax2 = ax.twinx()
    ax2.plot(iters, data["abs_mean"], "r-", label="Abs Mean", linewidth=1, alpha=0.7)

    # æ¨™è¨˜å´©æ½°é»
    ax.axvline(x=CRASH_ITER, color="red", linestyle="--", linewidth=2, label="å´©æ½°é»")

    ax.set_xlabel("Iteration")
    ax.set_ylabel("L2 Norm", color="b")
    ax2.set_ylabel("Abs Mean", color="r")
    ax.set_title(param_name, fontsize=10)
    ax.tick_params(axis="y", labelcolor="b")
    ax2.tick_params(axis="y", labelcolor="r")
    ax.grid(True, alpha=0.3)

    plot_idx += 1

plt.tight_layout()
plt.savefig("checkpoints/param_trends.png", dpi=150)
print("âœ… åœ–è¡¨å·²ä¿å­˜åˆ°: checkpoints/param_trends.png")

# === è¨ˆç®—å´©æ½°å‰å¾Œçš„åƒæ•¸è®ŠåŒ– ===
print("\nåˆ†æå´©æ½°å‰å¾Œçš„åƒæ•¸è®ŠåŒ–...")

crash_analysis = []

for param_name, data in param_history.items():
    iters = data["iters"]
    norms = data["norm"]

    # æ‰¾å´©æ½°å‰å¾Œçš„æœ€è¿‘æª¢æŸ¥é»
    before_idx = None
    after_idx = None

    for i, iter_num in enumerate(iters):
        if iter_num <= CRASH_ITER:
            before_idx = i
        if iter_num > CRASH_ITER and after_idx is None:
            after_idx = i
            break

    if before_idx is not None and after_idx is not None:
        norm_before = norms[before_idx]
        norm_after = norms[after_idx]

        if norm_before > 0:
            change_pct = (norm_after - norm_before) / norm_before * 100

            crash_analysis.append(
                {
                    "param": param_name,
                    "iter_before": iters[before_idx],
                    "iter_after": iters[after_idx],
                    "norm_before": norm_before,
                    "norm_after": norm_after,
                    "change_pct": change_pct,
                    "abs_change": abs(change_pct),
                }
            )

# æŒ‰è®ŠåŒ–å¹…åº¦æ’åº
crash_analysis.sort(key=lambda x: x["abs_change"], reverse=True)

print(f"\nğŸ” å´©æ½°å‰å¾Œåƒæ•¸è®ŠåŒ– (Top 10):")
print(f"   {'åƒæ•¸':<20} {'å´©æ½°å‰':>12} {'å´©æ½°å¾Œ':>12} {'è®ŠåŒ–%':>10}")
print("-" * 60)
for i, item in enumerate(crash_analysis[:10]):
    print(
        f"{i+1:2d}. {item['param']:<20} {item['norm_before']:>12.6f} {item['norm_after']:>12.6f} {item['change_pct']:>+9.1f}%"
    )

# === åˆ†ææ•´é«”è¶¨å‹¢ ===
print("\nåˆ†ææ•´é«”åƒæ•¸è¶¨å‹¢...")

overall_trends = {}

for param_name, data in param_history.items():
    norms = data["norm"]

    if len(norms) > 5:
        # è¨ˆç®—ç·šæ€§è¶¨å‹¢ï¼ˆæ–œç‡ï¼‰
        x = np.arange(len(norms))
        slope = np.polyfit(x, norms, 1)[0]

        # è¨ˆç®—è®Šç•°ä¿‚æ•¸ï¼ˆCVï¼‰
        mean_norm = np.mean(norms)
        std_norm = np.std(norms)
        cv = (std_norm / mean_norm * 100) if mean_norm > 0 else 0

        # è¨ˆç®—ç¸½è®ŠåŒ–
        total_change_pct = (
            ((norms[-1] - norms[0]) / norms[0] * 100) if norms[0] > 0 else 0
        )

        overall_trends[param_name] = {
            "slope": slope,
            "cv": cv,
            "total_change_pct": total_change_pct,
            "mean_norm": mean_norm,
        }

# æ‰¾å‡ºè¶¨å‹¢æœ€ä¸ç©©å®šçš„åƒæ•¸
unstable_params = sorted(overall_trends.items(), key=lambda x: x[1]["cv"], reverse=True)

print(f"\nğŸ“‰ æœ€ä¸ç©©å®šçš„åƒæ•¸ (è®Šç•°ä¿‚æ•¸æœ€é«˜):")
print(f"   {'åƒæ•¸':<20} {'CV%':>10} {'ç¸½è®ŠåŒ–%':>12} {'å¹³å‡ Norm':>12}")
print("-" * 60)
for i, (param_name, trends) in enumerate(unstable_params[:10]):
    print(
        f"{i+1:2d}. {param_name:<20} {trends['cv']:>9.1f}% {trends['total_change_pct']:>+11.1f}% {trends['mean_norm']:>12.6f}"
    )

# === ä¿å­˜è©³ç´°åˆ†æ ===
print("\nä¿å­˜è©³ç´°åˆ†æ...")

detailed_report = {
    "analysis_time": str(np.datetime64("now")),
    "iteration_range": [START_ITER, END_ITER],
    "crash_iteration": CRASH_ITER,
    "checkpoints_analyzed": len(checkpoints_to_analyze),
    "parameters_tracked": len(param_history),
    "crash_impact": crash_analysis[:20],
    "unstable_parameters": [
        {"param": name, **trends} for name, trends in unstable_params[:20]
    ],
    "all_parameter_stats": {
        name: {
            "final_norm": data["norm"][-1] if data["norm"] else 0,
            "mean_norm": np.mean(data["norm"]) if data["norm"] else 0,
            "std_norm": np.std(data["norm"]) if data["norm"] else 0,
        }
        for name, data in param_history.items()
    },
}

with open("checkpoints/detailed_parameter_analysis.json", "w", encoding="utf-8") as f:
    json.dump(detailed_report, f, ensure_ascii=False, indent=2)

print("âœ… è©³ç´°å ±å‘Šå·²ä¿å­˜åˆ°: checkpoints/detailed_parameter_analysis.json")

# === ç¸½çµå»ºè­° ===
print("\n" + "=" * 80)
print("ğŸ“‹ åˆ†æçµè«–")
print("=" * 80)

print(f"\nçµ±è¨ˆ:")
print(f"   æª¢æŸ¥é»æ•¸é‡: {len(checkpoints_to_analyze)}")
print(f"   è¿½è¹¤çš„åƒæ•¸: {len(param_history)}")
print(f"   å´©æ½°è¿­ä»£: #{CRASH_ITER}")

if crash_analysis:
    max_change = crash_analysis[0]
    print(f"\nå´©æ½°å‰å¾Œæœ€å¤§è®ŠåŒ–:")
    print(f"   åƒæ•¸: {max_change['param']}")
    print(f"   è®ŠåŒ–å¹…åº¦: {max_change['change_pct']:+.1f}%")

if unstable_params:
    most_unstable = unstable_params[0]
    print(f"\næœ€ä¸ç©©å®šçš„åƒæ•¸:")
    print(f"   åƒæ•¸: {most_unstable[0]}")
    print(f"   è®Šç•°ä¿‚æ•¸: {most_unstable[1]['cv']:.1f}%")

print(f"\nğŸ¯ è¨“ç·´æ”¹é€²å»ºè­°:")

# æ ¹æ“šåˆ†æçµæœçµ¦å‡ºå»ºè­°
high_cv_count = sum(1 for _, trends in unstable_params if trends["cv"] > 50)
if high_cv_count > 0:
    print(f"   âš ï¸ {high_cv_count} å€‹åƒæ•¸çš„è®Šç•°ä¿‚æ•¸ > 50%")
    print(f"      â†’ å»ºè­°: é™ä½å­¸ç¿’ç‡ (ç›®å‰ 0.00025 â†’ å»ºè­° 0.0001)")
    print(f"      â†’ å»ºè­°: å¢åŠ æ¢¯åº¦è£å‰ªå¼·åº¦")

big_changes = [item for item in crash_analysis if abs(item["change_pct"]) > 20]
if big_changes:
    print(f"   âš ï¸ {len(big_changes)} å€‹åƒæ•¸åœ¨å´©æ½°å‰å¾Œè®ŠåŒ– > 20%")
    print(f"      â†’ å»ºè­°: å¢åŠ è¨“ç·´ç©©å®šæ€§ (ä½¿ç”¨ batch normalization)")
    print(f"      â†’ å»ºè­°: é™ä½ PPO clip range")

print(f"\nğŸ“ˆ è¶¨å‹¢åœ–:")
print(f"   è«‹æŸ¥çœ‹: checkpoints/param_trends.png")

print("\n" + "=" * 80)
