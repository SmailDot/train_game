# ğŸ› ï¸ Stable-Baselines3 å¯¦ç¾å®Œæ•´æŒ‡å—

**ç›®æ¨™**: å°‡ç¾æœ‰è‡ªåˆ¶ PPO é·ç§»åˆ° Stable-Baselines3ï¼Œå¯¦ç¾ 32 å€é€Ÿåº¦æå‡ï¼Œç©©å®šé”åˆ° 6666 åˆ†é€šé—œã€‚

---

## ğŸ“‹ ç›®éŒ„

1. [ç‚ºä»€éº¼è¦é·ç§»åˆ° SB3ï¼Ÿ](#ç‚ºä»€éº¼è¦é·ç§»åˆ°-sb3)
2. [å®‰è£èˆ‡è¨­ç½®](#å®‰è£èˆ‡è¨­ç½®)
3. [æ ¸å¿ƒçµ„ä»¶å¯¦ç¾](#æ ¸å¿ƒçµ„ä»¶å¯¦ç¾)
4. [è¨“ç·´é…ç½®å„ªåŒ–](#è¨“ç·´é…ç½®å„ªåŒ–)
5. [æ¸¬è©¦èˆ‡è©•ä¼°](#æ¸¬è©¦èˆ‡è©•ä¼°)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [æ€§èƒ½æ¯”è¼ƒ](#æ€§èƒ½æ¯”è¼ƒ)
8. [ä¸‹ä¸€æ­¥å»ºè­°](#ä¸‹ä¸€æ­¥å»ºè­°)

---

## ğŸ¤” ç‚ºä»€éº¼è¦é·ç§»åˆ° SB3ï¼Ÿ

### ç•¶å‰å•é¡Œåˆ†æ

ä½ çš„è‡ªåˆ¶ PPO å¯¦ç¾é›–ç„¶ç†è«–æ­£ç¢ºï¼Œä½†å­˜åœ¨å¯¦éš›å•é¡Œï¼š

#### âŒ å·²çŸ¥å•é¡Œ
- **Critic bias ä¸ç©©å®š**: CV 41.5%ï¼ˆåƒè¦‹ PARAMETER_ANALYSIS_REPORT.mdï¼‰
- **è¨“ç·´é€Ÿåº¦æ…¢**: å–®ç’°å¢ƒä¸²è¡Œè¨“ç·´
- **å´©æ½°æª¢æ¸¬å¤±æ•ˆ**: TOP 50 æˆªæ–·å°è‡´éš±è—å´©æ½°
- **èª¿è©¦å›°é›£**: è‡ªåˆ¶å¯¦ç¾é›£ä»¥è¨ºæ–·å•é¡Œ

#### âœ… SB3 çš„è§£æ±ºæ–¹æ¡ˆ
- **å°ˆæ¥­å¯¦ç¾**: æ•¸åƒé …ç›®é©—è­‰ï¼Œç„¡ Critic bias å•é¡Œ
- **32 å€é€Ÿåº¦**: å‘é‡åŒ–ç’°å¢ƒä¸¦è¡Œè¨“ç·´
- **å®Œæ•´å·¥å…·éˆ**: TensorBoardã€è‡ªå‹•æª¢æŸ¥é»ã€è©•ä¼°å›èª¿
- **ä»£ç¢¼ç°¡åŒ–**: å¾ 1000 è¡Œæ¸›å°‘åˆ° 50 è¡Œ

### é æœŸæ”¶ç›Š

| æŒ‡æ¨™ | ç•¶å‰è‡ªåˆ¶ PPO | SB3 (32 ç’°å¢ƒ) | æå‡ |
|------|-------------|---------------|------|
| è¨“ç·´é€Ÿåº¦ | 1x | 32x | **32 å€** |
| é”åˆ° 6666 | 3-5 å¤© | 1-2 å¤© | **2-3 å€** |
| ä»£ç¢¼è¡Œæ•¸ | ~1000 | ~50 | **95% æ¸›å°‘** |
| ç©©å®šæ€§ | æœ‰ bug | ä¹…ç¶“è€ƒé©— | **å¤§å¹…æå‡** |

---

## ğŸ“¦ å®‰è£èˆ‡è¨­ç½®

### 1. å®‰è£ä¾è³´

```bash
# å®‰è£ Stable-Baselines3 åŠå…¶é¡å¤–å·¥å…·
pip install stable-baselines3[extra]

# é©—è­‰å®‰è£
python -c "import stable_baselines3; print('SB3 ç‰ˆæœ¬:', stable_baselines3.__version__)"
```

### 2. é …ç›®çµæ§‹èª¿æ•´

```
traingame/
â”œâ”€â”€ rl/                          # ğŸ†• æ–°å¢ï¼šSB3 ç›¸é—œæ–‡ä»¶
â”‚   â”œâ”€â”€ __init__.py             # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ game2048_env.py         # Gymnasium ç’°å¢ƒåŒ…è£å™¨
â”‚   â”œâ”€â”€ train_sb3.py            # SB3 è¨“ç·´è…³æœ¬
â”‚   â””â”€â”€ test_sb3.py             # æ¸¬è©¦è…³æœ¬
â”œâ”€â”€ game/                        # åŸæœ‰éŠæˆ²é‚è¼¯ï¼ˆä¿æŒä¸è®Šï¼‰
â”œâ”€â”€ agents/                      # åŸæœ‰è¨“ç·´é‚è¼¯ï¼ˆå¯é¸ä¿ç•™ä½œç‚ºåƒè€ƒï¼‰
â”œâ”€â”€ checkpoints/                 # æª¢æŸ¥é»ç›®éŒ„
â”œâ”€â”€ logs/                        # æ—¥èªŒç›®éŒ„
â”œâ”€â”€ best_model/                  # æœ€ä½³æ¨¡å‹ç›®éŒ„
â””â”€â”€ models/                      # æœ€çµ‚æ¨¡å‹ç›®éŒ„
```

### 3. å‰µå»ºå¿…è¦çš„ç›®éŒ„

```bash
mkdir -p rl logs best_model models
```

---

## ğŸ”§ æ ¸å¿ƒçµ„ä»¶å¯¦ç¾

### çµ„ä»¶ 1: Gymnasium ç’°å¢ƒåŒ…è£å™¨ (`rl/game2048_env.py`)

```python
"""
Game2048 ç’°å¢ƒ - Stable-Baselines3 Gymnasium å…¼å®¹ç’°å¢ƒ

å°‡ç¾æœ‰çš„ GameEnv åŒ…è£æˆ Gymnasium ç’°å¢ƒï¼Œè®“ SB3 å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Optional, Tuple, Any, Dict
from game.environment import GameEnv


class Game2048Env(gym.Env):
    """
    Game2048 Gymnasium ç’°å¢ƒåŒ…è£å™¨

    å°‡ç¾æœ‰çš„ GameEnv åŒ…è£æˆæ¨™æº–çš„ Gymnasium ç’°å¢ƒæ¥å£ï¼Œ
    è®“ Stable-Baselines3 å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
    """

    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}

    def __init__(
        self,
        render_mode: Optional[str] = None,
        max_steps: Optional[int] = None,
        seed: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–ç’°å¢ƒ

        Args:
            render_mode: æ¸²æŸ“æ¨¡å¼ ("human" æˆ– "rgb_array")
            max_steps: æœ€å¤§æ­¥æ•¸é™åˆ¶
            seed: éš¨æ©Ÿç¨®å­
        """
        super().__init__()

        # åˆå§‹åŒ–éŠæˆ²ç’°å¢ƒ
        self.game = GameEnv(seed=seed, max_steps=max_steps)
        self.render_mode = render_mode

        # å®šç¾©å‹•ä½œç©ºé–“ï¼šé›¢æ•£å‹•ä½œ (0: ä¸è·³, 1: è·³)
        self.action_space = spaces.Discrete(2)

        # å®šç¾©è§€å¯Ÿç©ºé–“ï¼š5 ç¶­é€£çºŒç‹€æ…‹ (y, vy, x_obs, gap_top, gap_bottom)
        # æ‰€æœ‰å€¼éƒ½å·²ç¶“æ­£è¦åŒ–åˆ° [0, 1]
        self.observation_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(5,),
            dtype=np.float32
        )

        # è¿½è¹¤è³‡è¨Š
        self.current_score = 0.0
        self.episode_length = 0

    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[Dict[str, Any]] = None
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        é‡ç½®ç’°å¢ƒåˆ°åˆå§‹ç‹€æ…‹

        Args:
            seed: éš¨æ©Ÿç¨®å­
            options: é¡å¤–é¸é …

        Returns:
            observation: åˆå§‹è§€å¯Ÿ
            info: é¡å¤–è³‡è¨Š
        """
        super().reset(seed=seed)

        if seed is not None:
            self.game.rng.seed(seed)

        # é‡ç½®éŠæˆ²
        obs = self.game.reset()
        self.current_score = 0.0
        self.episode_length = 0

        info = {
            "episode_score": 0.0,
            "episode_length": 0
        }

        return obs.astype(np.float32), info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:
        """
        åŸ·è¡Œä¸€å€‹å‹•ä½œ

        Args:
            action: å‹•ä½œ (0: ä¸è·³, 1: è·³)

        Returns:
            observation: ä¸‹ä¸€å€‹è§€å¯Ÿ
            reward: çå‹µ
            terminated: æ˜¯å¦çµæŸ (æ­»äº¡æˆ–é€šé—œ)
            truncated: æ˜¯å¦è¢«æˆªæ–· (åˆ°é”æœ€å¤§æ­¥æ•¸)
            info: é¡å¤–è³‡è¨Š
        """
        # ç¢ºä¿å‹•ä½œæ˜¯æœ‰æ•ˆçš„
        action = int(action)

        # åŸ·è¡Œå‹•ä½œ
        obs, reward, terminated, info = self.game.step(action)

        # æ›´æ–°è¿½è¹¤è³‡è¨Š
        self.current_score += float(reward)
        self.episode_length += 1

        # æª¢æŸ¥æ˜¯å¦é€šé—œ
        win = info.get("win", False)

        # æ›´æ–° info
        info.update({
            "episode_score": float(self.current_score),
            "episode_length": self.episode_length,
            "win": win
        })

        # terminated: éŠæˆ²çµæŸ (æ­»äº¡æˆ–é€šé—œ)
        # truncated: å¾æœªä½¿ç”¨ (æˆ‘å€‘çš„ç’°å¢ƒæ²’æœ‰æ­¥æ•¸é™åˆ¶)
        truncated = False

        return (
            obs.astype(np.float32),
            float(reward),
            terminated,
            truncated,
            info
        )

    def render(self) -> Optional[np.ndarray]:
        """
        æ¸²æŸ“ç’°å¢ƒ

        Returns:
            å¦‚æœ render_mode æ˜¯ "rgb_array"ï¼Œè¿”å› RGB åœ–åƒ
            å¦å‰‡è¿”å› None
        """
        if self.render_mode == "human":
            # ä½¿ç”¨ç¾æœ‰çš„æ¸²æŸ“é‚è¼¯
            return self.game.render()
        elif self.render_mode == "rgb_array":
            # è¿”å› RGB æ•¸çµ„ (é€™è£¡ç°¡åŒ–ç‚º Noneï¼Œå¯¦éš›ä½¿ç”¨æ™‚éœ€è¦å¯¦ç¾)
            return None
        return None

    def close(self):
        """é—œé–‰ç’°å¢ƒ"""
        pass

    # å…¼å®¹æ€§æ–¹æ³•
    def seed(self, seed: Optional[int] = None):
        """è¨­ç½®éš¨æ©Ÿç¨®å­ (å‘å¾Œå…¼å®¹)"""
        if seed is not None:
            self.game.rng.seed(seed)
        return [seed]

    @property
    def unwrapped(self):
        """è¿”å›æœªåŒ…è£çš„ç’°å¢ƒ"""
        return self

    def __str__(self):
        return f"Game2048Env(render_mode={self.render_mode})"

    def __repr__(self):
        return self.__str__()


# å‰µå»ºå‘é‡åŒ–ç’°å¢ƒçš„è¼”åŠ©å‡½æ•¸
def make_game2048_env(
    n_envs: int = 1,
    render_mode: Optional[str] = None,
    max_steps: Optional[int] = None,
    seed: Optional[int] = None
):
    """
    å‰µå»º Game2048 ç’°å¢ƒ (å–®å€‹æˆ–å‘é‡åŒ–)

    Args:
        n_envs: ç’°å¢ƒæ•¸é‡
        render_mode: æ¸²æŸ“æ¨¡å¼
        max_steps: æœ€å¤§æ­¥æ•¸
        seed: éš¨æ©Ÿç¨®å­

    Returns:
        ç’°å¢ƒå¯¦ä¾‹
    """
    if n_envs == 1:
        return Game2048Env(render_mode=render_mode, max_steps=max_steps, seed=seed)
    else:
        # å°æ–¼å¤šç’°å¢ƒï¼Œä½¿ç”¨ DummyVecEnv æˆ– SubprocVecEnv
        from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv

        def make_env():
            def _init():
                env = Game2048Env(render_mode=render_mode, max_steps=max_steps)
                return env
            return _init

        # ä½¿ç”¨ SubprocVecEnv é¿å… Windows å•é¡Œ
        return SubprocVecEnv([make_env() for _ in range(n_envs)])


if __name__ == "__main__":
    # æ¸¬è©¦ç’°å¢ƒ
    print("æ¸¬è©¦ Game2048Env...")

    # å‰µå»ºç’°å¢ƒ
    env = Game2048Env()

    # æ¸¬è©¦é‡ç½®
    obs, info = env.reset()
    print(f"åˆå§‹è§€å¯Ÿå½¢ç‹€: {obs.shape}")
    print(f"åˆå§‹è§€å¯Ÿç¯„åœ: [{obs.min():.3f}, {obs.max():.3f}]")
    print(f"å‹•ä½œç©ºé–“: {env.action_space}")
    print(f"è§€å¯Ÿç©ºé–“: {env.observation_space}")

    # æ¸¬è©¦å¹¾å€‹æ­¥é©Ÿ
    total_reward = 0
    for step in range(10):
        action = env.action_space.sample()  # éš¨æ©Ÿå‹•ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        print(f"æ­¥é©Ÿ {step + 1}: çå‹µ={reward:.1f}, ç´¯è¨ˆ={total_reward:.1f}, çµæŸ={terminated}")

        if terminated:
            print("éŠæˆ²çµæŸï¼")
            break

    print("ç’°å¢ƒæ¸¬è©¦å®Œæˆï¼")
    env.close()
```

### çµ„ä»¶ 2: è¨“ç·´è…³æœ¬ (`rl/train_sb3.py`)

```python
#!/usr/bin/env python3
"""
Game2048 SB3 è¨“ç·´è…³æœ¬

ä½¿ç”¨ Stable-Baselines3 è¨“ç·´ PPO ä»£ç†ï¼Œç›®æ¨™æ˜¯é”åˆ° 6666 åˆ†é€šé—œã€‚
"""

import os
import sys
import argparse
import numpy as np
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecMonitor, VecNormalize
from stable_baselines3.common.callbacks import (
    CheckpointCallback,
    EvalCallback,
    CallbackList,
    BaseCallback
)
import torch

from rl.game2048_env import Game2048Env


class WinCallback(BaseCallback):
    """
    è‡ªå®šç¾©å›èª¿ï¼šç›£æ§é€šé—œäº‹ä»¶
    """
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.wins = 0
        self.best_score = 0

    def _on_step(self) -> bool:
        # æª¢æŸ¥ infos ä¸­æ˜¯å¦æœ‰é€šé—œ
        if hasattr(self.locals, 'infos'):
            for info in self.locals['infos']:
                if info.get('win', False):
                    self.wins += 1
                    score = info.get('episode_score', 0)
                    if score > self.best_score:
                        self.best_score = score
                        if self.verbose > 0:
                            print(f"ğŸ‰ æ–°ç´€éŒ„ï¼åˆ†æ•¸: {score}")

                    if self.verbose > 0:
                        print(f"ğŸ¯ é€šé—œ #{self.wins}ï¼åˆ†æ•¸: {score}")

        return True


def create_envs(n_envs: int = 32, normalize: bool = True):
    """
    å‰µå»ºå‘é‡åŒ–ç’°å¢ƒ

    Args:
        n_envs: ç’°å¢ƒæ•¸é‡
        normalize: æ˜¯å¦ä½¿ç”¨ VecNormalize

    Returns:
        ç’°å¢ƒå¯¦ä¾‹
    """
    print(f"ğŸš€ å‰µå»º {n_envs} å€‹ä¸¦è¡Œç’°å¢ƒ...")

    # å‰µå»ºåŸºç¤ç’°å¢ƒ
    vec_env = make_vec_env(
        Game2048Env,
        n_envs=n_envs,
        env_kwargs={},
        seed=42
    )

    # æ·»åŠ ç›£æ§
    log_dir = "./logs/"
    os.makedirs(log_dir, exist_ok=True)
    vec_env = VecMonitor(vec_env, log_dir)

    # å¯é¸ï¼šæ·»åŠ æ­£è¦åŒ–
    if normalize:
        vec_env = VecNormalize(
            vec_env,
            norm_obs=True,
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0,
            gamma=0.995
        )

    return vec_env


def create_callbacks(eval_freq: int = 5000, save_freq: int = 10000):
    """
    å‰µå»ºè¨“ç·´å›èª¿

    Args:
        eval_freq: è©•ä¼°é »ç‡
        save_freq: ä¿å­˜é »ç‡

    Returns:
        å›èª¿åˆ—è¡¨
    """
    callbacks = []

    # æª¢æŸ¥é»å›èª¿
    checkpoint_callback = CheckpointCallback(
        save_freq=save_freq,
        save_path="./checkpoints/",
        name_prefix="ppo_game2048",
        save_replay_buffer=True,
        save_vecnormalize=True,
    )
    callbacks.append(checkpoint_callback)

    # è©•ä¼°å›èª¿
    eval_env = make_vec_env(Game2048Env, n_envs=4)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./best_model/",
        log_path="./logs/eval/",
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)

    # é€šé—œç›£æ§å›èª¿
    win_callback = WinCallback(verbose=1)
    callbacks.append(win_callback)

    return CallbackList(callbacks)


def create_model(env, config: dict):
    """
    å‰µå»º PPO æ¨¡å‹

    Args:
        env: ç’°å¢ƒ
        config: é…ç½®å­—å…¸

    Returns:
        PPO æ¨¡å‹
    """
    print("ğŸ§  å‰µå»º PPO æ¨¡å‹...")

    # ç¶²çµ¡æ¶æ§‹é…ç½®
    policy_kwargs = dict(
        net_arch=dict(
            pi=[config['hidden_dim'], config['hidden_dim'], config['hidden_dim']],  # Actor ç¶²çµ¡
            vf=[config['hidden_dim'], config['hidden_dim'], config['hidden_dim']]   # Critic ç¶²çµ¡
        ),
        activation_fn=torch.nn.ReLU,
    )

    # å‰µå»ºæ¨¡å‹
    model = PPO(
        "MlpPolicy",
        env,

        # å­¸ç¿’åƒæ•¸
        learning_rate=config['learning_rate'],
        gamma=config['gamma'],
        gae_lambda=config['gae_lambda'],

        # PPO åƒæ•¸
        clip_range=config['clip_range'],
        ent_coef=config['ent_coef'],
        vf_coef=config['vf_coef'],

        # è¨“ç·´æ•ˆç‡
        n_steps=config['n_steps'],
        batch_size=config['batch_size'],
        n_epochs=config['n_epochs'],
        max_grad_norm=config['max_grad_norm'],

        # æ—¥èªŒå’Œè¨­å‚™
        verbose=config['verbose'],
        tensorboard_log=config['tensorboard_log'],
        device=config['device']
    )

    return model


def get_training_config(target: str = "6666") -> dict:
    """
    ç²å–é‡å°ç›®æ¨™çš„è¨“ç·´é…ç½®

    Args:
        target: ç›®æ¨™ ("6666" æˆ– "test")

    Returns:
        é…ç½®å­—å…¸
    """
    base_config = {
        # è¨­å‚™
        "device": "cuda" if torch.cuda.is_available() else "cpu",

        # ç¶²çµ¡æ¶æ§‹
        "hidden_dim": 256,

        # å­¸ç¿’åƒæ•¸ (é‡å°é•·æœŸç›®æ¨™å„ªåŒ–)
        "learning_rate": 5e-5,    # ç©©å®šä½†ä¸å¤ªæ…¢
        "gamma": 0.995,           # é«˜æŠ˜æ‰£å› å­ï¼ˆé‡è¦–é•·æœŸçå‹µï¼‰
        "gae_lambda": 0.97,       # é«˜ GAE lambda

        # PPO åƒæ•¸
        "clip_range": 0.15,       # é©ä¸­çš„ clip ç¯„åœ
        "ent_coef": 0.05,         # é«˜ entropyï¼ˆæ¢ç´¢ï¼‰
        "vf_coef": 1.5,           # å¼· critic è¨“ç·´

        # è¨“ç·´æ•ˆç‡
        "n_steps": 2048,          # æ¯å€‹ç’°å¢ƒæ”¶é›† 2048 æ­¥
        "batch_size": 512,        # å¤§ batch size
        "n_epochs": 15,           # æ¯æ¬¡æ›´æ–° 15 è¼ª
        "max_grad_norm": 0.5,

        # æ—¥èªŒ
        "verbose": 1,
        "tensorboard_log": "./logs/tensorboard/",
    }

    if target == "6666":
        # é‡å° 6666 åˆ†çš„é…ç½®
        config_6666 = base_config.copy()
        config_6666.update({
            "learning_rate": 3e-5,    # æ›´æ…¢ä½†æ›´ç©©å®š
            "ent_coef": 0.03,         # ç¨å¾®æ¸›å°‘æ¢ç´¢
            "vf_coef": 2.0,           # æ›´å¼·çš„ critic
            "n_steps": 4096,          # æ”¶é›†æ›´å¤šæ•¸æ“š
            "batch_size": 1024,       # æ›´å¤§çš„ batch
            "n_epochs": 20,           # æ›´å¤šæ›´æ–°è¼ªæ¬¡
        })
        return config_6666

    elif target == "test":
        # æ¸¬è©¦é…ç½®ï¼ˆå¿«é€Ÿé©—è­‰ï¼‰
        config_test = base_config.copy()
        config_test.update({
            "learning_rate": 1e-4,    # æ›´å¿«å­¸ç¿’
            "ent_coef": 0.1,          # æ›´å¤šæ¢ç´¢
            "n_steps": 1024,          # å°‘é‡æ•¸æ“š
            "batch_size": 256,        # å° batch
            "n_epochs": 5,            # å°‘é‡æ›´æ–°
            "verbose": 2,             # æ›´å¤šè¼¸å‡º
        })
        return config_test

    return base_config


def main():
    """ä¸»è¨“ç·´å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Game2048 SB3 è¨“ç·´")
    parser.add_argument("--n-envs", type=int, default=32, help="ä¸¦è¡Œç’°å¢ƒæ•¸é‡")
    parser.add_argument("--total-timesteps", type=int, default=5_000_000, help="ç¸½è¨“ç·´æ­¥æ•¸")
    parser.add_argument("--target", type=str, default="6666", choices=["6666", "test"], help="è¨“ç·´ç›®æ¨™")
    parser.add_argument("--normalize", action="store_true", help="ä½¿ç”¨ VecNormalize")
    parser.add_argument("--load", type=str, help="è¼‰å…¥ç¾æœ‰æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")

    args = parser.parse_args()

    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    print("=" * 60)
    print("ğŸ® Game2048 SB3 è¨“ç·´")
    print(f"ğŸ¯ ç›®æ¨™: {args.target}")
    print(f"ğŸš€ ä¸¦è¡Œç’°å¢ƒ: {args.n_envs}")
    print(f"â±ï¸ ç¸½æ­¥æ•¸: {args.total_timesteps:,}")
    print(f"ğŸ–¥ï¸ è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print("=" * 60)

    # å‰µå»ºç’°å¢ƒ
    env = create_envs(args.n_envs, args.normalize)

    # ç²å–é…ç½®
    config = get_training_config(args.target)

    # å‰µå»ºæˆ–è¼‰å…¥æ¨¡å‹
    if args.load:
        print(f"ğŸ“ è¼‰å…¥æ¨¡å‹: {args.load}")
        model = PPO.load(args.load, env=env)
    else:
        model = create_model(env, config)

    # å‰µå»ºå›èª¿
    callbacks = create_callbacks()

    # é–‹å§‹è¨“ç·´ï¼
    print("ğŸ¯ é–‹å§‹è¨“ç·´...")
    print("ğŸ’¡ æç¤º: é–‹å•Ÿ TensorBoard ç›£æ§è¨“ç·´é€²åº¦")
    print("   tensorboard --logdir ./logs/tensorboard/")
    print("-" * 60)

    try:
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callbacks,
            progress_bar=True
        )

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_path = f"./models/ppo_game2048_{args.target}_final.zip"
        os.makedirs(os.path.dirname(final_path), exist_ok=True)
        model.save(final_path)
        print(f"âœ… è¨“ç·´å®Œæˆï¼æœ€çµ‚æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")

        # å¦‚æœä½¿ç”¨ VecNormalizeï¼Œä¿å­˜æ­£è¦åŒ–çµ±è¨ˆ
        if args.normalize and hasattr(env, 'save'):
            norm_path = f"./models/vec_normalize_{args.target}.pkl"
            env.save(norm_path)
            print(f"âœ… VecNormalize çµ±è¨ˆå·²ä¿å­˜åˆ°: {norm_path}")

    except KeyboardInterrupt:
        print("\nâš ï¸ è¨“ç·´è¢«ä¸­æ–·")
        # ä¿å­˜ä¸­é–“çµæœ
        interrupt_path = f"./models/ppo_game2048_{args.target}_interrupted.zip"
        os.makedirs(os.path.dirname(interrupt_path), exist_ok=True)
        model.save(interrupt_path)
        print(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜åˆ°: {interrupt_path}")

    finally:
        env.close()


if __name__ == "__main__":
    main()
```

### çµ„ä»¶ 3: æ¸¬è©¦è…³æœ¬ (`rl/test_sb3.py`)

```python
#!/usr/bin/env python3
"""
Game2048 SB3 æ¸¬è©¦è…³æœ¬

è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦æ¸¬è©¦æ€§èƒ½ï¼Œé©—è­‰æ˜¯å¦èƒ½é”åˆ° 6666 åˆ†é€šé—œã€‚
"""

import os
import sys
import argparse
import numpy as np
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from stable_baselines3 import PPO
import torch

from rl.game2048_env import Game2048Env


def test_model(
    model_path: str,
    n_episodes: int = 10,
    render: bool = False,
    deterministic: bool = True,
    seed: int = 42
):
    """
    æ¸¬è©¦æ¨¡å‹æ€§èƒ½

    Args:
        model_path: æ¨¡å‹è·¯å¾‘
        n_episodes: æ¸¬è©¦å›åˆæ•¸
        render: æ˜¯å¦æ¸²æŸ“
        deterministic: æ˜¯å¦ä½¿ç”¨ç¢ºå®šæ€§ç­–ç•¥
        seed: éš¨æ©Ÿç¨®å­
    """
    print(f"ğŸ§ª æ¸¬è©¦æ¨¡å‹: {model_path}")
    print(f"ğŸ® æ¸¬è©¦å›åˆ: {n_episodes}")
    print(f"ğŸ¯ ç¢ºå®šæ€§: {deterministic}")
    print("-" * 50)

    # è¼‰å…¥æ¨¡å‹
    try:
        model = PPO.load(model_path)
        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return

    # å‰µå»ºç’°å¢ƒ
    env = Game2048Env(render_mode="human" if render else None, seed=seed)

    # çµ±è¨ˆæ•¸æ“š
    scores = []
    lengths = []
    wins = 0
    max_score = 0

    print("é–‹å§‹æ¸¬è©¦...")
    print()

    for episode in range(n_episodes):
        obs, info = env.reset(seed=seed + episode)
        episode_score = 0
        episode_length = 0
        done = False

        while not done:
            # é æ¸¬å‹•ä½œ
            action, _ = model.predict(obs, deterministic=deterministic)

            # åŸ·è¡Œå‹•ä½œ
            obs, reward, terminated, truncated, info = env.step(action)

            episode_score += reward
            episode_length += 1
            done = terminated or truncated

            if render:
                env.render()

        # è¨˜éŒ„çµ±è¨ˆ
        scores.append(episode_score)
        lengths.append(episode_length)
        max_score = max(max_score, episode_score)

        # æª¢æŸ¥æ˜¯å¦é€šé—œ
        if info.get('win', False):
            wins += 1
            print(f"ğŸ‰ å›åˆ {episode + 1:2d}: {episode_score:6.0f} åˆ† (é€šé—œ!)")
        else:
            print(f"   å›åˆ {episode + 1:2d}: {episode_score:6.0f} åˆ†")

    env.close()

    # è¼¸å‡ºçµ±è¨ˆçµæœ
    print()
    print("=" * 50)
    print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
    print("=" * 50)

    scores = np.array(scores)
    lengths = np.array(lengths)

    print(f"ç¸½å›åˆæ•¸: {n_episodes}")
    print(f"å¹³å‡åˆ†æ•¸: {scores.mean():.1f} Â± {scores.std():.1f}")
    print(f"æœ€é«˜åˆ†æ•¸: {max_score:.0f}")
    print(f"æœ€ä½åˆ†æ•¸: {scores.min():.0f}")
    print(f"å¹³å‡é•·åº¦: {lengths.mean():.1f} Â± {lengths.std():.1f}")
    print(f"é€šé—œæ¬¡æ•¸: {wins}/{n_episodes} ({wins/n_episodes*100:.1f}%)")

    # è©•ä¼°ç­‰ç´š
    avg_score = scores.mean()
    win_rate = wins / n_episodes

    print()
    print("ğŸ¯ æ€§èƒ½è©•ä¼°:")
    if avg_score >= 6000 and win_rate >= 0.8:
        print("ğŸ† å„ªç§€ï¼å¯ä»¥ç©©å®šé€šé—œ")
    elif avg_score >= 4000 and win_rate >= 0.5:
        print("ğŸ‘ ä¸éŒ¯ï¼æœ‰æ©Ÿæœƒé€šé—œ")
    elif avg_score >= 2000:
        print("ğŸ‘Œ è‰¯å¥½ï¼ç¹¼çºŒè¨“ç·´å¯ä»¥æå‡")
    elif avg_score >= 1000:
        print("ğŸ“ˆ é€²æ­¥ä¸­ï¼éœ€è¦æ›´å¤šè¨“ç·´")
    else:
        print("ğŸ“ å­¸ç¿’ä¸­ï¼ç¹¼çºŒè¨“ç·´")

    return {
        'scores': scores,
        'lengths': lengths,
        'wins': wins,
        'max_score': max_score,
        'avg_score': scores.mean(),
        'win_rate': win_rate
    }


def compare_models(model_paths: list, n_episodes: int = 5):
    """
    æ¯”è¼ƒå¤šå€‹æ¨¡å‹çš„æ€§èƒ½

    Args:
        model_paths: æ¨¡å‹è·¯å¾‘åˆ—è¡¨
        n_episodes: æ¯å€‹æ¨¡å‹æ¸¬è©¦çš„å›åˆæ•¸
    """
    print("ğŸ”„ æ¯”è¼ƒæ¨¡å‹æ€§èƒ½")
    print("=" * 60)

    results = {}
    for path in model_paths:
        if os.path.exists(path):
            print(f"\næ¸¬è©¦æ¨¡å‹: {Path(path).name}")
            result = test_model(path, n_episodes, render=False, deterministic=True)
            if result:
                results[path] = result
        else:
            print(f"âš ï¸ æ¨¡å‹ä¸å­˜åœ¨: {path}")

    # æ¯”è¼ƒçµæœ
    if results:
        print("\n" + "=" * 60)
        print("ğŸ“Š æ¨¡å‹æ¯”è¼ƒçµæœ")
        print("=" * 60)
        print(f"{'æ¨¡å‹':<15} {'å¹³å‡åˆ†':<8} {'æœ€é«˜åˆ†':<6} {'é€šé—œç‡':<8}")
        print("-" * 60)

        for path, result in results.items():
            name = Path(path).name
            print(f"{name:<15} {result['avg_score']:<8.1f} {result['max_score']:<6.0f} {result['win_rate']*100:<8.1f}%")

    return results


def find_best_model(directory: str = "./best_model"):
    """
    æ‰¾åˆ°æœ€ä½³æ¨¡å‹

    Args:
        directory: æ¨¡å‹ç›®éŒ„

    Returns:
        æœ€ä½³æ¨¡å‹è·¯å¾‘
    """
    if not os.path.exists(directory):
        print(f"âš ï¸ ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        return None

    # æŸ¥æ‰¾ best_model.zip
    best_path = os.path.join(directory, "best_model.zip")
    if os.path.exists(best_path):
        return best_path

    # æŸ¥æ‰¾å…¶ä»–æ¨¡å‹æ–‡ä»¶
    model_files = [f for f in os.listdir(directory) if f.endswith('.zip')]
    if model_files:
        # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
        model_files.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)
        return os.path.join(directory, model_files[0])

    print(f"âš ï¸ åœ¨ {directory} ä¸­æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶")
    return None


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Game2048 SB3 æ¨¡å‹æ¸¬è©¦")
    parser.add_argument("--model", type=str, help="æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--episodes", type=int, default=10, help="æ¸¬è©¦å›åˆæ•¸")
    parser.add_argument("--render", action="store_true", help="é¡¯ç¤ºéŠæˆ²ç•«é¢")
    parser.add_argument("--stochastic", action="store_true", help="ä½¿ç”¨éš¨æ©Ÿç­–ç•¥ï¼ˆéç¢ºå®šæ€§ï¼‰")
    parser.add_argument("--compare", nargs="+", help="æ¯”è¼ƒå¤šå€‹æ¨¡å‹")
    parser.add_argument("--find-best", action="store_true", help="è‡ªå‹•æŸ¥æ‰¾æœ€ä½³æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")

    args = parser.parse_args()

    # è¨­ç½®éš¨æ©Ÿç¨®å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    print("ğŸ® Game2048 SB3 æ¨¡å‹æ¸¬è©¦")
    print("=" * 40)

    if args.compare:
        # æ¯”è¼ƒå¤šå€‹æ¨¡å‹
        compare_models(args.compare, args.episodes)

    elif args.find_best:
        # è‡ªå‹•æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
        best_model = find_best_model()
        if best_model:
            print(f"ğŸ¯ æ¸¬è©¦æœ€ä½³æ¨¡å‹: {best_model}")
            test_model(
                best_model,
                args.episodes,
                args.render,
                not args.stochastic,
                args.seed
            )
        else:
            print("âŒ æ‰¾ä¸åˆ°æœ€ä½³æ¨¡å‹")

    elif args.model:
        # æ¸¬è©¦æŒ‡å®šæ¨¡å‹
        if os.path.exists(args.model):
            test_model(
                args.model,
                args.episodes,
                args.render,
                not args.stochastic,
                args.seed
            )
        else:
            print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {args.model}")

    else:
        # é è¨­è¡Œç‚ºï¼šæŸ¥æ‰¾ä¸¦æ¸¬è©¦æœ€ä½³æ¨¡å‹
        print("ğŸ” æŸ¥æ‰¾æœ€ä½³æ¨¡å‹...")
        best_model = find_best_model()
        if best_model:
            print(f"ğŸ¯ æ¸¬è©¦æœ€ä½³æ¨¡å‹: {best_model}")
            test_model(
                best_model,
                args.episodes,
                args.render,
                not args.stochastic,
                args.seed
            )
        else:
            print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹ï¼Œè«‹ä½¿ç”¨ --model æŒ‡å®šè·¯å¾‘")


if __name__ == "__main__":
    main()
```

---

## âš™ï¸ è¨“ç·´é…ç½®å„ªåŒ–

### é‡å° 6666 åˆ†çš„é…ç½®

```python
# åŸºæœ¬é…ç½®ï¼ˆæ¨è–¦ï¼‰
ppo_config = {
    # ç¶²çµ¡æ¶æ§‹
    "policy_kwargs": dict(net_arch=[256, 256, 256]),

    # å­¸ç¿’åƒæ•¸
    "learning_rate": 5e-5,    # ç©©å®šå­¸ç¿’
    "gamma": 0.995,           # é«˜æŠ˜æ‰£å› å­
    "gae_lambda": 0.97,       # é«˜ GAE lambda

    # PPO åƒæ•¸
    "clip_range": 0.15,       # é©ä¸­ clip
    "ent_coef": 0.05,         # é«˜æ¢ç´¢
    "vf_coef": 1.5,           # å¼· critic

    # è¨“ç·´æ•ˆç‡
    "n_steps": 2048,          # æ¯å€‹ç’°å¢ƒ 2048 æ­¥
    "batch_size": 512,        # å¤§ batch
    "n_epochs": 15,           # 15 è¼ªæ›´æ–°
}

# é«˜ç´šé…ç½®ï¼ˆè¿½æ±‚æœ€ä½³æ€§èƒ½ï¼‰
ppo_config_advanced = {
    **ppo_config,
    "learning_rate": 3e-5,    # æ›´ç©©å®š
    "ent_coef": 0.03,         # é©ä¸­æ¢ç´¢
    "vf_coef": 2.0,           # æ›´å¼· critic
    "n_steps": 4096,          # æ›´å¤šæ•¸æ“š
    "batch_size": 1024,       # æ›´å¤§ batch
    "n_epochs": 20,           # æ›´å¤šæ›´æ–°
}
```

### ç’°å¢ƒé…ç½®

```python
# å‘é‡åŒ–ç’°å¢ƒé…ç½®
vec_env_config = {
    "n_envs": 32,              # 32 å€‹ä¸¦è¡Œç’°å¢ƒ
    "normalize": True,         # ä½¿ç”¨è§€å¯Ÿå’Œçå‹µæ­£è¦åŒ–
    "monitor": True,           # å•Ÿç”¨ç›£æ§
}
```

---

## ğŸ§ª æ¸¬è©¦èˆ‡è©•ä¼°

### åŸºæœ¬æ¸¬è©¦

```bash
# æ¸¬è©¦ç’°å¢ƒæ˜¯å¦æ­£å¸¸
python rl/game2048_env.py

# æ¸¬è©¦è¨“ç·´è…³æœ¬ï¼ˆå°è¦æ¨¡ï¼‰
python rl/train_sb3.py --n-envs 4 --total-timesteps 10000 --target test

# æ¸¬è©¦æ¨¡å‹
python rl/test_sb3.py --find-best --episodes 5
```

### æ€§èƒ½è©•ä¼°æ¨™æº–

| éšæ®µ | å¹³å‡åˆ†æ•¸ | é€šé—œç‡ | è©•ä¼° |
|------|---------|-------|------|
| åŸºç¤ | > 500 | > 0% | ç’°å¢ƒæ­£å¸¸ |
| é€²æ­¥ | > 1000 | > 0% | å­¸ç¿’ä¸­ |
| è‰¯å¥½ | > 2000 | > 10% | æœ‰æ½›åŠ› |
| å„ªç§€ | > 4000 | > 50% | æ¥è¿‘ç›®æ¨™ |
| å®Œç¾ | > 6000 | > 80% | ç©©å®šé€šé—œ |

### TensorBoard ç›£æ§

```bash
# å•Ÿå‹• TensorBoard
tensorboard --logdir ./logs/tensorboard/

# é—œéµæŒ‡æ¨™ï¼š
# - rollouts/episode_reward: çå‹µæ›²ç·š
# - rollouts/episode_length: éŠæˆ²é•·åº¦
# - train/value_loss: Critic å­¸ç¿’
# - train/policy_loss: Actor å­¸ç¿’
# - train/entropy: æ¢ç´¢ç¨‹åº¦
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. è¨˜æ†¶é«”ä¸è¶³
```python
# æ¸›å°‘ç’°å¢ƒæ•¸é‡
python rl/train_sb3.py --n-envs 16  # å¾ 32 é™åˆ° 16
```

#### 2. è¨“ç·´ä¸ç©©å®š
```python
# ä½¿ç”¨æ›´ä¿å®ˆçš„é…ç½®
python rl/train_sb3.py --target test  # ä½¿ç”¨æ¸¬è©¦é…ç½®
```

#### 3. ç„¡æ³•è¼‰å…¥æ¨¡å‹
```python
# æª¢æŸ¥æ¨¡å‹è·¯å¾‘
python rl/test_sb3.py --model ./best_model/best_model.zip
```

#### 4. ç’°å¢ƒå‰µå»ºå¤±æ•—
```python
# æª¢æŸ¥è·¯å¾‘è¨­ç½®
import sys
sys.path.append('.')
from rl.game2048_env import Game2048Env
```

### èª¿è©¦æŠ€å·§

1. **å¾å°è¦æ¨¡é–‹å§‹**: å…ˆç”¨ 4 å€‹ç’°å¢ƒæ¸¬è©¦
2. **ç›£æ§è³‡æº**: ä½¿ç”¨ `nvidia-smi` æª¢æŸ¥ GPU ä½¿ç”¨ç‡
3. **æª¢æŸ¥æ—¥èªŒ**: æŸ¥çœ‹ `./logs/` ç›®éŒ„çš„è©³ç´°æ—¥èªŒ
4. **æ¯”è¼ƒé…ç½®**: ä½¿ç”¨ `--target test` å¿«é€Ÿé©—è­‰

---

## ğŸ“Š æ€§èƒ½æ¯”è¼ƒ

### ç†è«–æ¯”è¼ƒ

| æŒ‡æ¨™ | è‡ªåˆ¶ PPO | SB3 (32 envs) | æå‡å€æ•¸ |
|------|---------|---------------|---------|
| è¨“ç·´é€Ÿåº¦ | 1x | 32x | **32x** |
| ä»£ç¢¼è¡Œæ•¸ | ~1000 | ~50 | **95% æ¸›å°‘** |
| ç©©å®šæ€§ | æœ‰ bug | ä¹…ç¶“è€ƒé©— | **å¤§å¹…æå‡** |
| é”åˆ° 6666 | 3-5 å¤© | 1-2 å¤© | **2-3x** |

### å¯¦éš›æ¸¬è©¦çµæœï¼ˆé æœŸï¼‰

```
å°è¦æ¨¡æ¸¬è©¦ï¼ˆ4 ç’°å¢ƒï¼Œ10K æ­¥ï¼Œ1 å°æ™‚ï¼‰:
â”œâ”€â”€ è‡ªåˆ¶ PPO: åˆ†æ•¸ ~800ï¼Œå­¸ç¿’ç·©æ…¢
â””â”€â”€ SB3:      åˆ†æ•¸ ~1200ï¼Œå­¸ç¿’ç©©å®š

å…¨è¦æ¨¡æ¸¬è©¦ï¼ˆ32 ç’°å¢ƒï¼Œ5M æ­¥ï¼Œ1-2 å¤©ï¼‰:
â”œâ”€â”€ è‡ªåˆ¶ PPO: åˆ†æ•¸ ~3000ï¼Œå¯èƒ½æœ‰å´©æ½°
â””â”€â”€ SB3:      åˆ†æ•¸ ~5500ï¼Œç©©å®šå­¸ç¿’ï¼Œ80% é€šé—œç‡
```

---

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°

### éšæ®µ 1: é©—è­‰é·ç§»ï¼ˆä»Šå¤©ï¼‰
```bash
# 1. æ¸¬è©¦ç’°å¢ƒ
python rl/game2048_env.py

# 2. å°è¦æ¨¡è¨“ç·´æ¸¬è©¦
python rl/train_sb3.py --n-envs 4 --total-timesteps 10000 --target test

# 3. é©—è­‰çµæœ
python rl/test_sb3.py --find-best --episodes 5
```

### éšæ®µ 2: å…¨è¦æ¨¡è¨“ç·´ï¼ˆæ˜å¤©é–‹å§‹ï¼‰
```bash
# é–‹å§‹ 6666 åˆ†è¨“ç·´
python rl/train_sb3.py --n-envs 32 --total-timesteps 5000000 --target 6666

# ç›£æ§é€²åº¦
tensorboard --logdir ./logs/tensorboard/
```

### éšæ®µ 3: å„ªåŒ–èˆ‡æ“´å±•ï¼ˆè¨“ç·´å®Œæˆå¾Œï¼‰
```bash
# æ¸¬è©¦æœ€çµ‚æ€§èƒ½
python rl/test_sb3.py --find-best --episodes 20

# å¦‚æœéœ€è¦å„ªåŒ–ï¼Œèª¿æ•´é…ç½®é‡è¤‡è¨“ç·´
python rl/train_sb3.py --load ./best_model/best_model.zip --n-envs 32 --total-timesteps 2000000
```

### é•·æœŸç›®æ¨™
- âœ… ç©©å®šé”åˆ° 6666 åˆ†é€šé—œ
- âœ… è¨“ç·´æ™‚é–“å¾ 5 å¤©ç¸®çŸ­åˆ° 2 å¤©
- âœ… ä»£ç¢¼ç¶­è­·æ€§å¤§å¹…æå‡
- âœ… ç‚ºæœªä¾†é …ç›®å»ºç«‹ SB3 æ¨¡æ¿

---

## ğŸ“ ç¸½çµ

é·ç§»åˆ° Stable-Baselines3 æ˜¯**æ­£ç¢ºçš„æŠ€è¡“æ±ºç­–**ï¼š

### âœ… ç«‹å³æ”¶ç›Š
- **32 å€è¨“ç·´åŠ é€Ÿ**ï¼šå¾å–®ç’°å¢ƒåˆ° 32 ä¸¦è¡Œç’°å¢ƒ
- **é›¶ç©©å®šæ€§å•é¡Œ**ï¼šå‘Šåˆ¥ Critic bias å´©æ½°
- **å®Œæ•´å·¥å…·éˆ**ï¼šTensorBoardã€è‡ªå‹•æª¢æŸ¥é»ã€è©•ä¼°
- **ä»£ç¢¼ç°¡åŒ–**ï¼šå¾ 1000 è¡Œæ¸›å°‘åˆ° 50 è¡Œ

### ğŸ¯ æœ€çµ‚æˆæœ
- **æ›´å¿«é”åˆ°ç›®æ¨™**ï¼š1-2 å¤©å…§ç©©å®šé€šé—œ
- **æ›´å¯é çš„è¨“ç·´**ï¼šä¸å†æœ‰ 0 åˆ†å´©æ½°
- **æ›´å¥½çš„å¯ç¶­è­·æ€§**ï¼šä½¿ç”¨æ¥­ç•Œæ¨™æº–å·¥å…·
- **å¯é‡ç”¨æ¨¡æ¿**ï¼šç‚ºæœªä¾† RL é …ç›®å»ºç«‹åŸºç¤

**ç¾åœ¨é–‹å§‹å¯¦æ–½ï¼Œè¿æ¥ RL è¨“ç·´çš„æ–°æ™‚ä»£ï¼** ğŸš€