# PPO 訓練效果改進指南

## 問題診斷

如果訓練1400回合、388次迭代後仍無進展，可能原因包括：

### 1. 獎勵函數問題 ⚠️
**症狀**: 平均獎勵始終為負或接近零
**原因**: 
- 碰撞懲罰過大（-5），通過獎勵過小（+5）
- 沒有鼓勵存活時間的小獎勵
- 獎勵信號不夠明確

**解決方案**: 修改 `game/environment.py` 的獎勵設定
```python
# 建議的獎勵設定
通過障礙物: +10.0 (增加)
碰撞: -5.0 (保持)
每步存活: +0.1 (新增)
成功穿越間隙: +2.0 (新增)
```

### 2. 學習率問題 📉
**症狀**: Loss 下降很慢或不下降
**原因**: 學習率 2.5e-4 可能過小

**解決方案**: 調整學習率
```python
# 在 utils/training_config.py
"lr": 5e-4,  # 增加到 5e-4
```

### 3. 探索不足 🔍
**症狀**: 熵值(entropy)過低 (< 0.3)
**原因**: ent_coef 太小(0.01)，導致策略過早收斂

**解決方案**: 
```python
"ent_coef": 0.05,  # 增加探索
```

### 4. 網路容量問題 🧠
**症狀**: 梯度範數過小 (< 1e-6)
**原因**: 網路可能過於簡單，無法學習複雜策略

**解決方案**: 檢查 `agents/networks.py` 的網路結構
```python
# 當前結構: 5 -> 64 -> 64 -> 2
# 建議增加層數或神經元數
# 5 -> 128 -> 128 -> 64 -> 2
```

## 如何監控訓練

### 1. 即時診斷信息
每10次迭代會自動打印：
- Loss 指標
- 平均獎勵
- 權重統計
- 梯度範數
- 學習進度評估

### 2. 訓練視窗觀察重點

#### Loss Function
- **Policy Loss**: 應該逐漸下降並穩定
- **Value Loss**: 應該逐漸下降
- **Entropy**: 初期高(探索)，後期低(利用)
- **Total Loss**: 整體趨勢向下

#### 神經網路可視化
- **節點顏色變化**: 深藍色(權重小) -> 紅色(權重大)
- **應該看到**: 節點顏色隨訓練動態變化
- **如果沒變化**: 權重沒有更新，檢查梯度

#### 獎勵曲線（控制台）
- **目標**: 平均獎勵逐漸上升
- **正常範圍**: 初期 -5 到 0，中期 0 到 10，後期 10+
- **如果持續負值**: 檢查獎勵函數設定

### 3. 並行環境效果

#### 理論加速
- 4個環境: 4x 數據收集速度
- 8個環境: 8x 數據收集速度
- 12個環境: 12x 數據收集速度

#### 實際效果
- **CPU**: 加速效果明顯（4個環境 ~3x）
- **GPU**: 加速效果更明顯（8個環境 ~6-7x）
- **觀察點**: 每次迭代的時間應該減少

#### 驗證方法
```bash
# 單環境訓練10次迭代
並行環境 = 1, 觀察時間 T1

# 多環境訓練10次迭代  
並行環境 = 8, 觀察時間 T2

# 加速比 = T1 / T2 (應該接近 4-6)
```

## 快速修復建議

### 立即嘗試 (5分鐘)

1. **增加獎勵尺度**
   ```python
   # game/environment.py, line 155-165
   if gap_top < self.y < gap_bottom:
       reward += 10.0  # 改為 10
       self.passed_count += 1
   ```

2. **降低初始難度**
   ```python
   # game/environment.py, line 132-137
   gap_half = self.rng.uniform(60.0, 80.0)  # 增大間隙
   ```

3. **增加探索**
   ```python
   # utils/training_config.py, line 19
   "ent_coef": 0.05,  # 改為 0.05
   ```

### 中期調整 (1小時)

1. **改進獎勵函數**
   - 添加小步獎勵(+0.1/step)
   - 添加高度懲罰(離中心太遠)
   - 調整通過/碰撞獎勵比例

2. **調整超參數**
   - 學習率: 2.5e-4 -> 5e-4
   - Batch size: 256 -> 128 (如果內存不足)
   - PPO epochs: 10 -> 8 (加快訓練)

3. **檢查碰撞檢測**
   - 確認球體碰撞判定正確
   - 測試手動遊玩是否合理

### 長期優化 (數小時)

1. **網路架構優化**
   ```python
   # agents/networks.py
   self.fc1 = nn.Linear(5, 128)  # 增加到 128
   self.fc2 = nn.Linear(128, 128)
   self.fc3 = nn.Linear(128, 64)  # 新增一層
   ```

2. **課程學習** (Curriculum Learning)
   - 初期: 大間隙、慢速度
   - 中期: 正常間隙、中速度
   - 後期: 小間隙、快速度

3. **獎勵塑造** (Reward Shaping)
   - 距離中心線的獎勵
   - 平穩飛行的獎勵
   - 連續通過的獎勵加成

## 預期訓練曲線

### 正常學習曲線
```
迭代     平均獎勵    Loss      學習狀態
0-50     -5 到 -2    5-10      初期探索
50-100   -2 到  0    3-5       開始學習
100-200   0 到  5    2-3       穩定進步
200-300   5 到 10    1-2       持續改進
300+     10 到 20+   0.5-1     收斂階段
```

### 異常情況

#### Loss 不下降
- 檢查學習率是否過小
- 檢查梯度是否為零
- 檢查數據是否有問題

#### 獎勵震蕩
- 降低學習率
- 增加 batch size
- 減少 PPO epochs

#### 過早收斂到差策略
- 增加熵係數
- 重置訓練(初始化按鈕)
- 調整獎勵函數

## 實驗記錄模板

建議記錄每次實驗：

```
實驗 #1
日期: 2025-11-14
修改: 學習率 2.5e-4 -> 5e-4
訓練次數: 100 次迭代
結果: 平均獎勵從 -3 提升到 2
結論: 學習率提升有效

實驗 #2
...
```

## 常見問題 FAQ

**Q: 為什麼GPU訓練比CPU還差？**
A: 可能原因：
1. Batch size 過大導致訓練不穩定（嘗試減半）
2. 學習率需要調整（GPU建議稍微降低）
3. 隨機種子不同導致初始化不同

**Q: 並行環境真的有用嗎？**
A: 有用，但：
1. 需要確認實際使用了向量化環境（看控制台輸出）
2. CPU上4-8個環境即可，更多無益
3. GPU上8-12個環境效果最好

**Q: 訓練多久才能看到效果？**
A: 通常：
- 50-100次迭代: 開始有進步跡象
- 200-300次迭代: 應該能穩定通過幾個障礙物
- 500+次迭代: 達到較好的效果

**Q: 如何判斷是否在正確訓練？**
A: 觀察以下指標：
1. Loss 逐漸下降（不用很快，但要有趨勢）
2. 權重統計值在變化（平均值和標準差）
3. 平均梯度範數 > 1e-6
4. 平均獎勵有上升趨勢（即使很慢）

## 獲取幫助

如果以上都無法解決，提供以下信息：
1. 控制台最近10次迭代的診斷信息
2. Loss Function 圖表截圖
3. 訓練配置 (utils/training_config.py)
4. 獎勵函數修改 (game/environment.py)
5. 網路結構 (agents/networks.py)
